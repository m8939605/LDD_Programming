diff -Ncwr -x .orig -x .rej linux-2.6.23.1/fs/proc/proc_misc.c linux-2.6.23.1km/fs/proc/proc_misc.c
*** linux-2.6.23.1/fs/proc/proc_misc.c	2007-10-13 01:43:44.000000000 +0900
--- linux-2.6.23.1km/fs/proc/proc_misc.c	2008-02-05 20:48:38.000000000 +0900
***************
*** 438,443 ****
--- 438,461 ----
  #endif
  #endif
  
+ #ifdef CONFIG_KMALLOC_ACCOUNTING
+ 
+ extern struct seq_operations kmalloc_account_op;
+ 
+ static int kmalloc_account_open(struct inode *inode, struct file *file)
+ {
+ 	return seq_open(file, &kmalloc_account_op);
+ }
+ 
+ static struct file_operations proc_kmalloc_account_operations = {
+ 	.open		= kmalloc_account_open,
+ 	.read		= seq_read,
+ 	.llseek		= seq_lseek,
+ 	.release	= seq_release,
+ };
+ 
+ #endif
+ 
  static int show_stat(struct seq_file *p, void *v)
  {
  	int i;
***************
*** 527,533 ****
  
  	kfree(per_irq_sum);
  	return 0;
! }
  
  static int stat_open(struct inode *inode, struct file *file)
  {
--- 545,551 ----
  
  	kfree(per_irq_sum);
  	return 0;
! }
  
  static int stat_open(struct inode *inode, struct file *file)
  {
***************
*** 659,665 ****
  struct proc_dir_entry *proc_root_kcore;
  
  void create_seq_entry(char *name, mode_t mode, const struct file_operations *f)
! {
  	struct proc_dir_entry *entry;
  	entry = create_proc_entry(name, mode, NULL);
  	if (entry)
--- 677,683 ----
  struct proc_dir_entry *proc_root_kcore;
  
  void create_seq_entry(char *name, mode_t mode, const struct file_operations *f)
! {
  	struct proc_dir_entry *entry;
  	entry = create_proc_entry(name, mode, NULL);
  	if (entry)
***************
*** 715,720 ****
--- 733,741 ----
  	create_seq_entry("slab_allocators", 0 ,&proc_slabstats_operations);
  #endif
  #endif
+ #ifdef CONFIG_KMALLOC_ACCOUNTING
+  	create_seq_entry("kmalloc",S_IRUGO,&proc_kmalloc_account_operations);
+ #endif
  	create_seq_entry("buddyinfo",S_IRUGO, &fragmentation_file_operations);
  	create_seq_entry("vmstat",S_IRUGO, &proc_vmstat_file_operations);
  	create_seq_entry("zoneinfo",S_IRUGO, &proc_zoneinfo_file_operations);
diff -Ncwr -x .orig -x .rej linux-2.6.23.1/fs/proc/proc_misc.c.orig linux-2.6.23.1km/fs/proc/proc_misc.c.orig
*** linux-2.6.23.1/fs/proc/proc_misc.c.orig	1970-01-01 09:00:00.000000000 +0900
--- linux-2.6.23.1km/fs/proc/proc_misc.c.orig	2007-10-13 01:43:44.000000000 +0900
***************
*** 0 ****
--- 1,751 ----
+ /*
+  *  linux/fs/proc/proc_misc.c
+  *
+  *  linux/fs/proc/array.c
+  *  Copyright (C) 1992  by Linus Torvalds
+  *  based on ideas by Darren Senn
+  *
+  *  This used to be the part of array.c. See the rest of history and credits
+  *  there. I took this into a separate file and switched the thing to generic
+  *  proc_file_inode_operations, leaving in array.c only per-process stuff.
+  *  Inumbers allocation made dynamic (via create_proc_entry()).  AV, May 1999.
+  *
+  * Changes:
+  * Fulton Green      :  Encapsulated position metric calculations.
+  *			<kernel@FultonGreen.com>
+  */
+ 
+ #include <linux/types.h>
+ #include <linux/errno.h>
+ #include <linux/time.h>
+ #include <linux/kernel.h>
+ #include <linux/kernel_stat.h>
+ #include <linux/fs.h>
+ #include <linux/tty.h>
+ #include <linux/string.h>
+ #include <linux/mman.h>
+ #include <linux/proc_fs.h>
+ #include <linux/ioport.h>
+ #include <linux/mm.h>
+ #include <linux/mmzone.h>
+ #include <linux/pagemap.h>
+ #include <linux/swap.h>
+ #include <linux/slab.h>
+ #include <linux/smp.h>
+ #include <linux/signal.h>
+ #include <linux/module.h>
+ #include <linux/init.h>
+ #include <linux/seq_file.h>
+ #include <linux/times.h>
+ #include <linux/profile.h>
+ #include <linux/utsname.h>
+ #include <linux/blkdev.h>
+ #include <linux/hugetlb.h>
+ #include <linux/jiffies.h>
+ #include <linux/sysrq.h>
+ #include <linux/vmalloc.h>
+ #include <linux/crash_dump.h>
+ #include <linux/pid_namespace.h>
+ #include <asm/uaccess.h>
+ #include <asm/pgtable.h>
+ #include <asm/io.h>
+ #include <asm/tlb.h>
+ #include <asm/div64.h>
+ #include "internal.h"
+ 
+ #define LOAD_INT(x) ((x) >> FSHIFT)
+ #define LOAD_FRAC(x) LOAD_INT(((x) & (FIXED_1-1)) * 100)
+ /*
+  * Warning: stuff below (imported functions) assumes that its output will fit
+  * into one page. For some of those functions it may be wrong. Moreover, we
+  * have a way to deal with that gracefully. Right now I used straightforward
+  * wrappers, but this needs further analysis wrt potential overflows.
+  */
+ extern int get_hardware_list(char *);
+ extern int get_stram_list(char *);
+ extern int get_filesystem_list(char *);
+ extern int get_exec_domain_list(char *);
+ extern int get_dma_list(char *);
+ extern int get_locks_status (char *, char **, off_t, int);
+ 
+ static int proc_calc_metrics(char *page, char **start, off_t off,
+ 				 int count, int *eof, int len)
+ {
+ 	if (len <= off+count) *eof = 1;
+ 	*start = page + off;
+ 	len -= off;
+ 	if (len>count) len = count;
+ 	if (len<0) len = 0;
+ 	return len;
+ }
+ 
+ static int loadavg_read_proc(char *page, char **start, off_t off,
+ 				 int count, int *eof, void *data)
+ {
+ 	int a, b, c;
+ 	int len;
+ 
+ 	a = avenrun[0] + (FIXED_1/200);
+ 	b = avenrun[1] + (FIXED_1/200);
+ 	c = avenrun[2] + (FIXED_1/200);
+ 	len = sprintf(page,"%d.%02d %d.%02d %d.%02d %ld/%d %d\n",
+ 		LOAD_INT(a), LOAD_FRAC(a),
+ 		LOAD_INT(b), LOAD_FRAC(b),
+ 		LOAD_INT(c), LOAD_FRAC(c),
+ 		nr_running(), nr_threads, current->nsproxy->pid_ns->last_pid);
+ 	return proc_calc_metrics(page, start, off, count, eof, len);
+ }
+ 
+ static int uptime_read_proc(char *page, char **start, off_t off,
+ 				 int count, int *eof, void *data)
+ {
+ 	struct timespec uptime;
+ 	struct timespec idle;
+ 	int len;
+ 	cputime_t idletime = cputime_add(init_task.utime, init_task.stime);
+ 
+ 	do_posix_clock_monotonic_gettime(&uptime);
+ 	monotonic_to_bootbased(&uptime);
+ 	cputime_to_timespec(idletime, &idle);
+ 	len = sprintf(page,"%lu.%02lu %lu.%02lu\n",
+ 			(unsigned long) uptime.tv_sec,
+ 			(uptime.tv_nsec / (NSEC_PER_SEC / 100)),
+ 			(unsigned long) idle.tv_sec,
+ 			(idle.tv_nsec / (NSEC_PER_SEC / 100)));
+ 
+ 	return proc_calc_metrics(page, start, off, count, eof, len);
+ }
+ 
+ static int meminfo_read_proc(char *page, char **start, off_t off,
+ 				 int count, int *eof, void *data)
+ {
+ 	struct sysinfo i;
+ 	int len;
+ 	unsigned long committed;
+ 	unsigned long allowed;
+ 	struct vmalloc_info vmi;
+ 	long cached;
+ 
+ /*
+  * display in kilobytes.
+  */
+ #define K(x) ((x) << (PAGE_SHIFT - 10))
+ 	si_meminfo(&i);
+ 	si_swapinfo(&i);
+ 	committed = atomic_read(&vm_committed_space);
+ 	allowed = ((totalram_pages - hugetlb_total_pages())
+ 		* sysctl_overcommit_ratio / 100) + total_swap_pages;
+ 
+ 	cached = global_page_state(NR_FILE_PAGES) -
+ 			total_swapcache_pages - i.bufferram;
+ 	if (cached < 0)
+ 		cached = 0;
+ 
+ 	get_vmalloc_info(&vmi);
+ 
+ 	/*
+ 	 * Tagged format, for easy grepping and expansion.
+ 	 */
+ 	len = sprintf(page,
+ 		"MemTotal:     %8lu kB\n"
+ 		"MemFree:      %8lu kB\n"
+ 		"Buffers:      %8lu kB\n"
+ 		"Cached:       %8lu kB\n"
+ 		"SwapCached:   %8lu kB\n"
+ 		"Active:       %8lu kB\n"
+ 		"Inactive:     %8lu kB\n"
+ #ifdef CONFIG_HIGHMEM
+ 		"HighTotal:    %8lu kB\n"
+ 		"HighFree:     %8lu kB\n"
+ 		"LowTotal:     %8lu kB\n"
+ 		"LowFree:      %8lu kB\n"
+ #endif
+ 		"SwapTotal:    %8lu kB\n"
+ 		"SwapFree:     %8lu kB\n"
+ 		"Dirty:        %8lu kB\n"
+ 		"Writeback:    %8lu kB\n"
+ 		"AnonPages:    %8lu kB\n"
+ 		"Mapped:       %8lu kB\n"
+ 		"Slab:         %8lu kB\n"
+ 		"SReclaimable: %8lu kB\n"
+ 		"SUnreclaim:   %8lu kB\n"
+ 		"PageTables:   %8lu kB\n"
+ 		"NFS_Unstable: %8lu kB\n"
+ 		"Bounce:       %8lu kB\n"
+ 		"CommitLimit:  %8lu kB\n"
+ 		"Committed_AS: %8lu kB\n"
+ 		"VmallocTotal: %8lu kB\n"
+ 		"VmallocUsed:  %8lu kB\n"
+ 		"VmallocChunk: %8lu kB\n",
+ 		K(i.totalram),
+ 		K(i.freeram),
+ 		K(i.bufferram),
+ 		K(cached),
+ 		K(total_swapcache_pages),
+ 		K(global_page_state(NR_ACTIVE)),
+ 		K(global_page_state(NR_INACTIVE)),
+ #ifdef CONFIG_HIGHMEM
+ 		K(i.totalhigh),
+ 		K(i.freehigh),
+ 		K(i.totalram-i.totalhigh),
+ 		K(i.freeram-i.freehigh),
+ #endif
+ 		K(i.totalswap),
+ 		K(i.freeswap),
+ 		K(global_page_state(NR_FILE_DIRTY)),
+ 		K(global_page_state(NR_WRITEBACK)),
+ 		K(global_page_state(NR_ANON_PAGES)),
+ 		K(global_page_state(NR_FILE_MAPPED)),
+ 		K(global_page_state(NR_SLAB_RECLAIMABLE) +
+ 				global_page_state(NR_SLAB_UNRECLAIMABLE)),
+ 		K(global_page_state(NR_SLAB_RECLAIMABLE)),
+ 		K(global_page_state(NR_SLAB_UNRECLAIMABLE)),
+ 		K(global_page_state(NR_PAGETABLE)),
+ 		K(global_page_state(NR_UNSTABLE_NFS)),
+ 		K(global_page_state(NR_BOUNCE)),
+ 		K(allowed),
+ 		K(committed),
+ 		(unsigned long)VMALLOC_TOTAL >> 10,
+ 		vmi.used >> 10,
+ 		vmi.largest_chunk >> 10
+ 		);
+ 
+ 		len += hugetlb_report_meminfo(page + len);
+ 
+ 	return proc_calc_metrics(page, start, off, count, eof, len);
+ #undef K
+ }
+ 
+ extern struct seq_operations fragmentation_op;
+ static int fragmentation_open(struct inode *inode, struct file *file)
+ {
+ 	(void)inode;
+ 	return seq_open(file, &fragmentation_op);
+ }
+ 
+ static const struct file_operations fragmentation_file_operations = {
+ 	.open		= fragmentation_open,
+ 	.read		= seq_read,
+ 	.llseek		= seq_lseek,
+ 	.release	= seq_release,
+ };
+ 
+ extern struct seq_operations zoneinfo_op;
+ static int zoneinfo_open(struct inode *inode, struct file *file)
+ {
+ 	return seq_open(file, &zoneinfo_op);
+ }
+ 
+ static const struct file_operations proc_zoneinfo_file_operations = {
+ 	.open		= zoneinfo_open,
+ 	.read		= seq_read,
+ 	.llseek		= seq_lseek,
+ 	.release	= seq_release,
+ };
+ 
+ static int version_read_proc(char *page, char **start, off_t off,
+ 				 int count, int *eof, void *data)
+ {
+ 	int len;
+ 
+ 	len = snprintf(page, PAGE_SIZE, linux_proc_banner,
+ 		utsname()->sysname,
+ 		utsname()->release,
+ 		utsname()->version);
+ 	return proc_calc_metrics(page, start, off, count, eof, len);
+ }
+ 
+ extern struct seq_operations cpuinfo_op;
+ static int cpuinfo_open(struct inode *inode, struct file *file)
+ {
+ 	return seq_open(file, &cpuinfo_op);
+ }
+ 
+ static const struct file_operations proc_cpuinfo_operations = {
+ 	.open		= cpuinfo_open,
+ 	.read		= seq_read,
+ 	.llseek		= seq_lseek,
+ 	.release	= seq_release,
+ };
+ 
+ static int devinfo_show(struct seq_file *f, void *v)
+ {
+ 	int i = *(loff_t *) v;
+ 
+ 	if (i < CHRDEV_MAJOR_HASH_SIZE) {
+ 		if (i == 0)
+ 			seq_printf(f, "Character devices:\n");
+ 		chrdev_show(f, i);
+ 	}
+ #ifdef CONFIG_BLOCK
+ 	else {
+ 		i -= CHRDEV_MAJOR_HASH_SIZE;
+ 		if (i == 0)
+ 			seq_printf(f, "\nBlock devices:\n");
+ 		blkdev_show(f, i);
+ 	}
+ #endif
+ 	return 0;
+ }
+ 
+ static void *devinfo_start(struct seq_file *f, loff_t *pos)
+ {
+ 	if (*pos < (BLKDEV_MAJOR_HASH_SIZE + CHRDEV_MAJOR_HASH_SIZE))
+ 		return pos;
+ 	return NULL;
+ }
+ 
+ static void *devinfo_next(struct seq_file *f, void *v, loff_t *pos)
+ {
+ 	(*pos)++;
+ 	if (*pos >= (BLKDEV_MAJOR_HASH_SIZE + CHRDEV_MAJOR_HASH_SIZE))
+ 		return NULL;
+ 	return pos;
+ }
+ 
+ static void devinfo_stop(struct seq_file *f, void *v)
+ {
+ 	/* Nothing to do */
+ }
+ 
+ static struct seq_operations devinfo_ops = {
+ 	.start = devinfo_start,
+ 	.next  = devinfo_next,
+ 	.stop  = devinfo_stop,
+ 	.show  = devinfo_show
+ };
+ 
+ static int devinfo_open(struct inode *inode, struct file *filp)
+ {
+ 	return seq_open(filp, &devinfo_ops);
+ }
+ 
+ static const struct file_operations proc_devinfo_operations = {
+ 	.open		= devinfo_open,
+ 	.read		= seq_read,
+ 	.llseek		= seq_lseek,
+ 	.release	= seq_release,
+ };
+ 
+ extern struct seq_operations vmstat_op;
+ static int vmstat_open(struct inode *inode, struct file *file)
+ {
+ 	return seq_open(file, &vmstat_op);
+ }
+ static const struct file_operations proc_vmstat_file_operations = {
+ 	.open		= vmstat_open,
+ 	.read		= seq_read,
+ 	.llseek		= seq_lseek,
+ 	.release	= seq_release,
+ };
+ 
+ #ifdef CONFIG_PROC_HARDWARE
+ static int hardware_read_proc(char *page, char **start, off_t off,
+ 				 int count, int *eof, void *data)
+ {
+ 	int len = get_hardware_list(page);
+ 	return proc_calc_metrics(page, start, off, count, eof, len);
+ }
+ #endif
+ 
+ #ifdef CONFIG_STRAM_PROC
+ static int stram_read_proc(char *page, char **start, off_t off,
+ 				 int count, int *eof, void *data)
+ {
+ 	int len = get_stram_list(page);
+ 	return proc_calc_metrics(page, start, off, count, eof, len);
+ }
+ #endif
+ 
+ #ifdef CONFIG_BLOCK
+ extern struct seq_operations partitions_op;
+ static int partitions_open(struct inode *inode, struct file *file)
+ {
+ 	return seq_open(file, &partitions_op);
+ }
+ static const struct file_operations proc_partitions_operations = {
+ 	.open		= partitions_open,
+ 	.read		= seq_read,
+ 	.llseek		= seq_lseek,
+ 	.release	= seq_release,
+ };
+ 
+ extern struct seq_operations diskstats_op;
+ static int diskstats_open(struct inode *inode, struct file *file)
+ {
+ 	return seq_open(file, &diskstats_op);
+ }
+ static const struct file_operations proc_diskstats_operations = {
+ 	.open		= diskstats_open,
+ 	.read		= seq_read,
+ 	.llseek		= seq_lseek,
+ 	.release	= seq_release,
+ };
+ #endif
+ 
+ #ifdef CONFIG_MODULES
+ extern struct seq_operations modules_op;
+ static int modules_open(struct inode *inode, struct file *file)
+ {
+ 	return seq_open(file, &modules_op);
+ }
+ static const struct file_operations proc_modules_operations = {
+ 	.open		= modules_open,
+ 	.read		= seq_read,
+ 	.llseek		= seq_lseek,
+ 	.release	= seq_release,
+ };
+ #endif
+ 
+ #ifdef CONFIG_SLAB
+ static int slabinfo_open(struct inode *inode, struct file *file)
+ {
+ 	return seq_open(file, &slabinfo_op);
+ }
+ static const struct file_operations proc_slabinfo_operations = {
+ 	.open		= slabinfo_open,
+ 	.read		= seq_read,
+ 	.write		= slabinfo_write,
+ 	.llseek		= seq_lseek,
+ 	.release	= seq_release,
+ };
+ 
+ #ifdef CONFIG_DEBUG_SLAB_LEAK
+ extern struct seq_operations slabstats_op;
+ static int slabstats_open(struct inode *inode, struct file *file)
+ {
+ 	unsigned long *n = kzalloc(PAGE_SIZE, GFP_KERNEL);
+ 	int ret = -ENOMEM;
+ 	if (n) {
+ 		ret = seq_open(file, &slabstats_op);
+ 		if (!ret) {
+ 			struct seq_file *m = file->private_data;
+ 			*n = PAGE_SIZE / (2 * sizeof(unsigned long));
+ 			m->private = n;
+ 			n = NULL;
+ 		}
+ 		kfree(n);
+ 	}
+ 	return ret;
+ }
+ 
+ static const struct file_operations proc_slabstats_operations = {
+ 	.open		= slabstats_open,
+ 	.read		= seq_read,
+ 	.llseek		= seq_lseek,
+ 	.release	= seq_release_private,
+ };
+ #endif
+ #endif
+ 
+ static int show_stat(struct seq_file *p, void *v)
+ {
+ 	int i;
+ 	unsigned long jif;
+ 	cputime64_t user, nice, system, idle, iowait, irq, softirq, steal;
+ 	u64 sum = 0;
+ 	struct timespec boottime;
+ 	unsigned int *per_irq_sum;
+ 
+ 	per_irq_sum = kzalloc(sizeof(unsigned int)*NR_IRQS, GFP_KERNEL);
+ 	if (!per_irq_sum)
+ 		return -ENOMEM;
+ 
+ 	user = nice = system = idle = iowait =
+ 		irq = softirq = steal = cputime64_zero;
+ 	getboottime(&boottime);
+ 	jif = boottime.tv_sec;
+ 
+ 	for_each_possible_cpu(i) {
+ 		int j;
+ 
+ 		user = cputime64_add(user, kstat_cpu(i).cpustat.user);
+ 		nice = cputime64_add(nice, kstat_cpu(i).cpustat.nice);
+ 		system = cputime64_add(system, kstat_cpu(i).cpustat.system);
+ 		idle = cputime64_add(idle, kstat_cpu(i).cpustat.idle);
+ 		iowait = cputime64_add(iowait, kstat_cpu(i).cpustat.iowait);
+ 		irq = cputime64_add(irq, kstat_cpu(i).cpustat.irq);
+ 		softirq = cputime64_add(softirq, kstat_cpu(i).cpustat.softirq);
+ 		steal = cputime64_add(steal, kstat_cpu(i).cpustat.steal);
+ 		for (j = 0; j < NR_IRQS; j++) {
+ 			unsigned int temp = kstat_cpu(i).irqs[j];
+ 			sum += temp;
+ 			per_irq_sum[j] += temp;
+ 		}
+ 	}
+ 
+ 	seq_printf(p, "cpu  %llu %llu %llu %llu %llu %llu %llu %llu\n",
+ 		(unsigned long long)cputime64_to_clock_t(user),
+ 		(unsigned long long)cputime64_to_clock_t(nice),
+ 		(unsigned long long)cputime64_to_clock_t(system),
+ 		(unsigned long long)cputime64_to_clock_t(idle),
+ 		(unsigned long long)cputime64_to_clock_t(iowait),
+ 		(unsigned long long)cputime64_to_clock_t(irq),
+ 		(unsigned long long)cputime64_to_clock_t(softirq),
+ 		(unsigned long long)cputime64_to_clock_t(steal));
+ 	for_each_online_cpu(i) {
+ 
+ 		/* Copy values here to work around gcc-2.95.3, gcc-2.96 */
+ 		user = kstat_cpu(i).cpustat.user;
+ 		nice = kstat_cpu(i).cpustat.nice;
+ 		system = kstat_cpu(i).cpustat.system;
+ 		idle = kstat_cpu(i).cpustat.idle;
+ 		iowait = kstat_cpu(i).cpustat.iowait;
+ 		irq = kstat_cpu(i).cpustat.irq;
+ 		softirq = kstat_cpu(i).cpustat.softirq;
+ 		steal = kstat_cpu(i).cpustat.steal;
+ 		seq_printf(p, "cpu%d %llu %llu %llu %llu %llu %llu %llu %llu\n",
+ 			i,
+ 			(unsigned long long)cputime64_to_clock_t(user),
+ 			(unsigned long long)cputime64_to_clock_t(nice),
+ 			(unsigned long long)cputime64_to_clock_t(system),
+ 			(unsigned long long)cputime64_to_clock_t(idle),
+ 			(unsigned long long)cputime64_to_clock_t(iowait),
+ 			(unsigned long long)cputime64_to_clock_t(irq),
+ 			(unsigned long long)cputime64_to_clock_t(softirq),
+ 			(unsigned long long)cputime64_to_clock_t(steal));
+ 	}
+ 	seq_printf(p, "intr %llu", (unsigned long long)sum);
+ 
+ #ifndef CONFIG_SMP
+ 	/* Touches too many cache lines on SMP setups */
+ 	for (i = 0; i < NR_IRQS; i++)
+ 		seq_printf(p, " %u", per_irq_sum[i]);
+ #endif
+ 
+ 	seq_printf(p,
+ 		"\nctxt %llu\n"
+ 		"btime %lu\n"
+ 		"processes %lu\n"
+ 		"procs_running %lu\n"
+ 		"procs_blocked %lu\n",
+ 		nr_context_switches(),
+ 		(unsigned long)jif,
+ 		total_forks,
+ 		nr_running(),
+ 		nr_iowait());
+ 
+ 	kfree(per_irq_sum);
+ 	return 0;
+ }
+ 
+ static int stat_open(struct inode *inode, struct file *file)
+ {
+ 	unsigned size = 4096 * (1 + num_possible_cpus() / 32);
+ 	char *buf;
+ 	struct seq_file *m;
+ 	int res;
+ 
+ 	/* don't ask for more than the kmalloc() max size, currently 128 KB */
+ 	if (size > 128 * 1024)
+ 		size = 128 * 1024;
+ 	buf = kmalloc(size, GFP_KERNEL);
+ 	if (!buf)
+ 		return -ENOMEM;
+ 
+ 	res = single_open(file, show_stat, NULL);
+ 	if (!res) {
+ 		m = file->private_data;
+ 		m->buf = buf;
+ 		m->size = size;
+ 	} else
+ 		kfree(buf);
+ 	return res;
+ }
+ static const struct file_operations proc_stat_operations = {
+ 	.open		= stat_open,
+ 	.read		= seq_read,
+ 	.llseek		= seq_lseek,
+ 	.release	= single_release,
+ };
+ 
+ /*
+  * /proc/interrupts
+  */
+ static void *int_seq_start(struct seq_file *f, loff_t *pos)
+ {
+ 	return (*pos <= NR_IRQS) ? pos : NULL;
+ }
+ 
+ static void *int_seq_next(struct seq_file *f, void *v, loff_t *pos)
+ {
+ 	(*pos)++;
+ 	if (*pos > NR_IRQS)
+ 		return NULL;
+ 	return pos;
+ }
+ 
+ static void int_seq_stop(struct seq_file *f, void *v)
+ {
+ 	/* Nothing to do */
+ }
+ 
+ 
+ extern int show_interrupts(struct seq_file *f, void *v); /* In arch code */
+ static struct seq_operations int_seq_ops = {
+ 	.start = int_seq_start,
+ 	.next  = int_seq_next,
+ 	.stop  = int_seq_stop,
+ 	.show  = show_interrupts
+ };
+ 
+ static int interrupts_open(struct inode *inode, struct file *filp)
+ {
+ 	return seq_open(filp, &int_seq_ops);
+ }
+ 
+ static const struct file_operations proc_interrupts_operations = {
+ 	.open		= interrupts_open,
+ 	.read		= seq_read,
+ 	.llseek		= seq_lseek,
+ 	.release	= seq_release,
+ };
+ 
+ static int filesystems_read_proc(char *page, char **start, off_t off,
+ 				 int count, int *eof, void *data)
+ {
+ 	int len = get_filesystem_list(page);
+ 	return proc_calc_metrics(page, start, off, count, eof, len);
+ }
+ 
+ static int cmdline_read_proc(char *page, char **start, off_t off,
+ 				 int count, int *eof, void *data)
+ {
+ 	int len;
+ 
+ 	len = sprintf(page, "%s\n", saved_command_line);
+ 	return proc_calc_metrics(page, start, off, count, eof, len);
+ }
+ 
+ static int locks_read_proc(char *page, char **start, off_t off,
+ 				 int count, int *eof, void *data)
+ {
+ 	int len = get_locks_status(page, start, off, count);
+ 
+ 	if (len < count)
+ 		*eof = 1;
+ 	return len;
+ }
+ 
+ static int execdomains_read_proc(char *page, char **start, off_t off,
+ 				 int count, int *eof, void *data)
+ {
+ 	int len = get_exec_domain_list(page);
+ 	return proc_calc_metrics(page, start, off, count, eof, len);
+ }
+ 
+ #ifdef CONFIG_MAGIC_SYSRQ
+ /*
+  * writing 'C' to /proc/sysrq-trigger is like sysrq-C
+  */
+ static ssize_t write_sysrq_trigger(struct file *file, const char __user *buf,
+ 				   size_t count, loff_t *ppos)
+ {
+ 	if (count) {
+ 		char c;
+ 
+ 		if (get_user(c, buf))
+ 			return -EFAULT;
+ 		__handle_sysrq(c, NULL, 0);
+ 	}
+ 	return count;
+ }
+ 
+ static const struct file_operations proc_sysrq_trigger_operations = {
+ 	.write		= write_sysrq_trigger,
+ };
+ #endif
+ 
+ struct proc_dir_entry *proc_root_kcore;
+ 
+ void create_seq_entry(char *name, mode_t mode, const struct file_operations *f)
+ {
+ 	struct proc_dir_entry *entry;
+ 	entry = create_proc_entry(name, mode, NULL);
+ 	if (entry)
+ 		entry->proc_fops = f;
+ }
+ 
+ void __init proc_misc_init(void)
+ {
+ 	static struct {
+ 		char *name;
+ 		int (*read_proc)(char*,char**,off_t,int,int*,void*);
+ 	} *p, simple_ones[] = {
+ 		{"loadavg",     loadavg_read_proc},
+ 		{"uptime",	uptime_read_proc},
+ 		{"meminfo",	meminfo_read_proc},
+ 		{"version",	version_read_proc},
+ #ifdef CONFIG_PROC_HARDWARE
+ 		{"hardware",	hardware_read_proc},
+ #endif
+ #ifdef CONFIG_STRAM_PROC
+ 		{"stram",	stram_read_proc},
+ #endif
+ 		{"filesystems",	filesystems_read_proc},
+ 		{"cmdline",	cmdline_read_proc},
+ 		{"locks",	locks_read_proc},
+ 		{"execdomains",	execdomains_read_proc},
+ 		{NULL,}
+ 	};
+ 	for (p = simple_ones; p->name; p++)
+ 		create_proc_read_entry(p->name, 0, NULL, p->read_proc, NULL);
+ 
+ 	proc_symlink("mounts", NULL, "self/mounts");
+ 
+ 	/* And now for trickier ones */
+ #ifdef CONFIG_PRINTK
+ 	{
+ 		struct proc_dir_entry *entry;
+ 		entry = create_proc_entry("kmsg", S_IRUSR, &proc_root);
+ 		if (entry)
+ 			entry->proc_fops = &proc_kmsg_operations;
+ 	}
+ #endif
+ 	create_seq_entry("devices", 0, &proc_devinfo_operations);
+ 	create_seq_entry("cpuinfo", 0, &proc_cpuinfo_operations);
+ #ifdef CONFIG_BLOCK
+ 	create_seq_entry("partitions", 0, &proc_partitions_operations);
+ #endif
+ 	create_seq_entry("stat", 0, &proc_stat_operations);
+ 	create_seq_entry("interrupts", 0, &proc_interrupts_operations);
+ #ifdef CONFIG_SLAB
+ 	create_seq_entry("slabinfo",S_IWUSR|S_IRUGO,&proc_slabinfo_operations);
+ #ifdef CONFIG_DEBUG_SLAB_LEAK
+ 	create_seq_entry("slab_allocators", 0 ,&proc_slabstats_operations);
+ #endif
+ #endif
+ 	create_seq_entry("buddyinfo",S_IRUGO, &fragmentation_file_operations);
+ 	create_seq_entry("vmstat",S_IRUGO, &proc_vmstat_file_operations);
+ 	create_seq_entry("zoneinfo",S_IRUGO, &proc_zoneinfo_file_operations);
+ #ifdef CONFIG_BLOCK
+ 	create_seq_entry("diskstats", 0, &proc_diskstats_operations);
+ #endif
+ #ifdef CONFIG_MODULES
+ 	create_seq_entry("modules", 0, &proc_modules_operations);
+ #endif
+ #ifdef CONFIG_SCHEDSTATS
+ 	create_seq_entry("schedstat", 0, &proc_schedstat_operations);
+ #endif
+ #ifdef CONFIG_PROC_KCORE
+ 	proc_root_kcore = create_proc_entry("kcore", S_IRUSR, NULL);
+ 	if (proc_root_kcore) {
+ 		proc_root_kcore->proc_fops = &proc_kcore_operations;
+ 		proc_root_kcore->size =
+ 				(size_t)high_memory - PAGE_OFFSET + PAGE_SIZE;
+ 	}
+ #endif
+ #ifdef CONFIG_PROC_VMCORE
+ 	proc_vmcore = create_proc_entry("vmcore", S_IRUSR, NULL);
+ 	if (proc_vmcore)
+ 		proc_vmcore->proc_fops = &proc_vmcore_operations;
+ #endif
+ #ifdef CONFIG_MAGIC_SYSRQ
+ 	{
+ 		struct proc_dir_entry *entry;
+ 		entry = create_proc_entry("sysrq-trigger", S_IWUSR, NULL);
+ 		if (entry)
+ 			entry->proc_fops = &proc_sysrq_trigger_operations;
+ 	}
+ #endif
+ }
diff -Ncwr -x .orig -x .rej linux-2.6.23.1/include/linux/slab.h linux-2.6.23.1km/include/linux/slab.h
*** linux-2.6.23.1/include/linux/slab.h	2007-10-13 01:43:44.000000000 +0900
--- linux-2.6.23.1km/include/linux/slab.h	2008-02-05 20:54:56.000000000 +0900
***************
*** 43,48 ****
--- 43,65 ----
  #define ZERO_OR_NULL_PTR(x) ((unsigned long)(x) <= \
  				(unsigned long)ZERO_SIZE_PTR)
  
+ #ifdef CONFIG_KMALLOC_ACCOUNTING
+ void __kmalloc_account(const void *, const void *, int, int);
+ 
+ static void inline kmalloc_account(const void *addr, int size, int req)
+ {
+ 	__kmalloc_account(__builtin_return_address(0), addr, size, req);
+ }
+ 
+ static void inline kfree_account(const void *addr, int size)
+ {
+ 	__kmalloc_account(__builtin_return_address(0), addr, size, -1);
+ }
+ #else
+ #define kmalloc_account(a, b, c)
+ #define kfree_account(a, b)
+ #endif
+ 
  /*
   * struct kmem_cache related prototypes
   */
diff -Ncwr -x .orig -x .rej linux-2.6.23.1/include/linux/slab.h.orig linux-2.6.23.1km/include/linux/slab.h.orig
*** linux-2.6.23.1/include/linux/slab.h.orig	1970-01-01 09:00:00.000000000 +0900
--- linux-2.6.23.1km/include/linux/slab.h.orig	2007-10-13 01:43:44.000000000 +0900
***************
*** 0 ****
--- 1,273 ----
+ /*
+  * Written by Mark Hemment, 1996 (markhe@nextd.demon.co.uk).
+  *
+  * (C) SGI 2006, Christoph Lameter <clameter@sgi.com>
+  * 	Cleaned up and restructured to ease the addition of alternative
+  * 	implementations of SLAB allocators.
+  */
+ 
+ #ifndef _LINUX_SLAB_H
+ #define	_LINUX_SLAB_H
+ 
+ #ifdef __KERNEL__
+ 
+ #include <linux/gfp.h>
+ #include <linux/types.h>
+ 
+ /*
+  * Flags to pass to kmem_cache_create().
+  * The ones marked DEBUG are only valid if CONFIG_SLAB_DEBUG is set.
+  */
+ #define SLAB_DEBUG_FREE		0x00000100UL	/* DEBUG: Perform (expensive) checks on free */
+ #define SLAB_RED_ZONE		0x00000400UL	/* DEBUG: Red zone objs in a cache */
+ #define SLAB_POISON		0x00000800UL	/* DEBUG: Poison objects */
+ #define SLAB_HWCACHE_ALIGN	0x00002000UL	/* Align objs on cache lines */
+ #define SLAB_CACHE_DMA		0x00004000UL	/* Use GFP_DMA memory */
+ #define SLAB_STORE_USER		0x00010000UL	/* DEBUG: Store the last owner for bug hunting */
+ #define SLAB_RECLAIM_ACCOUNT	0x00020000UL	/* Objects are reclaimable */
+ #define SLAB_PANIC		0x00040000UL	/* Panic if kmem_cache_create() fails */
+ #define SLAB_DESTROY_BY_RCU	0x00080000UL	/* Defer freeing slabs to RCU */
+ #define SLAB_MEM_SPREAD		0x00100000UL	/* Spread some memory over cpuset */
+ #define SLAB_TRACE		0x00200000UL	/* Trace allocations and frees */
+ 
+ /*
+  * ZERO_SIZE_PTR will be returned for zero sized kmalloc requests.
+  *
+  * Dereferencing ZERO_SIZE_PTR will lead to a distinct access fault.
+  *
+  * ZERO_SIZE_PTR can be passed to kfree though in the same way that NULL can.
+  * Both make kfree a no-op.
+  */
+ #define ZERO_SIZE_PTR ((void *)16)
+ 
+ #define ZERO_OR_NULL_PTR(x) ((unsigned long)(x) <= \
+ 				(unsigned long)ZERO_SIZE_PTR)
+ 
+ /*
+  * struct kmem_cache related prototypes
+  */
+ void __init kmem_cache_init(void);
+ int slab_is_available(void);
+ 
+ struct kmem_cache *kmem_cache_create(const char *, size_t, size_t,
+ 			unsigned long,
+ 			void (*)(void *, struct kmem_cache *, unsigned long));
+ void kmem_cache_destroy(struct kmem_cache *);
+ int kmem_cache_shrink(struct kmem_cache *);
+ void kmem_cache_free(struct kmem_cache *, void *);
+ unsigned int kmem_cache_size(struct kmem_cache *);
+ const char *kmem_cache_name(struct kmem_cache *);
+ int kmem_ptr_validate(struct kmem_cache *cachep, const void *ptr);
+ 
+ /*
+  * Please use this macro to create slab caches. Simply specify the
+  * name of the structure and maybe some flags that are listed above.
+  *
+  * The alignment of the struct determines object alignment. If you
+  * f.e. add ____cacheline_aligned_in_smp to the struct declaration
+  * then the objects will be properly aligned in SMP configurations.
+  */
+ #define KMEM_CACHE(__struct, __flags) kmem_cache_create(#__struct,\
+ 		sizeof(struct __struct), __alignof__(struct __struct),\
+ 		(__flags), NULL)
+ 
+ /*
+  * The largest kmalloc size supported by the slab allocators is
+  * 32 megabyte (2^25) or the maximum allocatable page order if that is
+  * less than 32 MB.
+  *
+  * WARNING: Its not easy to increase this value since the allocators have
+  * to do various tricks to work around compiler limitations in order to
+  * ensure proper constant folding.
+  */
+ #define KMALLOC_SHIFT_HIGH	((MAX_ORDER + PAGE_SHIFT - 1) <= 25 ? \
+ 				(MAX_ORDER + PAGE_SHIFT - 1) : 25)
+ 
+ #define KMALLOC_MAX_SIZE	(1UL << KMALLOC_SHIFT_HIGH)
+ #define KMALLOC_MAX_ORDER	(KMALLOC_SHIFT_HIGH - PAGE_SHIFT)
+ 
+ /*
+  * Common kmalloc functions provided by all allocators
+  */
+ void * __must_check krealloc(const void *, size_t, gfp_t);
+ void kfree(const void *);
+ size_t ksize(const void *);
+ 
+ /*
+  * Allocator specific definitions. These are mainly used to establish optimized
+  * ways to convert kmalloc() calls to kmem_cache_alloc() invocations by
+  * selecting the appropriate general cache at compile time.
+  *
+  * Allocators must define at least:
+  *
+  *	kmem_cache_alloc()
+  *	__kmalloc()
+  *	kmalloc()
+  *
+  * Those wishing to support NUMA must also define:
+  *
+  *	kmem_cache_alloc_node()
+  *	kmalloc_node()
+  *
+  * See each allocator definition file for additional comments and
+  * implementation notes.
+  */
+ #ifdef CONFIG_SLUB
+ #include <linux/slub_def.h>
+ #elif defined(CONFIG_SLOB)
+ #include <linux/slob_def.h>
+ #else
+ #include <linux/slab_def.h>
+ #endif
+ 
+ /**
+  * kcalloc - allocate memory for an array. The memory is set to zero.
+  * @n: number of elements.
+  * @size: element size.
+  * @flags: the type of memory to allocate.
+  *
+  * The @flags argument may be one of:
+  *
+  * %GFP_USER - Allocate memory on behalf of user.  May sleep.
+  *
+  * %GFP_KERNEL - Allocate normal kernel ram.  May sleep.
+  *
+  * %GFP_ATOMIC - Allocation will not sleep.  May use emergency pools.
+  *   For example, use this inside interrupt handlers.
+  *
+  * %GFP_HIGHUSER - Allocate pages from high memory.
+  *
+  * %GFP_NOIO - Do not do any I/O at all while trying to get memory.
+  *
+  * %GFP_NOFS - Do not make any fs calls while trying to get memory.
+  *
+  * %GFP_NOWAIT - Allocation will not sleep.
+  *
+  * %GFP_THISNODE - Allocate node-local memory only.
+  *
+  * %GFP_DMA - Allocation suitable for DMA.
+  *   Should only be used for kmalloc() caches. Otherwise, use a
+  *   slab created with SLAB_DMA.
+  *
+  * Also it is possible to set different flags by OR'ing
+  * in one or more of the following additional @flags:
+  *
+  * %__GFP_COLD - Request cache-cold pages instead of
+  *   trying to return cache-warm pages.
+  *
+  * %__GFP_HIGH - This allocation has high priority and may use emergency pools.
+  *
+  * %__GFP_NOFAIL - Indicate that this allocation is in no way allowed to fail
+  *   (think twice before using).
+  *
+  * %__GFP_NORETRY - If memory is not immediately available,
+  *   then give up at once.
+  *
+  * %__GFP_NOWARN - If allocation fails, don't issue any warnings.
+  *
+  * %__GFP_REPEAT - If allocation fails initially, try once more before failing.
+  *
+  * There are other flags available as well, but these are not intended
+  * for general use, and so are not documented here. For a full list of
+  * potential flags, always refer to linux/gfp.h.
+  */
+ static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
+ {
+ 	if (n != 0 && size > ULONG_MAX / n)
+ 		return NULL;
+ 	return __kmalloc(n * size, flags | __GFP_ZERO);
+ }
+ 
+ #if !defined(CONFIG_NUMA) && !defined(CONFIG_SLOB)
+ /**
+  * kmalloc_node - allocate memory from a specific node
+  * @size: how many bytes of memory are required.
+  * @flags: the type of memory to allocate (see kcalloc).
+  * @node: node to allocate from.
+  *
+  * kmalloc() for non-local nodes, used to allocate from a specific node
+  * if available. Equivalent to kmalloc() in the non-NUMA single-node
+  * case.
+  */
+ static inline void *kmalloc_node(size_t size, gfp_t flags, int node)
+ {
+ 	return kmalloc(size, flags);
+ }
+ 
+ static inline void *__kmalloc_node(size_t size, gfp_t flags, int node)
+ {
+ 	return __kmalloc(size, flags);
+ }
+ 
+ void *kmem_cache_alloc(struct kmem_cache *, gfp_t);
+ 
+ static inline void *kmem_cache_alloc_node(struct kmem_cache *cachep,
+ 					gfp_t flags, int node)
+ {
+ 	return kmem_cache_alloc(cachep, flags);
+ }
+ #endif /* !CONFIG_NUMA && !CONFIG_SLOB */
+ 
+ /*
+  * kmalloc_track_caller is a special version of kmalloc that records the
+  * calling function of the routine calling it for slab leak tracking instead
+  * of just the calling function (confusing, eh?).
+  * It's useful when the call to kmalloc comes from a widely-used standard
+  * allocator where we care about the real place the memory allocation
+  * request comes from.
+  */
+ #if defined(CONFIG_DEBUG_SLAB) || defined(CONFIG_SLUB)
+ extern void *__kmalloc_track_caller(size_t, gfp_t, void*);
+ #define kmalloc_track_caller(size, flags) \
+ 	__kmalloc_track_caller(size, flags, __builtin_return_address(0))
+ #else
+ #define kmalloc_track_caller(size, flags) \
+ 	__kmalloc(size, flags)
+ #endif /* DEBUG_SLAB */
+ 
+ #ifdef CONFIG_NUMA
+ /*
+  * kmalloc_node_track_caller is a special version of kmalloc_node that
+  * records the calling function of the routine calling it for slab leak
+  * tracking instead of just the calling function (confusing, eh?).
+  * It's useful when the call to kmalloc_node comes from a widely-used
+  * standard allocator where we care about the real place the memory
+  * allocation request comes from.
+  */
+ #if defined(CONFIG_DEBUG_SLAB) || defined(CONFIG_SLUB)
+ extern void *__kmalloc_node_track_caller(size_t, gfp_t, int, void *);
+ #define kmalloc_node_track_caller(size, flags, node) \
+ 	__kmalloc_node_track_caller(size, flags, node, \
+ 			__builtin_return_address(0))
+ #else
+ #define kmalloc_node_track_caller(size, flags, node) \
+ 	__kmalloc_node(size, flags, node)
+ #endif
+ 
+ #else /* CONFIG_NUMA */
+ 
+ #define kmalloc_node_track_caller(size, flags, node) \
+ 	kmalloc_track_caller(size, flags)
+ 
+ #endif /* DEBUG_SLAB */
+ 
+ /*
+  * Shortcuts
+  */
+ static inline void *kmem_cache_zalloc(struct kmem_cache *k, gfp_t flags)
+ {
+ 	return kmem_cache_alloc(k, flags | __GFP_ZERO);
+ }
+ 
+ /**
+  * kzalloc - allocate memory. The memory is set to zero.
+  * @size: how many bytes of memory are required.
+  * @flags: the type of memory to allocate (see kmalloc).
+  */
+ static inline void *kzalloc(size_t size, gfp_t flags)
+ {
+ 	return kmalloc(size, flags | __GFP_ZERO);
+ }
+ 
+ #endif	/* __KERNEL__ */
+ #endif	/* _LINUX_SLAB_H */
diff -Ncwr -x .orig -x .rej linux-2.6.23.1/include/linux/slab.h.rej linux-2.6.23.1km/include/linux/slab.h.rej
*** linux-2.6.23.1/include/linux/slab.h.rej	1970-01-01 09:00:00.000000000 +0900
--- linux-2.6.23.1km/include/linux/slab.h.rej	2008-02-05 20:48:38.000000000 +0900
***************
*** 0 ****
--- 1,64 ----
+ ***************
+ *** 54,59 ****
+   
+   #ifndef CONFIG_SLOB
+   
+   /* prototypes */
+   extern void __init kmem_cache_init(void);
+   
+ --- 54,76 ----
+   
+   #ifndef CONFIG_SLOB
+   
+ + #ifdef CONFIG_KMALLOC_ACCOUNTING
+ + void __kmalloc_account(const void *, const void *, int, int);
+ + 
+ + static void inline kmalloc_account(const void *addr, int size, int req)
+ + {
+ + 	__kmalloc_account(__builtin_return_address(0), addr, size, req);
+ + }
+ + 
+ + static void inline kfree_account(const void *addr, int size)
+ + {
+ + 	__kmalloc_account(__builtin_return_address(0), addr, size, -1);
+ + }
+ + #else
+ + #define kmalloc_account(a, b, c)
+ + #define kfree_account(a, b)
+ + #endif
+ + 
+   /* prototypes */
+   extern void __init kmem_cache_init(void);
+   
+ ***************
+ *** 133,138 ****
+    */
+   static inline void *kmalloc(size_t size, gfp_t flags)
+   {
+   	if (__builtin_constant_p(size)) {
+   		int i = 0;
+   #define CACHE(x) \
+ --- 150,156 ----
+    */
+   static inline void *kmalloc(size_t size, gfp_t flags)
+   {
+ + #ifndef CONFIG_KMALLOC_ACCOUNTING
+   	if (__builtin_constant_p(size)) {
+   		int i = 0;
+   #define CACHE(x) \
+ ***************
+ *** 151,156 ****
+   			malloc_sizes[i].cs_dmacachep :
+   			malloc_sizes[i].cs_cachep, flags);
+   	}
+   	return __kmalloc(size, flags);
+   }
+   
+ --- 169,175 ----
+   			malloc_sizes[i].cs_dmacachep :
+   			malloc_sizes[i].cs_cachep, flags);
+   	}
+ + #endif
+   	return __kmalloc(size, flags);
+   }
+   
diff -Ncwr -x .orig -x .rej linux-2.6.23.1/include/linux/slab_def.h linux-2.6.23.1km/include/linux/slab_def.h
*** linux-2.6.23.1/include/linux/slab_def.h	2007-10-13 01:43:44.000000000 +0900
--- linux-2.6.23.1km/include/linux/slab_def.h	2008-02-05 20:57:51.000000000 +0900
***************
*** 30,35 ****
--- 30,36 ----
  
  static inline void *kmalloc(size_t size, gfp_t flags)
  {
+ #ifndef CONFIG_KMALLOC_ACCOUNTING
  	if (__builtin_constant_p(size)) {
  		int i = 0;
  
***************
*** 55,60 ****
--- 56,62 ----
  #endif
  		return kmem_cache_alloc(malloc_sizes[i].cs_cachep, flags);
  	}
+ #endif
  	return __kmalloc(size, flags);
  }
  
diff -Ncwr -x .orig -x .rej linux-2.6.23.1/include/linux/slub_def.h linux-2.6.23.1km/include/linux/slub_def.h
*** linux-2.6.23.1/include/linux/slub_def.h	2007-10-13 01:43:44.000000000 +0900
--- linux-2.6.23.1km/include/linux/slub_def.h	2008-02-05 21:32:41.000000000 +0900
***************
*** 168,173 ****
--- 168,174 ----
  
  static __always_inline void *kmalloc(size_t size, gfp_t flags)
  {
+ #ifndef CONFIG_KMALLOC_ACCOUNTING
  	if (__builtin_constant_p(size) && !(flags & SLUB_DMA)) {
  		struct kmem_cache *s = kmalloc_slab(size);
  
***************
*** 176,181 ****
--- 177,183 ----
  
  		return kmem_cache_alloc(s, flags);
  	} else
+ #endif
  		return __kmalloc(size, flags);
  }
  
diff -Ncwr -x .orig -x .rej linux-2.6.23.1/init/Kconfig linux-2.6.23.1km/init/Kconfig
*** linux-2.6.23.1/init/Kconfig	2007-10-13 01:43:44.000000000 +0900
--- linux-2.6.23.1km/init/Kconfig	2008-02-05 20:48:38.000000000 +0900
***************
*** 455,460 ****
--- 455,467 ----
  	  kernel data structures. This saves memory on small machines,
  	  but may reduce performance.
  
+ config KMALLOC_ACCOUNTING
+ 	default n
+ 	bool "Enabled accounting of kmalloc/kfree allocations" if EMBEDDED
+ 	help
+ 	  This option records kmalloc and kfree activity and reports it via
+ 	  /proc/kmalloc.
+ 
  config FUTEX
  	bool "Enable futex support" if EMBEDDED
  	default y
diff -Ncwr -x .orig -x .rej linux-2.6.23.1/init/Kconfig.orig linux-2.6.23.1km/init/Kconfig.orig
*** linux-2.6.23.1/init/Kconfig.orig	1970-01-01 09:00:00.000000000 +0900
--- linux-2.6.23.1km/init/Kconfig.orig	2007-10-13 01:43:44.000000000 +0900
***************
*** 0 ****
--- 1,670 ----
+ config DEFCONFIG_LIST
+ 	string
+ 	depends on !UML
+ 	option defconfig_list
+ 	default "/lib/modules/$UNAME_RELEASE/.config"
+ 	default "/etc/kernel-config"
+ 	default "/boot/config-$UNAME_RELEASE"
+ 	default "arch/$ARCH/defconfig"
+ 
+ menu "General setup"
+ 
+ config EXPERIMENTAL
+ 	bool "Prompt for development and/or incomplete code/drivers"
+ 	---help---
+ 	  Some of the various things that Linux supports (such as network
+ 	  drivers, file systems, network protocols, etc.) can be in a state
+ 	  of development where the functionality, stability, or the level of
+ 	  testing is not yet high enough for general use. This is usually
+ 	  known as the "alpha-test" phase among developers. If a feature is
+ 	  currently in alpha-test, then the developers usually discourage
+ 	  uninformed widespread use of this feature by the general public to
+ 	  avoid "Why doesn't this work?" type mail messages. However, active
+ 	  testing and use of these systems is welcomed. Just be aware that it
+ 	  may not meet the normal level of reliability or it may fail to work
+ 	  in some special cases. Detailed bug reports from people familiar
+ 	  with the kernel internals are usually welcomed by the developers
+ 	  (before submitting bug reports, please read the documents
+ 	  <file:README>, <file:MAINTAINERS>, <file:REPORTING-BUGS>,
+ 	  <file:Documentation/BUG-HUNTING>, and
+ 	  <file:Documentation/oops-tracing.txt> in the kernel source).
+ 
+ 	  This option will also make obsoleted drivers available. These are
+ 	  drivers that have been replaced by something else, and/or are
+ 	  scheduled to be removed in a future kernel release.
+ 
+ 	  Unless you intend to help test and develop a feature or driver that
+ 	  falls into this category, or you have a situation that requires
+ 	  using these features, you should probably say N here, which will
+ 	  cause the configurator to present you with fewer choices. If
+ 	  you say Y here, you will be offered the choice of using features or
+ 	  drivers that are currently considered to be in the alpha-test phase.
+ 
+ config BROKEN
+ 	bool
+ 
+ config BROKEN_ON_SMP
+ 	bool
+ 	depends on BROKEN || !SMP
+ 	default y
+ 
+ config LOCK_KERNEL
+ 	bool
+ 	depends on SMP || PREEMPT
+ 	default y
+ 
+ config INIT_ENV_ARG_LIMIT
+ 	int
+ 	default 32 if !UML
+ 	default 128 if UML
+ 	help
+ 	  Maximum of each of the number of arguments and environment
+ 	  variables passed to init from the kernel command line.
+ 
+ 
+ config LOCALVERSION
+ 	string "Local version - append to kernel release"
+ 	help
+ 	  Append an extra string to the end of your kernel version.
+ 	  This will show up when you type uname, for example.
+ 	  The string you set here will be appended after the contents of
+ 	  any files with a filename matching localversion* in your
+ 	  object and source tree, in that order.  Your total string can
+ 	  be a maximum of 64 characters.
+ 
+ config LOCALVERSION_AUTO
+ 	bool "Automatically append version information to the version string"
+ 	default y
+ 	help
+ 	  This will try to automatically determine if the current tree is a
+ 	  release tree by looking for git tags that belong to the current
+ 	  top of tree revision.
+ 
+ 	  A string of the format -gxxxxxxxx will be added to the localversion
+ 	  if a git-based tree is found.  The string generated by this will be
+ 	  appended after any matching localversion* files, and after the value
+ 	  set in CONFIG_LOCALVERSION.
+ 
+ 	  (The actual string used here is the first eight characters produced
+ 	  by running the command:
+ 
+ 	    $ git rev-parse --verify HEAD
+ 
+ 	  which is done within the script "scripts/setlocalversion".)
+ 
+ config SWAP
+ 	bool "Support for paging of anonymous memory (swap)"
+ 	depends on MMU && BLOCK
+ 	default y
+ 	help
+ 	  This option allows you to choose whether you want to have support
+ 	  for so called swap devices or swap files in your kernel that are
+ 	  used to provide more virtual memory than the actual RAM present
+ 	  in your computer.  If unsure say Y.
+ 
+ config SYSVIPC
+ 	bool "System V IPC"
+ 	---help---
+ 	  Inter Process Communication is a suite of library functions and
+ 	  system calls which let processes (running programs) synchronize and
+ 	  exchange information. It is generally considered to be a good thing,
+ 	  and some programs won't run unless you say Y here. In particular, if
+ 	  you want to run the DOS emulator dosemu under Linux (read the
+ 	  DOSEMU-HOWTO, available from <http://www.tldp.org/docs.html#howto>),
+ 	  you'll need to say Y here.
+ 
+ 	  You can find documentation about IPC with "info ipc" and also in
+ 	  section 6.4 of the Linux Programmer's Guide, available from
+ 	  <http://www.tldp.org/guides.html>.
+ 
+ config SYSVIPC_SYSCTL
+ 	bool
+ 	depends on SYSVIPC
+ 	depends on SYSCTL
+ 	default y
+ 
+ config POSIX_MQUEUE
+ 	bool "POSIX Message Queues"
+ 	depends on NET && EXPERIMENTAL
+ 	---help---
+ 	  POSIX variant of message queues is a part of IPC. In POSIX message
+ 	  queues every message has a priority which decides about succession
+ 	  of receiving it by a process. If you want to compile and run
+ 	  programs written e.g. for Solaris with use of its POSIX message
+ 	  queues (functions mq_*) say Y here.
+ 
+ 	  POSIX message queues are visible as a filesystem called 'mqueue'
+ 	  and can be mounted somewhere if you want to do filesystem
+ 	  operations on message queues.
+ 
+ 	  If unsure, say Y.
+ 
+ config BSD_PROCESS_ACCT
+ 	bool "BSD Process Accounting"
+ 	help
+ 	  If you say Y here, a user level program will be able to instruct the
+ 	  kernel (via a special system call) to write process accounting
+ 	  information to a file: whenever a process exits, information about
+ 	  that process will be appended to the file by the kernel.  The
+ 	  information includes things such as creation time, owning user,
+ 	  command name, memory usage, controlling terminal etc. (the complete
+ 	  list is in the struct acct in <file:include/linux/acct.h>).  It is
+ 	  up to the user level program to do useful things with this
+ 	  information.  This is generally a good idea, so say Y.
+ 
+ config BSD_PROCESS_ACCT_V3
+ 	bool "BSD Process Accounting version 3 file format"
+ 	depends on BSD_PROCESS_ACCT
+ 	default n
+ 	help
+ 	  If you say Y here, the process accounting information is written
+ 	  in a new file format that also logs the process IDs of each
+ 	  process and it's parent. Note that this file format is incompatible
+ 	  with previous v0/v1/v2 file formats, so you will need updated tools
+ 	  for processing it. A preliminary version of these tools is available
+ 	  at <http://www.physik3.uni-rostock.de/tim/kernel/utils/acct/>.
+ 
+ config TASKSTATS
+ 	bool "Export task/process statistics through netlink (EXPERIMENTAL)"
+ 	depends on NET
+ 	default n
+ 	help
+ 	  Export selected statistics for tasks/processes through the
+ 	  generic netlink interface. Unlike BSD process accounting, the
+ 	  statistics are available during the lifetime of tasks/processes as
+ 	  responses to commands. Like BSD accounting, they are sent to user
+ 	  space on task exit.
+ 
+ 	  Say N if unsure.
+ 
+ config TASK_DELAY_ACCT
+ 	bool "Enable per-task delay accounting (EXPERIMENTAL)"
+ 	depends on TASKSTATS
+ 	help
+ 	  Collect information on time spent by a task waiting for system
+ 	  resources like cpu, synchronous block I/O completion and swapping
+ 	  in pages. Such statistics can help in setting a task's priorities
+ 	  relative to other tasks for cpu, io, rss limits etc.
+ 
+ 	  Say N if unsure.
+ 
+ config TASK_XACCT
+ 	bool "Enable extended accounting over taskstats (EXPERIMENTAL)"
+ 	depends on TASKSTATS
+ 	help
+ 	  Collect extended task accounting data and send the data
+ 	  to userland for processing over the taskstats interface.
+ 
+ 	  Say N if unsure.
+ 
+ config TASK_IO_ACCOUNTING
+ 	bool "Enable per-task storage I/O accounting (EXPERIMENTAL)"
+ 	depends on TASK_XACCT
+ 	help
+ 	  Collect information on the number of bytes of storage I/O which this
+ 	  task has caused.
+ 
+ 	  Say N if unsure.
+ 
+ config USER_NS
+ 	bool "User Namespaces (EXPERIMENTAL)"
+ 	default n
+ 	depends on EXPERIMENTAL
+ 	help
+ 	  Support user namespaces.  This allows containers, i.e.
+ 	  vservers, to use user namespaces to provide different
+ 	  user info for different servers.  If unsure, say N.
+ 
+ config AUDIT
+ 	bool "Auditing support"
+ 	depends on NET
+ 	help
+ 	  Enable auditing infrastructure that can be used with another
+ 	  kernel subsystem, such as SELinux (which requires this for
+ 	  logging of avc messages output).  Does not do system-call
+ 	  auditing without CONFIG_AUDITSYSCALL.
+ 
+ config AUDITSYSCALL
+ 	bool "Enable system-call auditing support"
+ 	depends on AUDIT && (X86 || PPC || PPC64 || S390 || IA64 || UML || SPARC64)
+ 	default y if SECURITY_SELINUX
+ 	help
+ 	  Enable low-overhead system-call auditing infrastructure that
+ 	  can be used independently or with another kernel subsystem,
+ 	  such as SELinux.  To use audit's filesystem watch feature, please
+ 	  ensure that INOTIFY is configured.
+ 
+ config IKCONFIG
+ 	tristate "Kernel .config support"
+ 	---help---
+ 	  This option enables the complete Linux kernel ".config" file
+ 	  contents to be saved in the kernel. It provides documentation
+ 	  of which kernel options are used in a running kernel or in an
+ 	  on-disk kernel.  This information can be extracted from the kernel
+ 	  image file with the script scripts/extract-ikconfig and used as
+ 	  input to rebuild the current kernel or to build another kernel.
+ 	  It can also be extracted from a running kernel by reading
+ 	  /proc/config.gz if enabled (below).
+ 
+ config IKCONFIG_PROC
+ 	bool "Enable access to .config through /proc/config.gz"
+ 	depends on IKCONFIG && PROC_FS
+ 	---help---
+ 	  This option enables access to the kernel configuration file
+ 	  through /proc/config.gz.
+ 
+ config LOG_BUF_SHIFT
+ 	int "Kernel log buffer size (16 => 64KB, 17 => 128KB)"
+ 	range 12 21
+ 	default 17 if S390 || LOCKDEP
+ 	default 16 if X86_NUMAQ || IA64
+ 	default 15 if SMP
+ 	default 14
+ 	help
+ 	  Select kernel log buffer size as a power of 2.
+ 	  Defaults and Examples:
+ 	  	     17 => 128 KB for S/390
+ 		     16 => 64 KB for x86 NUMAQ or IA-64
+ 	             15 => 32 KB for SMP
+ 	             14 => 16 KB for uniprocessor
+ 		     13 =>  8 KB
+ 		     12 =>  4 KB
+ 
+ config CPUSETS
+ 	bool "Cpuset support"
+ 	depends on SMP
+ 	help
+ 	  This option will let you create and manage CPUSETs which
+ 	  allow dynamically partitioning a system into sets of CPUs and
+ 	  Memory Nodes and assigning tasks to run only within those sets.
+ 	  This is primarily useful on large SMP or NUMA systems.
+ 
+ 	  Say N if unsure.
+ 
+ config SYSFS_DEPRECATED
+ 	bool "Create deprecated sysfs files"
+ 	default y
+ 	help
+ 	  This option creates deprecated symlinks such as the
+ 	  "device"-link, the <subsystem>:<name>-link, and the
+ 	  "bus"-link. It may also add deprecated key in the
+ 	  uevent environment.
+ 	  None of these features or values should be used today, as
+ 	  they export driver core implementation details to userspace
+ 	  or export properties which can't be kept stable across kernel
+ 	  releases.
+ 
+ 	  If enabled, this option will also move any device structures
+ 	  that belong to a class, back into the /sys/class hierarchy, in
+ 	  order to support older versions of udev.
+ 
+ 	  If you are using a distro that was released in 2006 or later,
+ 	  it should be safe to say N here.
+ 
+ config RELAY
+ 	bool "Kernel->user space relay support (formerly relayfs)"
+ 	help
+ 	  This option enables support for relay interface support in
+ 	  certain file systems (such as debugfs).
+ 	  It is designed to provide an efficient mechanism for tools and
+ 	  facilities to relay large amounts of data from kernel space to
+ 	  user space.
+ 
+ 	  If unsure, say N.
+ 
+ config BLK_DEV_INITRD
+ 	bool "Initial RAM filesystem and RAM disk (initramfs/initrd) support"
+ 	depends on BROKEN || !FRV
+ 	help
+ 	  The initial RAM filesystem is a ramfs which is loaded by the
+ 	  boot loader (loadlin or lilo) and that is mounted as root
+ 	  before the normal boot procedure. It is typically used to
+ 	  load modules needed to mount the "real" root file system,
+ 	  etc. See <file:Documentation/initrd.txt> for details.
+ 
+ 	  If RAM disk support (BLK_DEV_RAM) is also included, this
+ 	  also enables initial RAM disk (initrd) support and adds
+ 	  15 Kbytes (more on some other architectures) to the kernel size.
+ 
+ 	  If unsure say Y.
+ 
+ if BLK_DEV_INITRD
+ 
+ source "usr/Kconfig"
+ 
+ endif
+ 
+ config CC_OPTIMIZE_FOR_SIZE
+ 	bool "Optimize for size (Look out for broken compilers!)"
+ 	default y
+ 	depends on ARM || H8300 || SUPERH || EXPERIMENTAL
+ 	help
+ 	  Enabling this option will pass "-Os" instead of "-O2" to gcc
+ 	  resulting in a smaller kernel.
+ 
+ 	  WARNING: some versions of gcc may generate incorrect code with this
+ 	  option.  If problems are observed, a gcc upgrade may be needed.
+ 
+ 	  If unsure, say N.
+ 
+ config SYSCTL
+ 	bool
+ 
+ menuconfig EMBEDDED
+ 	bool "Configure standard kernel features (for small systems)"
+ 	help
+ 	  This option allows certain base kernel options and settings
+           to be disabled or tweaked. This is for specialized
+           environments which can tolerate a "non-standard" kernel.
+           Only use this if you really know what you are doing.
+ 
+ config UID16
+ 	bool "Enable 16-bit UID system calls" if EMBEDDED
+ 	depends on ARM || BFIN || CRIS || FRV || H8300 || X86_32 || M68K || (S390 && !64BIT) || SUPERH || SPARC32 || (SPARC64 && SPARC32_COMPAT) || UML || (X86_64 && IA32_EMULATION)
+ 	default y
+ 	help
+ 	  This enables the legacy 16-bit UID syscall wrappers.
+ 
+ config SYSCTL_SYSCALL
+ 	bool "Sysctl syscall support" if EMBEDDED
+ 	default y
+ 	select SYSCTL
+ 	---help---
+ 	  sys_sysctl uses binary paths that have been found challenging
+ 	  to properly maintain and use.  The interface in /proc/sys
+ 	  using paths with ascii names is now the primary path to this
+ 	  information.
+ 
+ 	  Almost nothing using the binary sysctl interface so if you are
+ 	  trying to save some space it is probably safe to disable this,
+ 	  making your kernel marginally smaller.
+ 
+ 	  If unsure say Y here.
+ 
+ config KALLSYMS
+ 	 bool "Load all symbols for debugging/ksymoops" if EMBEDDED
+ 	 default y
+ 	 help
+ 	   Say Y here to let the kernel print out symbolic crash information and
+ 	   symbolic stack backtraces. This increases the size of the kernel
+ 	   somewhat, as all symbols have to be loaded into the kernel image.
+ 
+ config KALLSYMS_ALL
+ 	bool "Include all symbols in kallsyms"
+ 	depends on DEBUG_KERNEL && KALLSYMS
+ 	help
+ 	   Normally kallsyms only contains the symbols of functions, for nicer
+ 	   OOPS messages.  Some debuggers can use kallsyms for other
+ 	   symbols too: say Y here to include all symbols, if you need them 
+ 	   and you don't care about adding 300k to the size of your kernel.
+ 
+ 	   Say N.
+ 
+ config KALLSYMS_EXTRA_PASS
+ 	bool "Do an extra kallsyms pass"
+ 	depends on KALLSYMS
+ 	help
+ 	   If kallsyms is not working correctly, the build will fail with
+ 	   inconsistent kallsyms data.  If that occurs, log a bug report and
+ 	   turn on KALLSYMS_EXTRA_PASS which should result in a stable build.
+ 	   Always say N here unless you find a bug in kallsyms, which must be
+ 	   reported.  KALLSYMS_EXTRA_PASS is only a temporary workaround while
+ 	   you wait for kallsyms to be fixed.
+ 
+ 
+ config HOTPLUG
+ 	bool "Support for hot-pluggable devices" if EMBEDDED
+ 	default y
+ 	help
+ 	  This option is provided for the case where no hotplug or uevent
+ 	  capabilities is wanted by the kernel.  You should only consider
+ 	  disabling this option for embedded systems that do not use modules, a
+ 	  dynamic /dev tree, or dynamic device discovery.  Just say Y.
+ 
+ config PRINTK
+ 	default y
+ 	bool "Enable support for printk" if EMBEDDED
+ 	help
+ 	  This option enables normal printk support. Removing it
+ 	  eliminates most of the message strings from the kernel image
+ 	  and makes the kernel more or less silent. As this makes it
+ 	  very difficult to diagnose system problems, saying N here is
+ 	  strongly discouraged.
+ 
+ config BUG
+ 	bool "BUG() support" if EMBEDDED
+ 	default y
+ 	help
+           Disabling this option eliminates support for BUG and WARN, reducing
+           the size of your kernel image and potentially quietly ignoring
+           numerous fatal conditions. You should only consider disabling this
+           option for embedded systems with no facilities for reporting errors.
+           Just say Y.
+ 
+ config ELF_CORE
+ 	default y
+ 	bool "Enable ELF core dumps" if EMBEDDED
+ 	help
+ 	  Enable support for generating core dumps. Disabling saves about 4k.
+ 
+ config BASE_FULL
+ 	default y
+ 	bool "Enable full-sized data structures for core" if EMBEDDED
+ 	help
+ 	  Disabling this option reduces the size of miscellaneous core
+ 	  kernel data structures. This saves memory on small machines,
+ 	  but may reduce performance.
+ 
+ config FUTEX
+ 	bool "Enable futex support" if EMBEDDED
+ 	default y
+ 	select RT_MUTEXES
+ 	help
+ 	  Disabling this option will cause the kernel to be built without
+ 	  support for "fast userspace mutexes".  The resulting kernel may not
+ 	  run glibc-based applications correctly.
+ 
+ config ANON_INODES
+ 	bool
+ 
+ config EPOLL
+ 	bool "Enable eventpoll support" if EMBEDDED
+ 	default y
+ 	select ANON_INODES
+ 	help
+ 	  Disabling this option will cause the kernel to be built without
+ 	  support for epoll family of system calls.
+ 
+ config SIGNALFD
+ 	bool "Enable signalfd() system call" if EMBEDDED
+ 	select ANON_INODES
+ 	default y
+ 	help
+ 	  Enable the signalfd() system call that allows to receive signals
+ 	  on a file descriptor.
+ 
+ 	  If unsure, say Y.
+ 
+ config TIMERFD
+ 	bool "Enable timerfd() system call" if EMBEDDED
+ 	select ANON_INODES
+ 	depends on BROKEN
+ 	default y
+ 	help
+ 	  Enable the timerfd() system call that allows to receive timer
+ 	  events on a file descriptor.
+ 
+ 	  If unsure, say Y.
+ 
+ config EVENTFD
+ 	bool "Enable eventfd() system call" if EMBEDDED
+ 	select ANON_INODES
+ 	default y
+ 	help
+ 	  Enable the eventfd() system call that allows to receive both
+ 	  kernel notification (ie. KAIO) or userspace notifications.
+ 
+ 	  If unsure, say Y.
+ 
+ config SHMEM
+ 	bool "Use full shmem filesystem" if EMBEDDED
+ 	default y
+ 	depends on MMU
+ 	help
+ 	  The shmem is an internal filesystem used to manage shared memory.
+ 	  It is backed by swap and manages resource limits. It is also exported
+ 	  to userspace as tmpfs if TMPFS is enabled. Disabling this
+ 	  option replaces shmem and tmpfs with the much simpler ramfs code,
+ 	  which may be appropriate on small systems without swap.
+ 
+ config VM_EVENT_COUNTERS
+ 	default y
+ 	bool "Enable VM event counters for /proc/vmstat" if EMBEDDED
+ 	help
+ 	  VM event counters are needed for event counts to be shown.
+ 	  This option allows the disabling of the VM event counters
+ 	  on EMBEDDED systems.  /proc/vmstat will only show page counts
+ 	  if VM event counters are disabled.
+ 
+ config SLUB_DEBUG
+ 	default y
+ 	bool "Enable SLUB debugging support" if EMBEDDED
+ 	depends on SLUB
+ 	help
+ 	  SLUB has extensive debug support features. Disabling these can
+ 	  result in significant savings in code size. This also disables
+ 	  SLUB sysfs support. /sys/slab will not exist and there will be
+ 	  no support for cache validation etc.
+ 
+ choice
+ 	prompt "Choose SLAB allocator"
+ 	default SLUB
+ 	help
+ 	   This option allows to select a slab allocator.
+ 
+ config SLAB
+ 	bool "SLAB"
+ 	help
+ 	  The regular slab allocator that is established and known to work
+ 	  well in all environments. It organizes cache hot objects in
+ 	  per cpu and per node queues. SLAB is the default choice for
+ 	  a slab allocator.
+ 
+ config SLUB
+ 	bool "SLUB (Unqueued Allocator)"
+ 	help
+ 	   SLUB is a slab allocator that minimizes cache line usage
+ 	   instead of managing queues of cached objects (SLAB approach).
+ 	   Per cpu caching is realized using slabs of objects instead
+ 	   of queues of objects. SLUB can use memory efficiently
+ 	   and has enhanced diagnostics.
+ 
+ config SLOB
+ 	depends on EMBEDDED
+ 	bool "SLOB (Simple Allocator)"
+ 	help
+ 	   SLOB replaces the SLAB allocator with a drastically simpler
+ 	   allocator.  SLOB is more space efficient than SLAB but does not
+ 	   scale well (single lock for all operations) and is also highly
+ 	   susceptible to fragmentation. SLUB can accomplish a higher object
+ 	   density. It is usually better to use SLUB instead of SLOB.
+ 
+ endchoice
+ 
+ endmenu		# General setup
+ 
+ config RT_MUTEXES
+ 	boolean
+ 	select PLIST
+ 
+ config TINY_SHMEM
+ 	default !SHMEM
+ 	bool
+ 
+ config BASE_SMALL
+ 	int
+ 	default 0 if BASE_FULL
+ 	default 1 if !BASE_FULL
+ 
+ menuconfig MODULES
+ 	bool "Enable loadable module support"
+ 	help
+ 	  Kernel modules are small pieces of compiled code which can
+ 	  be inserted in the running kernel, rather than being
+ 	  permanently built into the kernel.  You use the "modprobe"
+ 	  tool to add (and sometimes remove) them.  If you say Y here,
+ 	  many parts of the kernel can be built as modules (by
+ 	  answering M instead of Y where indicated): this is most
+ 	  useful for infrequently used options which are not required
+ 	  for booting.  For more information, see the man pages for
+ 	  modprobe, lsmod, modinfo, insmod and rmmod.
+ 
+ 	  If you say Y here, you will need to run "make
+ 	  modules_install" to put the modules under /lib/modules/
+ 	  where modprobe can find them (you may need to be root to do
+ 	  this).
+ 
+ 	  If unsure, say Y.
+ 
+ config MODULE_UNLOAD
+ 	bool "Module unloading"
+ 	depends on MODULES
+ 	help
+ 	  Without this option you will not be able to unload any
+ 	  modules (note that some modules may not be unloadable
+ 	  anyway), which makes your kernel slightly smaller and
+ 	  simpler.  If unsure, say Y.
+ 
+ config MODULE_FORCE_UNLOAD
+ 	bool "Forced module unloading"
+ 	depends on MODULE_UNLOAD && EXPERIMENTAL
+ 	help
+ 	  This option allows you to force a module to unload, even if the
+ 	  kernel believes it is unsafe: the kernel will remove the module
+ 	  without waiting for anyone to stop using it (using the -f option to
+ 	  rmmod).  This is mainly for kernel developers and desperate users.
+ 	  If unsure, say N.
+ 
+ config MODVERSIONS
+ 	bool "Module versioning support"
+ 	depends on MODULES
+ 	help
+ 	  Usually, you have to use modules compiled with your kernel.
+ 	  Saying Y here makes it sometimes possible to use modules
+ 	  compiled for different kernels, by adding enough information
+ 	  to the modules to (hopefully) spot any changes which would
+ 	  make them incompatible with the kernel you are running.  If
+ 	  unsure, say N.
+ 
+ config MODULE_SRCVERSION_ALL
+ 	bool "Source checksum for all modules"
+ 	depends on MODULES
+ 	help
+ 	  Modules which contain a MODULE_VERSION get an extra "srcversion"
+ 	  field inserted into their modinfo section, which contains a
+     	  sum of the source files which made it.  This helps maintainers
+ 	  see exactly which source was used to build a module (since
+ 	  others sometimes change the module source without updating
+ 	  the version).  With this option, such a "srcversion" field
+ 	  will be created for all modules.  If unsure, say N.
+ 
+ config KMOD
+ 	bool "Automatic kernel module loading"
+ 	depends on MODULES
+ 	help
+ 	  Normally when you have selected some parts of the kernel to
+ 	  be created as kernel modules, you must load them (using the
+ 	  "modprobe" command) before you can use them. If you say Y
+ 	  here, some parts of the kernel will be able to load modules
+ 	  automatically: when a part of the kernel needs a module, it
+ 	  runs modprobe with the appropriate arguments, thereby
+ 	  loading the module if it is available.  If unsure, say Y.
+ 
+ config STOP_MACHINE
+ 	bool
+ 	default y
+ 	depends on (SMP && MODULE_UNLOAD) || HOTPLUG_CPU
+ 	help
+ 	  Need stop_machine() primitive.
+ 
+ source "block/Kconfig"
diff -Ncwr -x .orig -x .rej linux-2.6.23.1/mm/Makefile linux-2.6.23.1km/mm/Makefile
*** linux-2.6.23.1/mm/Makefile	2007-10-13 01:43:44.000000000 +0900
--- linux-2.6.23.1km/mm/Makefile	2008-02-05 20:53:20.000000000 +0900
***************
*** 14,19 ****
--- 14,20 ----
  			   $(mmu-y)
  
  obj-$(CONFIG_BOUNCE)	+= bounce.o
+ obj-$(CONFIG_KMALLOC_ACCOUNTING) += kmallocacct.o
  obj-$(CONFIG_SWAP)	+= page_io.o swap_state.o swapfile.o thrash.o
  obj-$(CONFIG_HUGETLBFS)	+= hugetlb.o
  obj-$(CONFIG_NUMA) 	+= mempolicy.o
diff -Ncwr -x .orig -x .rej linux-2.6.23.1/mm/Makefile.orig linux-2.6.23.1km/mm/Makefile.orig
*** linux-2.6.23.1/mm/Makefile.orig	1970-01-01 09:00:00.000000000 +0900
--- linux-2.6.23.1km/mm/Makefile.orig	2007-10-13 01:43:44.000000000 +0900
***************
*** 0 ****
--- 1,32 ----
+ #
+ # Makefile for the linux memory manager.
+ #
+ 
+ mmu-y			:= nommu.o
+ mmu-$(CONFIG_MMU)	:= fremap.o highmem.o madvise.o memory.o mincore.o \
+ 			   mlock.o mmap.o mprotect.o mremap.o msync.o rmap.o \
+ 			   vmalloc.o
+ 
+ obj-y			:= bootmem.o filemap.o mempool.o oom_kill.o fadvise.o \
+ 			   page_alloc.o page-writeback.o pdflush.o \
+ 			   readahead.o swap.o truncate.o vmscan.o \
+ 			   prio_tree.o util.o mmzone.o vmstat.o backing-dev.o \
+ 			   $(mmu-y)
+ 
+ obj-$(CONFIG_BOUNCE)	+= bounce.o
+ obj-$(CONFIG_SWAP)	+= page_io.o swap_state.o swapfile.o thrash.o
+ obj-$(CONFIG_HUGETLBFS)	+= hugetlb.o
+ obj-$(CONFIG_NUMA) 	+= mempolicy.o
+ obj-$(CONFIG_SPARSEMEM)	+= sparse.o
+ obj-$(CONFIG_SHMEM) += shmem.o
+ obj-$(CONFIG_TMPFS_POSIX_ACL) += shmem_acl.o
+ obj-$(CONFIG_TINY_SHMEM) += tiny-shmem.o
+ obj-$(CONFIG_SLOB) += slob.o
+ obj-$(CONFIG_SLAB) += slab.o
+ obj-$(CONFIG_SLUB) += slub.o
+ obj-$(CONFIG_MEMORY_HOTPLUG) += memory_hotplug.o
+ obj-$(CONFIG_FS_XIP) += filemap_xip.o
+ obj-$(CONFIG_MIGRATION) += migrate.o
+ obj-$(CONFIG_SMP) += allocpercpu.o
+ obj-$(CONFIG_QUICKLIST) += quicklist.o
+ 
diff -Ncwr -x .orig -x .rej linux-2.6.23.1/mm/Makefile.rej linux-2.6.23.1km/mm/Makefile.rej
*** linux-2.6.23.1/mm/Makefile.rej	1970-01-01 09:00:00.000000000 +0900
--- linux-2.6.23.1km/mm/Makefile.rej	2008-02-05 20:48:38.000000000 +0900
***************
*** 0 ****
--- 1,16 ----
+ ***************
+ *** 12,17 ****
+   			   readahead.o swap.o truncate.o vmscan.o \
+   			   prio_tree.o util.o mmzone.o vmstat.o $(mmu-y)
+   
+   obj-$(CONFIG_SWAP)	+= page_io.o swap_state.o swapfile.o thrash.o
+   obj-$(CONFIG_HUGETLBFS)	+= hugetlb.o
+   obj-$(CONFIG_NUMA) 	+= mempolicy.o
+ --- 12,18 ----
+   			   readahead.o swap.o truncate.o vmscan.o \
+   			   prio_tree.o util.o mmzone.o vmstat.o $(mmu-y)
+   
+ + obj-$(CONFIG_KMALLOC_ACCOUNTING) += kmallocacct.o
+   obj-$(CONFIG_SWAP)	+= page_io.o swap_state.o swapfile.o thrash.o
+   obj-$(CONFIG_HUGETLBFS)	+= hugetlb.o
+   obj-$(CONFIG_NUMA) 	+= mempolicy.o
diff -Ncwr -x .orig -x .rej linux-2.6.23.1/mm/kmallocacct.c linux-2.6.23.1km/mm/kmallocacct.c
*** linux-2.6.23.1/mm/kmallocacct.c	1970-01-01 09:00:00.000000000 +0900
--- linux-2.6.23.1km/mm/kmallocacct.c	2008-02-05 21:29:51.000000000 +0900
***************
*** 0 ****
--- 1,183 ----
+ //#include	<linux/config.h>
+ #include	<linux/seq_file.h>
+ #include	<linux/kallsyms.h>
+ #include	<linux/spinlock.h>
+ 
+ struct kma_caller {
+ 	const void *caller;
+ 	int total, net, slack, allocs, frees;
+ };
+ 
+ struct kma_list {
+ 	int callerhash;
+ 	const void *address;
+ };
+ 
+ #define MAX_CALLER_TABLE 1024
+ #define MAX_ALLOC_TRACK 32768
+ 
+ #define kma_hash(address, size) (((u32)address / (u32)size) % size)
+ 
+ static struct kma_list kma_alloc[MAX_ALLOC_TRACK];
+ static struct kma_caller kma_caller[MAX_CALLER_TABLE];
+ 
+ static int kma_callers;
+ static int kma_lost_callers, kma_lost_allocs, kma_unknown_frees;
+ static int kma_total, kma_net, kma_slack, kma_allocs, kma_frees;
+ static spinlock_t kma_lock = SPIN_LOCK_UNLOCKED;
+ 
+ void __kmalloc_account(const void *caller, const void *addr, int size, int req)
+ {
+ 	int i, hasha, hashc;
+ 	unsigned long flags;
+ 
+ 	spin_lock_irqsave(&kma_lock, flags);
+ 	if(req >= 0) /* kmalloc */
+ 	{
+ 		/* find callers slot */
+ 		hashc = kma_hash(caller, MAX_CALLER_TABLE);
+ 		for (i = 0; i < MAX_CALLER_TABLE; i++) {
+ 			if (!kma_caller[hashc].caller ||
+ 			    kma_caller[hashc].caller == caller)
+ 				break;
+ 			hashc = (hashc + 1) % MAX_CALLER_TABLE;
+ 		}
+ 
+ 		if (!kma_caller[hashc].caller)
+ 			kma_callers++;
+ 
+ 		if (i < MAX_CALLER_TABLE) {
+ 			/* update callers stats */
+ 			kma_caller[hashc].caller = caller;
+ 			kma_caller[hashc].total += size;
+ 			kma_caller[hashc].net += size;
+ 			kma_caller[hashc].slack += size - req;
+ 			kma_caller[hashc].allocs++;
+ 
+ 			/* add malloc to list */
+ 			hasha = kma_hash(addr, MAX_ALLOC_TRACK);
+ 			for (i = 0; i < MAX_ALLOC_TRACK; i++) {
+ 				if (!kma_alloc[hasha].callerhash)
+ 					break;
+ 				hasha = (hasha + 1) % MAX_ALLOC_TRACK;
+ 			}
+ 
+ 			if(i < MAX_ALLOC_TRACK) {
+ 				kma_alloc[hasha].callerhash = hashc;
+ 				kma_alloc[hasha].address = addr;
+ 			}
+ 			else
+ 				kma_lost_allocs++;
+ 		}
+ 		else {
+ 			kma_lost_callers++;
+ 			kma_lost_allocs++;
+ 		}
+ 
+ 		kma_total += size;
+ 		kma_net += size;
+ 		kma_slack += size - req;
+ 		kma_allocs++;
+ 	}
+ 	else { /* kfree */
+ 		hasha = kma_hash(addr, MAX_ALLOC_TRACK);
+ 		for (i = 0; i < MAX_ALLOC_TRACK ; i++) {
+ 			if (kma_alloc[hasha].address == addr)
+ 				break;
+ 			hasha = (hasha + 1) % MAX_ALLOC_TRACK;
+ 		}
+ 
+ 		if (i < MAX_ALLOC_TRACK) {
+ 			hashc = kma_alloc[hasha].callerhash;
+ 			kma_alloc[hasha].callerhash = 0;
+ 			kma_caller[hashc].net -= size;
+ 			kma_caller[hashc].frees++;
+ 		}
+ 		else
+ 			kma_unknown_frees++;
+ 
+ 		kma_net -= size;
+ 		kma_frees++;
+ 	}
+ 	spin_unlock_irqrestore(&kma_lock, flags);
+ }
+ 
+ static void *as_start(struct seq_file *m, loff_t *pos)
+ {
+ 	int i;
+ 	loff_t n = *pos;
+ 
+ 	if (!n) {
+ 		seq_printf(m, "total bytes allocated: %8d\n", kma_total);
+ 		seq_printf(m, "slack bytes allocated: %8d\n", kma_slack);
+ 		seq_printf(m, "net bytes allocated:   %8d\n", kma_net);
+ 		seq_printf(m, "number of allocs:      %8d\n", kma_allocs);
+ 		seq_printf(m, "number of frees:       %8d\n", kma_frees);
+ 		seq_printf(m, "number of callers:     %8d\n", kma_callers);
+ 		seq_printf(m, "lost callers:          %8d\n",
+ 			   kma_lost_callers);
+ 		seq_printf(m, "lost allocs:           %8d\n",
+ 			   kma_lost_allocs);
+ 		seq_printf(m, "unknown frees:         %8d\n",
+ 			   kma_unknown_frees);
+ 		seq_puts(m, "\n   total    slack      net alloc/free  caller\n");
+ 	}
+ 
+ 	for (i = 0; i < MAX_CALLER_TABLE; i++) {
+ 		if(kma_caller[i].caller)
+ 			n--;
+ 		if(n < 0)
+ 			return (void *)(i+1);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void *as_next(struct seq_file *m, void *p, loff_t *pos)
+ {
+ 	int n = (int)p-1, i;
+ 	++*pos;
+ 
+ 	for (i = n + 1; i < MAX_CALLER_TABLE; i++)
+ 		if(kma_caller[i].caller)
+ 			return (void *)(i+1);
+ 
+ 	return 0;
+ }
+ 
+ static void as_stop(struct seq_file *m, void *p)
+ {
+ }
+ 
+ static int as_show(struct seq_file *m, void *p)
+ {
+ 	int n = (int)p-1;
+ 	struct kma_caller *c;
+ #ifdef CONFIG_KALLSYMS
+ 	char *modname;
+ 	const char *name;
+ 	unsigned long offset = 0, size;
+ 	char namebuf[128];
+ 
+ 	c = &kma_caller[n];
+ 	name = kallsyms_lookup((int)c->caller, &size, &offset, &modname,
+ 			       namebuf);
+ 	seq_printf(m, "%8d %8d %8d %5d/%-5d %s+0x%lx\n",
+ 		   c->total, c->slack, c->net, c->allocs, c->frees,
+ 		   name, offset);
+ #else
+ 	c = &kma_caller[n];
+ 	seq_printf(m, "%8d %8d %8d %5d/%-5d %p\n",
+ 		   c->total, c->slack, c->net, c->allocs, c->frees, c->caller);
+ #endif
+ 
+ 	return 0;
+ }
+ 
+ struct seq_operations kmalloc_account_op = {
+ 	.start	= as_start,
+ 	.next	= as_next,
+ 	.stop	= as_stop,
+ 	.show	= as_show,
+ };
+ 
diff -Ncwr -x .orig -x .rej linux-2.6.23.1/mm/slab.c linux-2.6.23.1km/mm/slab.c
*** linux-2.6.23.1/mm/slab.c	2007-10-13 01:43:44.000000000 +0900
--- linux-2.6.23.1km/mm/slab.c	2008-02-05 20:52:29.000000000 +0900
***************
*** 3690,3695 ****
--- 3690,3696 ----
  					  void *caller)
  {
  	struct kmem_cache *cachep;
+  	void *a;
  
  	/* If you want to save a few bytes .text space: replace
  	 * __ with kmem_.
***************
*** 3699,3705 ****
  	cachep = __find_general_cachep(size, flags);
  	if (unlikely(ZERO_OR_NULL_PTR(cachep)))
  		return cachep;
! 	return __cache_alloc(cachep, flags, caller);
  }
  
  
--- 3700,3709 ----
  	cachep = __find_general_cachep(size, flags);
  	if (unlikely(ZERO_OR_NULL_PTR(cachep)))
  		return cachep;
!  	a = __cache_alloc(cachep, flags, caller);
!   	kmalloc_account(a, size, size);
! 	return a;
! //	return __cache_alloc(cachep, flags, caller);
  }
  
  
***************
*** 3759,3764 ****
--- 3763,3770 ----
  	struct kmem_cache *c;
  	unsigned long flags;
  
+  	kfree_account(objp, ksize(objp));
+ 
  	if (unlikely(ZERO_OR_NULL_PTR(objp)))
  		return;
  	local_irq_save(flags);
***************
*** 3809,3815 ****
  				free_alien_cache(new_alien);
  				goto fail;
  			}
! 		}
  
  		l3 = cachep->nodelists[node];
  		if (l3) {
--- 3815,3821 ----
  				free_alien_cache(new_alien);
  				goto fail;
  			}
! 		}
  
  		l3 = cachep->nodelists[node];
  		if (l3) {
diff -Ncwr -x .orig -x .rej linux-2.6.23.1/mm/slab.c.orig linux-2.6.23.1km/mm/slab.c.orig
*** linux-2.6.23.1/mm/slab.c.orig	1970-01-01 09:00:00.000000000 +0900
--- linux-2.6.23.1km/mm/slab.c.orig	2007-10-13 01:43:44.000000000 +0900
***************
*** 0 ****
--- 1,4453 ----
+ /*
+  * linux/mm/slab.c
+  * Written by Mark Hemment, 1996/97.
+  * (markhe@nextd.demon.co.uk)
+  *
+  * kmem_cache_destroy() + some cleanup - 1999 Andrea Arcangeli
+  *
+  * Major cleanup, different bufctl logic, per-cpu arrays
+  *	(c) 2000 Manfred Spraul
+  *
+  * Cleanup, make the head arrays unconditional, preparation for NUMA
+  * 	(c) 2002 Manfred Spraul
+  *
+  * An implementation of the Slab Allocator as described in outline in;
+  *	UNIX Internals: The New Frontiers by Uresh Vahalia
+  *	Pub: Prentice Hall	ISBN 0-13-101908-2
+  * or with a little more detail in;
+  *	The Slab Allocator: An Object-Caching Kernel Memory Allocator
+  *	Jeff Bonwick (Sun Microsystems).
+  *	Presented at: USENIX Summer 1994 Technical Conference
+  *
+  * The memory is organized in caches, one cache for each object type.
+  * (e.g. inode_cache, dentry_cache, buffer_head, vm_area_struct)
+  * Each cache consists out of many slabs (they are small (usually one
+  * page long) and always contiguous), and each slab contains multiple
+  * initialized objects.
+  *
+  * This means, that your constructor is used only for newly allocated
+  * slabs and you must pass objects with the same intializations to
+  * kmem_cache_free.
+  *
+  * Each cache can only support one memory type (GFP_DMA, GFP_HIGHMEM,
+  * normal). If you need a special memory type, then must create a new
+  * cache for that memory type.
+  *
+  * In order to reduce fragmentation, the slabs are sorted in 3 groups:
+  *   full slabs with 0 free objects
+  *   partial slabs
+  *   empty slabs with no allocated objects
+  *
+  * If partial slabs exist, then new allocations come from these slabs,
+  * otherwise from empty slabs or new slabs are allocated.
+  *
+  * kmem_cache_destroy() CAN CRASH if you try to allocate from the cache
+  * during kmem_cache_destroy(). The caller must prevent concurrent allocs.
+  *
+  * Each cache has a short per-cpu head array, most allocs
+  * and frees go into that array, and if that array overflows, then 1/2
+  * of the entries in the array are given back into the global cache.
+  * The head array is strictly LIFO and should improve the cache hit rates.
+  * On SMP, it additionally reduces the spinlock operations.
+  *
+  * The c_cpuarray may not be read with enabled local interrupts -
+  * it's changed with a smp_call_function().
+  *
+  * SMP synchronization:
+  *  constructors and destructors are called without any locking.
+  *  Several members in struct kmem_cache and struct slab never change, they
+  *	are accessed without any locking.
+  *  The per-cpu arrays are never accessed from the wrong cpu, no locking,
+  *  	and local interrupts are disabled so slab code is preempt-safe.
+  *  The non-constant members are protected with a per-cache irq spinlock.
+  *
+  * Many thanks to Mark Hemment, who wrote another per-cpu slab patch
+  * in 2000 - many ideas in the current implementation are derived from
+  * his patch.
+  *
+  * Further notes from the original documentation:
+  *
+  * 11 April '97.  Started multi-threading - markhe
+  *	The global cache-chain is protected by the mutex 'cache_chain_mutex'.
+  *	The sem is only needed when accessing/extending the cache-chain, which
+  *	can never happen inside an interrupt (kmem_cache_create(),
+  *	kmem_cache_shrink() and kmem_cache_reap()).
+  *
+  *	At present, each engine can be growing a cache.  This should be blocked.
+  *
+  * 15 March 2005. NUMA slab allocator.
+  *	Shai Fultheim <shai@scalex86.org>.
+  *	Shobhit Dayal <shobhit@calsoftinc.com>
+  *	Alok N Kataria <alokk@calsoftinc.com>
+  *	Christoph Lameter <christoph@lameter.com>
+  *
+  *	Modified the slab allocator to be node aware on NUMA systems.
+  *	Each node has its own list of partial, free and full slabs.
+  *	All object allocations for a node occur from node specific slab lists.
+  */
+ 
+ #include	<linux/slab.h>
+ #include	<linux/mm.h>
+ #include	<linux/poison.h>
+ #include	<linux/swap.h>
+ #include	<linux/cache.h>
+ #include	<linux/interrupt.h>
+ #include	<linux/init.h>
+ #include	<linux/compiler.h>
+ #include	<linux/cpuset.h>
+ #include	<linux/seq_file.h>
+ #include	<linux/notifier.h>
+ #include	<linux/kallsyms.h>
+ #include	<linux/cpu.h>
+ #include	<linux/sysctl.h>
+ #include	<linux/module.h>
+ #include	<linux/rcupdate.h>
+ #include	<linux/string.h>
+ #include	<linux/uaccess.h>
+ #include	<linux/nodemask.h>
+ #include	<linux/mempolicy.h>
+ #include	<linux/mutex.h>
+ #include	<linux/fault-inject.h>
+ #include	<linux/rtmutex.h>
+ #include	<linux/reciprocal_div.h>
+ 
+ #include	<asm/cacheflush.h>
+ #include	<asm/tlbflush.h>
+ #include	<asm/page.h>
+ 
+ /*
+  * DEBUG	- 1 for kmem_cache_create() to honour; SLAB_RED_ZONE & SLAB_POISON.
+  *		  0 for faster, smaller code (especially in the critical paths).
+  *
+  * STATS	- 1 to collect stats for /proc/slabinfo.
+  *		  0 for faster, smaller code (especially in the critical paths).
+  *
+  * FORCED_DEBUG	- 1 enables SLAB_RED_ZONE and SLAB_POISON (if possible)
+  */
+ 
+ #ifdef CONFIG_DEBUG_SLAB
+ #define	DEBUG		1
+ #define	STATS		1
+ #define	FORCED_DEBUG	1
+ #else
+ #define	DEBUG		0
+ #define	STATS		0
+ #define	FORCED_DEBUG	0
+ #endif
+ 
+ /* Shouldn't this be in a header file somewhere? */
+ #define	BYTES_PER_WORD		sizeof(void *)
+ #define	REDZONE_ALIGN		max(BYTES_PER_WORD, __alignof__(unsigned long long))
+ 
+ #ifndef cache_line_size
+ #define cache_line_size()	L1_CACHE_BYTES
+ #endif
+ 
+ #ifndef ARCH_KMALLOC_MINALIGN
+ /*
+  * Enforce a minimum alignment for the kmalloc caches.
+  * Usually, the kmalloc caches are cache_line_size() aligned, except when
+  * DEBUG and FORCED_DEBUG are enabled, then they are BYTES_PER_WORD aligned.
+  * Some archs want to perform DMA into kmalloc caches and need a guaranteed
+  * alignment larger than the alignment of a 64-bit integer.
+  * ARCH_KMALLOC_MINALIGN allows that.
+  * Note that increasing this value may disable some debug features.
+  */
+ #define ARCH_KMALLOC_MINALIGN __alignof__(unsigned long long)
+ #endif
+ 
+ #ifndef ARCH_SLAB_MINALIGN
+ /*
+  * Enforce a minimum alignment for all caches.
+  * Intended for archs that get misalignment faults even for BYTES_PER_WORD
+  * aligned buffers. Includes ARCH_KMALLOC_MINALIGN.
+  * If possible: Do not enable this flag for CONFIG_DEBUG_SLAB, it disables
+  * some debug features.
+  */
+ #define ARCH_SLAB_MINALIGN 0
+ #endif
+ 
+ #ifndef ARCH_KMALLOC_FLAGS
+ #define ARCH_KMALLOC_FLAGS SLAB_HWCACHE_ALIGN
+ #endif
+ 
+ /* Legal flag mask for kmem_cache_create(). */
+ #if DEBUG
+ # define CREATE_MASK	(SLAB_RED_ZONE | \
+ 			 SLAB_POISON | SLAB_HWCACHE_ALIGN | \
+ 			 SLAB_CACHE_DMA | \
+ 			 SLAB_STORE_USER | \
+ 			 SLAB_RECLAIM_ACCOUNT | SLAB_PANIC | \
+ 			 SLAB_DESTROY_BY_RCU | SLAB_MEM_SPREAD)
+ #else
+ # define CREATE_MASK	(SLAB_HWCACHE_ALIGN | \
+ 			 SLAB_CACHE_DMA | \
+ 			 SLAB_RECLAIM_ACCOUNT | SLAB_PANIC | \
+ 			 SLAB_DESTROY_BY_RCU | SLAB_MEM_SPREAD)
+ #endif
+ 
+ /*
+  * kmem_bufctl_t:
+  *
+  * Bufctl's are used for linking objs within a slab
+  * linked offsets.
+  *
+  * This implementation relies on "struct page" for locating the cache &
+  * slab an object belongs to.
+  * This allows the bufctl structure to be small (one int), but limits
+  * the number of objects a slab (not a cache) can contain when off-slab
+  * bufctls are used. The limit is the size of the largest general cache
+  * that does not use off-slab slabs.
+  * For 32bit archs with 4 kB pages, is this 56.
+  * This is not serious, as it is only for large objects, when it is unwise
+  * to have too many per slab.
+  * Note: This limit can be raised by introducing a general cache whose size
+  * is less than 512 (PAGE_SIZE<<3), but greater than 256.
+  */
+ 
+ typedef unsigned int kmem_bufctl_t;
+ #define BUFCTL_END	(((kmem_bufctl_t)(~0U))-0)
+ #define BUFCTL_FREE	(((kmem_bufctl_t)(~0U))-1)
+ #define	BUFCTL_ACTIVE	(((kmem_bufctl_t)(~0U))-2)
+ #define	SLAB_LIMIT	(((kmem_bufctl_t)(~0U))-3)
+ 
+ /*
+  * struct slab
+  *
+  * Manages the objs in a slab. Placed either at the beginning of mem allocated
+  * for a slab, or allocated from an general cache.
+  * Slabs are chained into three list: fully used, partial, fully free slabs.
+  */
+ struct slab {
+ 	struct list_head list;
+ 	unsigned long colouroff;
+ 	void *s_mem;		/* including colour offset */
+ 	unsigned int inuse;	/* num of objs active in slab */
+ 	kmem_bufctl_t free;
+ 	unsigned short nodeid;
+ };
+ 
+ /*
+  * struct slab_rcu
+  *
+  * slab_destroy on a SLAB_DESTROY_BY_RCU cache uses this structure to
+  * arrange for kmem_freepages to be called via RCU.  This is useful if
+  * we need to approach a kernel structure obliquely, from its address
+  * obtained without the usual locking.  We can lock the structure to
+  * stabilize it and check it's still at the given address, only if we
+  * can be sure that the memory has not been meanwhile reused for some
+  * other kind of object (which our subsystem's lock might corrupt).
+  *
+  * rcu_read_lock before reading the address, then rcu_read_unlock after
+  * taking the spinlock within the structure expected at that address.
+  *
+  * We assume struct slab_rcu can overlay struct slab when destroying.
+  */
+ struct slab_rcu {
+ 	struct rcu_head head;
+ 	struct kmem_cache *cachep;
+ 	void *addr;
+ };
+ 
+ /*
+  * struct array_cache
+  *
+  * Purpose:
+  * - LIFO ordering, to hand out cache-warm objects from _alloc
+  * - reduce the number of linked list operations
+  * - reduce spinlock operations
+  *
+  * The limit is stored in the per-cpu structure to reduce the data cache
+  * footprint.
+  *
+  */
+ struct array_cache {
+ 	unsigned int avail;
+ 	unsigned int limit;
+ 	unsigned int batchcount;
+ 	unsigned int touched;
+ 	spinlock_t lock;
+ 	void *entry[0];	/*
+ 			 * Must have this definition in here for the proper
+ 			 * alignment of array_cache. Also simplifies accessing
+ 			 * the entries.
+ 			 * [0] is for gcc 2.95. It should really be [].
+ 			 */
+ };
+ 
+ /*
+  * bootstrap: The caches do not work without cpuarrays anymore, but the
+  * cpuarrays are allocated from the generic caches...
+  */
+ #define BOOT_CPUCACHE_ENTRIES	1
+ struct arraycache_init {
+ 	struct array_cache cache;
+ 	void *entries[BOOT_CPUCACHE_ENTRIES];
+ };
+ 
+ /*
+  * The slab lists for all objects.
+  */
+ struct kmem_list3 {
+ 	struct list_head slabs_partial;	/* partial list first, better asm code */
+ 	struct list_head slabs_full;
+ 	struct list_head slabs_free;
+ 	unsigned long free_objects;
+ 	unsigned int free_limit;
+ 	unsigned int colour_next;	/* Per-node cache coloring */
+ 	spinlock_t list_lock;
+ 	struct array_cache *shared;	/* shared per node */
+ 	struct array_cache **alien;	/* on other nodes */
+ 	unsigned long next_reap;	/* updated without locking */
+ 	int free_touched;		/* updated without locking */
+ };
+ 
+ /*
+  * Need this for bootstrapping a per node allocator.
+  */
+ #define NUM_INIT_LISTS (2 * MAX_NUMNODES + 1)
+ struct kmem_list3 __initdata initkmem_list3[NUM_INIT_LISTS];
+ #define	CACHE_CACHE 0
+ #define	SIZE_AC 1
+ #define	SIZE_L3 (1 + MAX_NUMNODES)
+ 
+ static int drain_freelist(struct kmem_cache *cache,
+ 			struct kmem_list3 *l3, int tofree);
+ static void free_block(struct kmem_cache *cachep, void **objpp, int len,
+ 			int node);
+ static int enable_cpucache(struct kmem_cache *cachep);
+ static void cache_reap(struct work_struct *unused);
+ 
+ /*
+  * This function must be completely optimized away if a constant is passed to
+  * it.  Mostly the same as what is in linux/slab.h except it returns an index.
+  */
+ static __always_inline int index_of(const size_t size)
+ {
+ 	extern void __bad_size(void);
+ 
+ 	if (__builtin_constant_p(size)) {
+ 		int i = 0;
+ 
+ #define CACHE(x) \
+ 	if (size <=x) \
+ 		return i; \
+ 	else \
+ 		i++;
+ #include "linux/kmalloc_sizes.h"
+ #undef CACHE
+ 		__bad_size();
+ 	} else
+ 		__bad_size();
+ 	return 0;
+ }
+ 
+ static int slab_early_init = 1;
+ 
+ #define INDEX_AC index_of(sizeof(struct arraycache_init))
+ #define INDEX_L3 index_of(sizeof(struct kmem_list3))
+ 
+ static void kmem_list3_init(struct kmem_list3 *parent)
+ {
+ 	INIT_LIST_HEAD(&parent->slabs_full);
+ 	INIT_LIST_HEAD(&parent->slabs_partial);
+ 	INIT_LIST_HEAD(&parent->slabs_free);
+ 	parent->shared = NULL;
+ 	parent->alien = NULL;
+ 	parent->colour_next = 0;
+ 	spin_lock_init(&parent->list_lock);
+ 	parent->free_objects = 0;
+ 	parent->free_touched = 0;
+ }
+ 
+ #define MAKE_LIST(cachep, listp, slab, nodeid)				\
+ 	do {								\
+ 		INIT_LIST_HEAD(listp);					\
+ 		list_splice(&(cachep->nodelists[nodeid]->slab), listp);	\
+ 	} while (0)
+ 
+ #define	MAKE_ALL_LISTS(cachep, ptr, nodeid)				\
+ 	do {								\
+ 	MAKE_LIST((cachep), (&(ptr)->slabs_full), slabs_full, nodeid);	\
+ 	MAKE_LIST((cachep), (&(ptr)->slabs_partial), slabs_partial, nodeid); \
+ 	MAKE_LIST((cachep), (&(ptr)->slabs_free), slabs_free, nodeid);	\
+ 	} while (0)
+ 
+ /*
+  * struct kmem_cache
+  *
+  * manages a cache.
+  */
+ 
+ struct kmem_cache {
+ /* 1) per-cpu data, touched during every alloc/free */
+ 	struct array_cache *array[NR_CPUS];
+ /* 2) Cache tunables. Protected by cache_chain_mutex */
+ 	unsigned int batchcount;
+ 	unsigned int limit;
+ 	unsigned int shared;
+ 
+ 	unsigned int buffer_size;
+ 	u32 reciprocal_buffer_size;
+ /* 3) touched by every alloc & free from the backend */
+ 
+ 	unsigned int flags;		/* constant flags */
+ 	unsigned int num;		/* # of objs per slab */
+ 
+ /* 4) cache_grow/shrink */
+ 	/* order of pgs per slab (2^n) */
+ 	unsigned int gfporder;
+ 
+ 	/* force GFP flags, e.g. GFP_DMA */
+ 	gfp_t gfpflags;
+ 
+ 	size_t colour;			/* cache colouring range */
+ 	unsigned int colour_off;	/* colour offset */
+ 	struct kmem_cache *slabp_cache;
+ 	unsigned int slab_size;
+ 	unsigned int dflags;		/* dynamic flags */
+ 
+ 	/* constructor func */
+ 	void (*ctor) (void *, struct kmem_cache *, unsigned long);
+ 
+ /* 5) cache creation/removal */
+ 	const char *name;
+ 	struct list_head next;
+ 
+ /* 6) statistics */
+ #if STATS
+ 	unsigned long num_active;
+ 	unsigned long num_allocations;
+ 	unsigned long high_mark;
+ 	unsigned long grown;
+ 	unsigned long reaped;
+ 	unsigned long errors;
+ 	unsigned long max_freeable;
+ 	unsigned long node_allocs;
+ 	unsigned long node_frees;
+ 	unsigned long node_overflow;
+ 	atomic_t allochit;
+ 	atomic_t allocmiss;
+ 	atomic_t freehit;
+ 	atomic_t freemiss;
+ #endif
+ #if DEBUG
+ 	/*
+ 	 * If debugging is enabled, then the allocator can add additional
+ 	 * fields and/or padding to every object. buffer_size contains the total
+ 	 * object size including these internal fields, the following two
+ 	 * variables contain the offset to the user object and its size.
+ 	 */
+ 	int obj_offset;
+ 	int obj_size;
+ #endif
+ 	/*
+ 	 * We put nodelists[] at the end of kmem_cache, because we want to size
+ 	 * this array to nr_node_ids slots instead of MAX_NUMNODES
+ 	 * (see kmem_cache_init())
+ 	 * We still use [MAX_NUMNODES] and not [1] or [0] because cache_cache
+ 	 * is statically defined, so we reserve the max number of nodes.
+ 	 */
+ 	struct kmem_list3 *nodelists[MAX_NUMNODES];
+ 	/*
+ 	 * Do not add fields after nodelists[]
+ 	 */
+ };
+ 
+ #define CFLGS_OFF_SLAB		(0x80000000UL)
+ #define	OFF_SLAB(x)	((x)->flags & CFLGS_OFF_SLAB)
+ 
+ #define BATCHREFILL_LIMIT	16
+ /*
+  * Optimization question: fewer reaps means less probability for unnessary
+  * cpucache drain/refill cycles.
+  *
+  * OTOH the cpuarrays can contain lots of objects,
+  * which could lock up otherwise freeable slabs.
+  */
+ #define REAPTIMEOUT_CPUC	(2*HZ)
+ #define REAPTIMEOUT_LIST3	(4*HZ)
+ 
+ #if STATS
+ #define	STATS_INC_ACTIVE(x)	((x)->num_active++)
+ #define	STATS_DEC_ACTIVE(x)	((x)->num_active--)
+ #define	STATS_INC_ALLOCED(x)	((x)->num_allocations++)
+ #define	STATS_INC_GROWN(x)	((x)->grown++)
+ #define	STATS_ADD_REAPED(x,y)	((x)->reaped += (y))
+ #define	STATS_SET_HIGH(x)						\
+ 	do {								\
+ 		if ((x)->num_active > (x)->high_mark)			\
+ 			(x)->high_mark = (x)->num_active;		\
+ 	} while (0)
+ #define	STATS_INC_ERR(x)	((x)->errors++)
+ #define	STATS_INC_NODEALLOCS(x)	((x)->node_allocs++)
+ #define	STATS_INC_NODEFREES(x)	((x)->node_frees++)
+ #define STATS_INC_ACOVERFLOW(x)   ((x)->node_overflow++)
+ #define	STATS_SET_FREEABLE(x, i)					\
+ 	do {								\
+ 		if ((x)->max_freeable < i)				\
+ 			(x)->max_freeable = i;				\
+ 	} while (0)
+ #define STATS_INC_ALLOCHIT(x)	atomic_inc(&(x)->allochit)
+ #define STATS_INC_ALLOCMISS(x)	atomic_inc(&(x)->allocmiss)
+ #define STATS_INC_FREEHIT(x)	atomic_inc(&(x)->freehit)
+ #define STATS_INC_FREEMISS(x)	atomic_inc(&(x)->freemiss)
+ #else
+ #define	STATS_INC_ACTIVE(x)	do { } while (0)
+ #define	STATS_DEC_ACTIVE(x)	do { } while (0)
+ #define	STATS_INC_ALLOCED(x)	do { } while (0)
+ #define	STATS_INC_GROWN(x)	do { } while (0)
+ #define	STATS_ADD_REAPED(x,y)	do { } while (0)
+ #define	STATS_SET_HIGH(x)	do { } while (0)
+ #define	STATS_INC_ERR(x)	do { } while (0)
+ #define	STATS_INC_NODEALLOCS(x)	do { } while (0)
+ #define	STATS_INC_NODEFREES(x)	do { } while (0)
+ #define STATS_INC_ACOVERFLOW(x)   do { } while (0)
+ #define	STATS_SET_FREEABLE(x, i) do { } while (0)
+ #define STATS_INC_ALLOCHIT(x)	do { } while (0)
+ #define STATS_INC_ALLOCMISS(x)	do { } while (0)
+ #define STATS_INC_FREEHIT(x)	do { } while (0)
+ #define STATS_INC_FREEMISS(x)	do { } while (0)
+ #endif
+ 
+ #if DEBUG
+ 
+ /*
+  * memory layout of objects:
+  * 0		: objp
+  * 0 .. cachep->obj_offset - BYTES_PER_WORD - 1: padding. This ensures that
+  * 		the end of an object is aligned with the end of the real
+  * 		allocation. Catches writes behind the end of the allocation.
+  * cachep->obj_offset - BYTES_PER_WORD .. cachep->obj_offset - 1:
+  * 		redzone word.
+  * cachep->obj_offset: The real object.
+  * cachep->buffer_size - 2* BYTES_PER_WORD: redzone word [BYTES_PER_WORD long]
+  * cachep->buffer_size - 1* BYTES_PER_WORD: last caller address
+  *					[BYTES_PER_WORD long]
+  */
+ static int obj_offset(struct kmem_cache *cachep)
+ {
+ 	return cachep->obj_offset;
+ }
+ 
+ static int obj_size(struct kmem_cache *cachep)
+ {
+ 	return cachep->obj_size;
+ }
+ 
+ static unsigned long long *dbg_redzone1(struct kmem_cache *cachep, void *objp)
+ {
+ 	BUG_ON(!(cachep->flags & SLAB_RED_ZONE));
+ 	return (unsigned long long*) (objp + obj_offset(cachep) -
+ 				      sizeof(unsigned long long));
+ }
+ 
+ static unsigned long long *dbg_redzone2(struct kmem_cache *cachep, void *objp)
+ {
+ 	BUG_ON(!(cachep->flags & SLAB_RED_ZONE));
+ 	if (cachep->flags & SLAB_STORE_USER)
+ 		return (unsigned long long *)(objp + cachep->buffer_size -
+ 					      sizeof(unsigned long long) -
+ 					      REDZONE_ALIGN);
+ 	return (unsigned long long *) (objp + cachep->buffer_size -
+ 				       sizeof(unsigned long long));
+ }
+ 
+ static void **dbg_userword(struct kmem_cache *cachep, void *objp)
+ {
+ 	BUG_ON(!(cachep->flags & SLAB_STORE_USER));
+ 	return (void **)(objp + cachep->buffer_size - BYTES_PER_WORD);
+ }
+ 
+ #else
+ 
+ #define obj_offset(x)			0
+ #define obj_size(cachep)		(cachep->buffer_size)
+ #define dbg_redzone1(cachep, objp)	({BUG(); (unsigned long long *)NULL;})
+ #define dbg_redzone2(cachep, objp)	({BUG(); (unsigned long long *)NULL;})
+ #define dbg_userword(cachep, objp)	({BUG(); (void **)NULL;})
+ 
+ #endif
+ 
+ /*
+  * Do not go above this order unless 0 objects fit into the slab.
+  */
+ #define	BREAK_GFP_ORDER_HI	1
+ #define	BREAK_GFP_ORDER_LO	0
+ static int slab_break_gfp_order = BREAK_GFP_ORDER_LO;
+ 
+ /*
+  * Functions for storing/retrieving the cachep and or slab from the page
+  * allocator.  These are used to find the slab an obj belongs to.  With kfree(),
+  * these are used to find the cache which an obj belongs to.
+  */
+ static inline void page_set_cache(struct page *page, struct kmem_cache *cache)
+ {
+ 	page->lru.next = (struct list_head *)cache;
+ }
+ 
+ static inline struct kmem_cache *page_get_cache(struct page *page)
+ {
+ 	page = compound_head(page);
+ 	BUG_ON(!PageSlab(page));
+ 	return (struct kmem_cache *)page->lru.next;
+ }
+ 
+ static inline void page_set_slab(struct page *page, struct slab *slab)
+ {
+ 	page->lru.prev = (struct list_head *)slab;
+ }
+ 
+ static inline struct slab *page_get_slab(struct page *page)
+ {
+ 	BUG_ON(!PageSlab(page));
+ 	return (struct slab *)page->lru.prev;
+ }
+ 
+ static inline struct kmem_cache *virt_to_cache(const void *obj)
+ {
+ 	struct page *page = virt_to_head_page(obj);
+ 	return page_get_cache(page);
+ }
+ 
+ static inline struct slab *virt_to_slab(const void *obj)
+ {
+ 	struct page *page = virt_to_head_page(obj);
+ 	return page_get_slab(page);
+ }
+ 
+ static inline void *index_to_obj(struct kmem_cache *cache, struct slab *slab,
+ 				 unsigned int idx)
+ {
+ 	return slab->s_mem + cache->buffer_size * idx;
+ }
+ 
+ /*
+  * We want to avoid an expensive divide : (offset / cache->buffer_size)
+  *   Using the fact that buffer_size is a constant for a particular cache,
+  *   we can replace (offset / cache->buffer_size) by
+  *   reciprocal_divide(offset, cache->reciprocal_buffer_size)
+  */
+ static inline unsigned int obj_to_index(const struct kmem_cache *cache,
+ 					const struct slab *slab, void *obj)
+ {
+ 	u32 offset = (obj - slab->s_mem);
+ 	return reciprocal_divide(offset, cache->reciprocal_buffer_size);
+ }
+ 
+ /*
+  * These are the default caches for kmalloc. Custom caches can have other sizes.
+  */
+ struct cache_sizes malloc_sizes[] = {
+ #define CACHE(x) { .cs_size = (x) },
+ #include <linux/kmalloc_sizes.h>
+ 	CACHE(ULONG_MAX)
+ #undef CACHE
+ };
+ EXPORT_SYMBOL(malloc_sizes);
+ 
+ /* Must match cache_sizes above. Out of line to keep cache footprint low. */
+ struct cache_names {
+ 	char *name;
+ 	char *name_dma;
+ };
+ 
+ static struct cache_names __initdata cache_names[] = {
+ #define CACHE(x) { .name = "size-" #x, .name_dma = "size-" #x "(DMA)" },
+ #include <linux/kmalloc_sizes.h>
+ 	{NULL,}
+ #undef CACHE
+ };
+ 
+ static struct arraycache_init initarray_cache __initdata =
+     { {0, BOOT_CPUCACHE_ENTRIES, 1, 0} };
+ static struct arraycache_init initarray_generic =
+     { {0, BOOT_CPUCACHE_ENTRIES, 1, 0} };
+ 
+ /* internal cache of cache description objs */
+ static struct kmem_cache cache_cache = {
+ 	.batchcount = 1,
+ 	.limit = BOOT_CPUCACHE_ENTRIES,
+ 	.shared = 1,
+ 	.buffer_size = sizeof(struct kmem_cache),
+ 	.name = "kmem_cache",
+ };
+ 
+ #define BAD_ALIEN_MAGIC 0x01020304ul
+ 
+ #ifdef CONFIG_LOCKDEP
+ 
+ /*
+  * Slab sometimes uses the kmalloc slabs to store the slab headers
+  * for other slabs "off slab".
+  * The locking for this is tricky in that it nests within the locks
+  * of all other slabs in a few places; to deal with this special
+  * locking we put on-slab caches into a separate lock-class.
+  *
+  * We set lock class for alien array caches which are up during init.
+  * The lock annotation will be lost if all cpus of a node goes down and
+  * then comes back up during hotplug
+  */
+ static struct lock_class_key on_slab_l3_key;
+ static struct lock_class_key on_slab_alc_key;
+ 
+ static inline void init_lock_keys(void)
+ 
+ {
+ 	int q;
+ 	struct cache_sizes *s = malloc_sizes;
+ 
+ 	while (s->cs_size != ULONG_MAX) {
+ 		for_each_node(q) {
+ 			struct array_cache **alc;
+ 			int r;
+ 			struct kmem_list3 *l3 = s->cs_cachep->nodelists[q];
+ 			if (!l3 || OFF_SLAB(s->cs_cachep))
+ 				continue;
+ 			lockdep_set_class(&l3->list_lock, &on_slab_l3_key);
+ 			alc = l3->alien;
+ 			/*
+ 			 * FIXME: This check for BAD_ALIEN_MAGIC
+ 			 * should go away when common slab code is taught to
+ 			 * work even without alien caches.
+ 			 * Currently, non NUMA code returns BAD_ALIEN_MAGIC
+ 			 * for alloc_alien_cache,
+ 			 */
+ 			if (!alc || (unsigned long)alc == BAD_ALIEN_MAGIC)
+ 				continue;
+ 			for_each_node(r) {
+ 				if (alc[r])
+ 					lockdep_set_class(&alc[r]->lock,
+ 					     &on_slab_alc_key);
+ 			}
+ 		}
+ 		s++;
+ 	}
+ }
+ #else
+ static inline void init_lock_keys(void)
+ {
+ }
+ #endif
+ 
+ /*
+  * 1. Guard access to the cache-chain.
+  * 2. Protect sanity of cpu_online_map against cpu hotplug events
+  */
+ static DEFINE_MUTEX(cache_chain_mutex);
+ static struct list_head cache_chain;
+ 
+ /*
+  * chicken and egg problem: delay the per-cpu array allocation
+  * until the general caches are up.
+  */
+ static enum {
+ 	NONE,
+ 	PARTIAL_AC,
+ 	PARTIAL_L3,
+ 	FULL
+ } g_cpucache_up;
+ 
+ /*
+  * used by boot code to determine if it can use slab based allocator
+  */
+ int slab_is_available(void)
+ {
+ 	return g_cpucache_up == FULL;
+ }
+ 
+ static DEFINE_PER_CPU(struct delayed_work, reap_work);
+ 
+ static inline struct array_cache *cpu_cache_get(struct kmem_cache *cachep)
+ {
+ 	return cachep->array[smp_processor_id()];
+ }
+ 
+ static inline struct kmem_cache *__find_general_cachep(size_t size,
+ 							gfp_t gfpflags)
+ {
+ 	struct cache_sizes *csizep = malloc_sizes;
+ 
+ #if DEBUG
+ 	/* This happens if someone tries to call
+ 	 * kmem_cache_create(), or __kmalloc(), before
+ 	 * the generic caches are initialized.
+ 	 */
+ 	BUG_ON(malloc_sizes[INDEX_AC].cs_cachep == NULL);
+ #endif
+ 	if (!size)
+ 		return ZERO_SIZE_PTR;
+ 
+ 	while (size > csizep->cs_size)
+ 		csizep++;
+ 
+ 	/*
+ 	 * Really subtle: The last entry with cs->cs_size==ULONG_MAX
+ 	 * has cs_{dma,}cachep==NULL. Thus no special case
+ 	 * for large kmalloc calls required.
+ 	 */
+ #ifdef CONFIG_ZONE_DMA
+ 	if (unlikely(gfpflags & GFP_DMA))
+ 		return csizep->cs_dmacachep;
+ #endif
+ 	return csizep->cs_cachep;
+ }
+ 
+ static struct kmem_cache *kmem_find_general_cachep(size_t size, gfp_t gfpflags)
+ {
+ 	return __find_general_cachep(size, gfpflags);
+ }
+ 
+ static size_t slab_mgmt_size(size_t nr_objs, size_t align)
+ {
+ 	return ALIGN(sizeof(struct slab)+nr_objs*sizeof(kmem_bufctl_t), align);
+ }
+ 
+ /*
+  * Calculate the number of objects and left-over bytes for a given buffer size.
+  */
+ static void cache_estimate(unsigned long gfporder, size_t buffer_size,
+ 			   size_t align, int flags, size_t *left_over,
+ 			   unsigned int *num)
+ {
+ 	int nr_objs;
+ 	size_t mgmt_size;
+ 	size_t slab_size = PAGE_SIZE << gfporder;
+ 
+ 	/*
+ 	 * The slab management structure can be either off the slab or
+ 	 * on it. For the latter case, the memory allocated for a
+ 	 * slab is used for:
+ 	 *
+ 	 * - The struct slab
+ 	 * - One kmem_bufctl_t for each object
+ 	 * - Padding to respect alignment of @align
+ 	 * - @buffer_size bytes for each object
+ 	 *
+ 	 * If the slab management structure is off the slab, then the
+ 	 * alignment will already be calculated into the size. Because
+ 	 * the slabs are all pages aligned, the objects will be at the
+ 	 * correct alignment when allocated.
+ 	 */
+ 	if (flags & CFLGS_OFF_SLAB) {
+ 		mgmt_size = 0;
+ 		nr_objs = slab_size / buffer_size;
+ 
+ 		if (nr_objs > SLAB_LIMIT)
+ 			nr_objs = SLAB_LIMIT;
+ 	} else {
+ 		/*
+ 		 * Ignore padding for the initial guess. The padding
+ 		 * is at most @align-1 bytes, and @buffer_size is at
+ 		 * least @align. In the worst case, this result will
+ 		 * be one greater than the number of objects that fit
+ 		 * into the memory allocation when taking the padding
+ 		 * into account.
+ 		 */
+ 		nr_objs = (slab_size - sizeof(struct slab)) /
+ 			  (buffer_size + sizeof(kmem_bufctl_t));
+ 
+ 		/*
+ 		 * This calculated number will be either the right
+ 		 * amount, or one greater than what we want.
+ 		 */
+ 		if (slab_mgmt_size(nr_objs, align) + nr_objs*buffer_size
+ 		       > slab_size)
+ 			nr_objs--;
+ 
+ 		if (nr_objs > SLAB_LIMIT)
+ 			nr_objs = SLAB_LIMIT;
+ 
+ 		mgmt_size = slab_mgmt_size(nr_objs, align);
+ 	}
+ 	*num = nr_objs;
+ 	*left_over = slab_size - nr_objs*buffer_size - mgmt_size;
+ }
+ 
+ #define slab_error(cachep, msg) __slab_error(__FUNCTION__, cachep, msg)
+ 
+ static void __slab_error(const char *function, struct kmem_cache *cachep,
+ 			char *msg)
+ {
+ 	printk(KERN_ERR "slab error in %s(): cache `%s': %s\n",
+ 	       function, cachep->name, msg);
+ 	dump_stack();
+ }
+ 
+ /*
+  * By default on NUMA we use alien caches to stage the freeing of
+  * objects allocated from other nodes. This causes massive memory
+  * inefficiencies when using fake NUMA setup to split memory into a
+  * large number of small nodes, so it can be disabled on the command
+  * line
+   */
+ 
+ static int use_alien_caches __read_mostly = 1;
+ static int numa_platform __read_mostly = 1;
+ static int __init noaliencache_setup(char *s)
+ {
+ 	use_alien_caches = 0;
+ 	return 1;
+ }
+ __setup("noaliencache", noaliencache_setup);
+ 
+ #ifdef CONFIG_NUMA
+ /*
+  * Special reaping functions for NUMA systems called from cache_reap().
+  * These take care of doing round robin flushing of alien caches (containing
+  * objects freed on different nodes from which they were allocated) and the
+  * flushing of remote pcps by calling drain_node_pages.
+  */
+ static DEFINE_PER_CPU(unsigned long, reap_node);
+ 
+ static void init_reap_node(int cpu)
+ {
+ 	int node;
+ 
+ 	node = next_node(cpu_to_node(cpu), node_online_map);
+ 	if (node == MAX_NUMNODES)
+ 		node = first_node(node_online_map);
+ 
+ 	per_cpu(reap_node, cpu) = node;
+ }
+ 
+ static void next_reap_node(void)
+ {
+ 	int node = __get_cpu_var(reap_node);
+ 
+ 	node = next_node(node, node_online_map);
+ 	if (unlikely(node >= MAX_NUMNODES))
+ 		node = first_node(node_online_map);
+ 	__get_cpu_var(reap_node) = node;
+ }
+ 
+ #else
+ #define init_reap_node(cpu) do { } while (0)
+ #define next_reap_node(void) do { } while (0)
+ #endif
+ 
+ /*
+  * Initiate the reap timer running on the target CPU.  We run at around 1 to 2Hz
+  * via the workqueue/eventd.
+  * Add the CPU number into the expiration time to minimize the possibility of
+  * the CPUs getting into lockstep and contending for the global cache chain
+  * lock.
+  */
+ static void __cpuinit start_cpu_timer(int cpu)
+ {
+ 	struct delayed_work *reap_work = &per_cpu(reap_work, cpu);
+ 
+ 	/*
+ 	 * When this gets called from do_initcalls via cpucache_init(),
+ 	 * init_workqueues() has already run, so keventd will be setup
+ 	 * at that time.
+ 	 */
+ 	if (keventd_up() && reap_work->work.func == NULL) {
+ 		init_reap_node(cpu);
+ 		INIT_DELAYED_WORK(reap_work, cache_reap);
+ 		schedule_delayed_work_on(cpu, reap_work,
+ 					__round_jiffies_relative(HZ, cpu));
+ 	}
+ }
+ 
+ static struct array_cache *alloc_arraycache(int node, int entries,
+ 					    int batchcount)
+ {
+ 	int memsize = sizeof(void *) * entries + sizeof(struct array_cache);
+ 	struct array_cache *nc = NULL;
+ 
+ 	nc = kmalloc_node(memsize, GFP_KERNEL, node);
+ 	if (nc) {
+ 		nc->avail = 0;
+ 		nc->limit = entries;
+ 		nc->batchcount = batchcount;
+ 		nc->touched = 0;
+ 		spin_lock_init(&nc->lock);
+ 	}
+ 	return nc;
+ }
+ 
+ /*
+  * Transfer objects in one arraycache to another.
+  * Locking must be handled by the caller.
+  *
+  * Return the number of entries transferred.
+  */
+ static int transfer_objects(struct array_cache *to,
+ 		struct array_cache *from, unsigned int max)
+ {
+ 	/* Figure out how many entries to transfer */
+ 	int nr = min(min(from->avail, max), to->limit - to->avail);
+ 
+ 	if (!nr)
+ 		return 0;
+ 
+ 	memcpy(to->entry + to->avail, from->entry + from->avail -nr,
+ 			sizeof(void *) *nr);
+ 
+ 	from->avail -= nr;
+ 	to->avail += nr;
+ 	to->touched = 1;
+ 	return nr;
+ }
+ 
+ #ifndef CONFIG_NUMA
+ 
+ #define drain_alien_cache(cachep, alien) do { } while (0)
+ #define reap_alien(cachep, l3) do { } while (0)
+ 
+ static inline struct array_cache **alloc_alien_cache(int node, int limit)
+ {
+ 	return (struct array_cache **)BAD_ALIEN_MAGIC;
+ }
+ 
+ static inline void free_alien_cache(struct array_cache **ac_ptr)
+ {
+ }
+ 
+ static inline int cache_free_alien(struct kmem_cache *cachep, void *objp)
+ {
+ 	return 0;
+ }
+ 
+ static inline void *alternate_node_alloc(struct kmem_cache *cachep,
+ 		gfp_t flags)
+ {
+ 	return NULL;
+ }
+ 
+ static inline void *____cache_alloc_node(struct kmem_cache *cachep,
+ 		 gfp_t flags, int nodeid)
+ {
+ 	return NULL;
+ }
+ 
+ #else	/* CONFIG_NUMA */
+ 
+ static void *____cache_alloc_node(struct kmem_cache *, gfp_t, int);
+ static void *alternate_node_alloc(struct kmem_cache *, gfp_t);
+ 
+ static struct array_cache **alloc_alien_cache(int node, int limit)
+ {
+ 	struct array_cache **ac_ptr;
+ 	int memsize = sizeof(void *) * nr_node_ids;
+ 	int i;
+ 
+ 	if (limit > 1)
+ 		limit = 12;
+ 	ac_ptr = kmalloc_node(memsize, GFP_KERNEL, node);
+ 	if (ac_ptr) {
+ 		for_each_node(i) {
+ 			if (i == node || !node_online(i)) {
+ 				ac_ptr[i] = NULL;
+ 				continue;
+ 			}
+ 			ac_ptr[i] = alloc_arraycache(node, limit, 0xbaadf00d);
+ 			if (!ac_ptr[i]) {
+ 				for (i--; i <= 0; i--)
+ 					kfree(ac_ptr[i]);
+ 				kfree(ac_ptr);
+ 				return NULL;
+ 			}
+ 		}
+ 	}
+ 	return ac_ptr;
+ }
+ 
+ static void free_alien_cache(struct array_cache **ac_ptr)
+ {
+ 	int i;
+ 
+ 	if (!ac_ptr)
+ 		return;
+ 	for_each_node(i)
+ 	    kfree(ac_ptr[i]);
+ 	kfree(ac_ptr);
+ }
+ 
+ static void __drain_alien_cache(struct kmem_cache *cachep,
+ 				struct array_cache *ac, int node)
+ {
+ 	struct kmem_list3 *rl3 = cachep->nodelists[node];
+ 
+ 	if (ac->avail) {
+ 		spin_lock(&rl3->list_lock);
+ 		/*
+ 		 * Stuff objects into the remote nodes shared array first.
+ 		 * That way we could avoid the overhead of putting the objects
+ 		 * into the free lists and getting them back later.
+ 		 */
+ 		if (rl3->shared)
+ 			transfer_objects(rl3->shared, ac, ac->limit);
+ 
+ 		free_block(cachep, ac->entry, ac->avail, node);
+ 		ac->avail = 0;
+ 		spin_unlock(&rl3->list_lock);
+ 	}
+ }
+ 
+ /*
+  * Called from cache_reap() to regularly drain alien caches round robin.
+  */
+ static void reap_alien(struct kmem_cache *cachep, struct kmem_list3 *l3)
+ {
+ 	int node = __get_cpu_var(reap_node);
+ 
+ 	if (l3->alien) {
+ 		struct array_cache *ac = l3->alien[node];
+ 
+ 		if (ac && ac->avail && spin_trylock_irq(&ac->lock)) {
+ 			__drain_alien_cache(cachep, ac, node);
+ 			spin_unlock_irq(&ac->lock);
+ 		}
+ 	}
+ }
+ 
+ static void drain_alien_cache(struct kmem_cache *cachep,
+ 				struct array_cache **alien)
+ {
+ 	int i = 0;
+ 	struct array_cache *ac;
+ 	unsigned long flags;
+ 
+ 	for_each_online_node(i) {
+ 		ac = alien[i];
+ 		if (ac) {
+ 			spin_lock_irqsave(&ac->lock, flags);
+ 			__drain_alien_cache(cachep, ac, i);
+ 			spin_unlock_irqrestore(&ac->lock, flags);
+ 		}
+ 	}
+ }
+ 
+ static inline int cache_free_alien(struct kmem_cache *cachep, void *objp)
+ {
+ 	struct slab *slabp = virt_to_slab(objp);
+ 	int nodeid = slabp->nodeid;
+ 	struct kmem_list3 *l3;
+ 	struct array_cache *alien = NULL;
+ 	int node;
+ 
+ 	node = numa_node_id();
+ 
+ 	/*
+ 	 * Make sure we are not freeing a object from another node to the array
+ 	 * cache on this cpu.
+ 	 */
+ 	if (likely(slabp->nodeid == node))
+ 		return 0;
+ 
+ 	l3 = cachep->nodelists[node];
+ 	STATS_INC_NODEFREES(cachep);
+ 	if (l3->alien && l3->alien[nodeid]) {
+ 		alien = l3->alien[nodeid];
+ 		spin_lock(&alien->lock);
+ 		if (unlikely(alien->avail == alien->limit)) {
+ 			STATS_INC_ACOVERFLOW(cachep);
+ 			__drain_alien_cache(cachep, alien, nodeid);
+ 		}
+ 		alien->entry[alien->avail++] = objp;
+ 		spin_unlock(&alien->lock);
+ 	} else {
+ 		spin_lock(&(cachep->nodelists[nodeid])->list_lock);
+ 		free_block(cachep, &objp, 1, nodeid);
+ 		spin_unlock(&(cachep->nodelists[nodeid])->list_lock);
+ 	}
+ 	return 1;
+ }
+ #endif
+ 
+ static int __cpuinit cpuup_callback(struct notifier_block *nfb,
+ 				    unsigned long action, void *hcpu)
+ {
+ 	long cpu = (long)hcpu;
+ 	struct kmem_cache *cachep;
+ 	struct kmem_list3 *l3 = NULL;
+ 	int node = cpu_to_node(cpu);
+ 	const int memsize = sizeof(struct kmem_list3);
+ 
+ 	switch (action) {
+ 	case CPU_LOCK_ACQUIRE:
+ 		mutex_lock(&cache_chain_mutex);
+ 		break;
+ 	case CPU_UP_PREPARE:
+ 	case CPU_UP_PREPARE_FROZEN:
+ 		/*
+ 		 * We need to do this right in the beginning since
+ 		 * alloc_arraycache's are going to use this list.
+ 		 * kmalloc_node allows us to add the slab to the right
+ 		 * kmem_list3 and not this cpu's kmem_list3
+ 		 */
+ 
+ 		list_for_each_entry(cachep, &cache_chain, next) {
+ 			/*
+ 			 * Set up the size64 kmemlist for cpu before we can
+ 			 * begin anything. Make sure some other cpu on this
+ 			 * node has not already allocated this
+ 			 */
+ 			if (!cachep->nodelists[node]) {
+ 				l3 = kmalloc_node(memsize, GFP_KERNEL, node);
+ 				if (!l3)
+ 					goto bad;
+ 				kmem_list3_init(l3);
+ 				l3->next_reap = jiffies + REAPTIMEOUT_LIST3 +
+ 				    ((unsigned long)cachep) % REAPTIMEOUT_LIST3;
+ 
+ 				/*
+ 				 * The l3s don't come and go as CPUs come and
+ 				 * go.  cache_chain_mutex is sufficient
+ 				 * protection here.
+ 				 */
+ 				cachep->nodelists[node] = l3;
+ 			}
+ 
+ 			spin_lock_irq(&cachep->nodelists[node]->list_lock);
+ 			cachep->nodelists[node]->free_limit =
+ 				(1 + nr_cpus_node(node)) *
+ 				cachep->batchcount + cachep->num;
+ 			spin_unlock_irq(&cachep->nodelists[node]->list_lock);
+ 		}
+ 
+ 		/*
+ 		 * Now we can go ahead with allocating the shared arrays and
+ 		 * array caches
+ 		 */
+ 		list_for_each_entry(cachep, &cache_chain, next) {
+ 			struct array_cache *nc;
+ 			struct array_cache *shared = NULL;
+ 			struct array_cache **alien = NULL;
+ 
+ 			nc = alloc_arraycache(node, cachep->limit,
+ 						cachep->batchcount);
+ 			if (!nc)
+ 				goto bad;
+ 			if (cachep->shared) {
+ 				shared = alloc_arraycache(node,
+ 					cachep->shared * cachep->batchcount,
+ 					0xbaadf00d);
+ 				if (!shared)
+ 					goto bad;
+ 			}
+ 			if (use_alien_caches) {
+                                 alien = alloc_alien_cache(node, cachep->limit);
+                                 if (!alien)
+                                         goto bad;
+                         }
+ 			cachep->array[cpu] = nc;
+ 			l3 = cachep->nodelists[node];
+ 			BUG_ON(!l3);
+ 
+ 			spin_lock_irq(&l3->list_lock);
+ 			if (!l3->shared) {
+ 				/*
+ 				 * We are serialised from CPU_DEAD or
+ 				 * CPU_UP_CANCELLED by the cpucontrol lock
+ 				 */
+ 				l3->shared = shared;
+ 				shared = NULL;
+ 			}
+ #ifdef CONFIG_NUMA
+ 			if (!l3->alien) {
+ 				l3->alien = alien;
+ 				alien = NULL;
+ 			}
+ #endif
+ 			spin_unlock_irq(&l3->list_lock);
+ 			kfree(shared);
+ 			free_alien_cache(alien);
+ 		}
+ 		break;
+ 	case CPU_ONLINE:
+ 	case CPU_ONLINE_FROZEN:
+ 		start_cpu_timer(cpu);
+ 		break;
+ #ifdef CONFIG_HOTPLUG_CPU
+   	case CPU_DOWN_PREPARE:
+   	case CPU_DOWN_PREPARE_FROZEN:
+ 		/*
+ 		 * Shutdown cache reaper. Note that the cache_chain_mutex is
+ 		 * held so that if cache_reap() is invoked it cannot do
+ 		 * anything expensive but will only modify reap_work
+ 		 * and reschedule the timer.
+ 		*/
+ 		cancel_rearming_delayed_work(&per_cpu(reap_work, cpu));
+ 		/* Now the cache_reaper is guaranteed to be not running. */
+ 		per_cpu(reap_work, cpu).work.func = NULL;
+   		break;
+   	case CPU_DOWN_FAILED:
+   	case CPU_DOWN_FAILED_FROZEN:
+ 		start_cpu_timer(cpu);
+   		break;
+ 	case CPU_DEAD:
+ 	case CPU_DEAD_FROZEN:
+ 		/*
+ 		 * Even if all the cpus of a node are down, we don't free the
+ 		 * kmem_list3 of any cache. This to avoid a race between
+ 		 * cpu_down, and a kmalloc allocation from another cpu for
+ 		 * memory from the node of the cpu going down.  The list3
+ 		 * structure is usually allocated from kmem_cache_create() and
+ 		 * gets destroyed at kmem_cache_destroy().
+ 		 */
+ 		/* fall thru */
+ #endif
+ 	case CPU_UP_CANCELED:
+ 	case CPU_UP_CANCELED_FROZEN:
+ 		list_for_each_entry(cachep, &cache_chain, next) {
+ 			struct array_cache *nc;
+ 			struct array_cache *shared;
+ 			struct array_cache **alien;
+ 			cpumask_t mask;
+ 
+ 			mask = node_to_cpumask(node);
+ 			/* cpu is dead; no one can alloc from it. */
+ 			nc = cachep->array[cpu];
+ 			cachep->array[cpu] = NULL;
+ 			l3 = cachep->nodelists[node];
+ 
+ 			if (!l3)
+ 				goto free_array_cache;
+ 
+ 			spin_lock_irq(&l3->list_lock);
+ 
+ 			/* Free limit for this kmem_list3 */
+ 			l3->free_limit -= cachep->batchcount;
+ 			if (nc)
+ 				free_block(cachep, nc->entry, nc->avail, node);
+ 
+ 			if (!cpus_empty(mask)) {
+ 				spin_unlock_irq(&l3->list_lock);
+ 				goto free_array_cache;
+ 			}
+ 
+ 			shared = l3->shared;
+ 			if (shared) {
+ 				free_block(cachep, shared->entry,
+ 					   shared->avail, node);
+ 				l3->shared = NULL;
+ 			}
+ 
+ 			alien = l3->alien;
+ 			l3->alien = NULL;
+ 
+ 			spin_unlock_irq(&l3->list_lock);
+ 
+ 			kfree(shared);
+ 			if (alien) {
+ 				drain_alien_cache(cachep, alien);
+ 				free_alien_cache(alien);
+ 			}
+ free_array_cache:
+ 			kfree(nc);
+ 		}
+ 		/*
+ 		 * In the previous loop, all the objects were freed to
+ 		 * the respective cache's slabs,  now we can go ahead and
+ 		 * shrink each nodelist to its limit.
+ 		 */
+ 		list_for_each_entry(cachep, &cache_chain, next) {
+ 			l3 = cachep->nodelists[node];
+ 			if (!l3)
+ 				continue;
+ 			drain_freelist(cachep, l3, l3->free_objects);
+ 		}
+ 		break;
+ 	case CPU_LOCK_RELEASE:
+ 		mutex_unlock(&cache_chain_mutex);
+ 		break;
+ 	}
+ 	return NOTIFY_OK;
+ bad:
+ 	return NOTIFY_BAD;
+ }
+ 
+ static struct notifier_block __cpuinitdata cpucache_notifier = {
+ 	&cpuup_callback, NULL, 0
+ };
+ 
+ /*
+  * swap the static kmem_list3 with kmalloced memory
+  */
+ static void init_list(struct kmem_cache *cachep, struct kmem_list3 *list,
+ 			int nodeid)
+ {
+ 	struct kmem_list3 *ptr;
+ 
+ 	ptr = kmalloc_node(sizeof(struct kmem_list3), GFP_KERNEL, nodeid);
+ 	BUG_ON(!ptr);
+ 
+ 	local_irq_disable();
+ 	memcpy(ptr, list, sizeof(struct kmem_list3));
+ 	/*
+ 	 * Do not assume that spinlocks can be initialized via memcpy:
+ 	 */
+ 	spin_lock_init(&ptr->list_lock);
+ 
+ 	MAKE_ALL_LISTS(cachep, ptr, nodeid);
+ 	cachep->nodelists[nodeid] = ptr;
+ 	local_irq_enable();
+ }
+ 
+ /*
+  * Initialisation.  Called after the page allocator have been initialised and
+  * before smp_init().
+  */
+ void __init kmem_cache_init(void)
+ {
+ 	size_t left_over;
+ 	struct cache_sizes *sizes;
+ 	struct cache_names *names;
+ 	int i;
+ 	int order;
+ 	int node;
+ 
+ 	if (num_possible_nodes() == 1) {
+ 		use_alien_caches = 0;
+ 		numa_platform = 0;
+ 	}
+ 
+ 	for (i = 0; i < NUM_INIT_LISTS; i++) {
+ 		kmem_list3_init(&initkmem_list3[i]);
+ 		if (i < MAX_NUMNODES)
+ 			cache_cache.nodelists[i] = NULL;
+ 	}
+ 
+ 	/*
+ 	 * Fragmentation resistance on low memory - only use bigger
+ 	 * page orders on machines with more than 32MB of memory.
+ 	 */
+ 	if (num_physpages > (32 << 20) >> PAGE_SHIFT)
+ 		slab_break_gfp_order = BREAK_GFP_ORDER_HI;
+ 
+ 	/* Bootstrap is tricky, because several objects are allocated
+ 	 * from caches that do not exist yet:
+ 	 * 1) initialize the cache_cache cache: it contains the struct
+ 	 *    kmem_cache structures of all caches, except cache_cache itself:
+ 	 *    cache_cache is statically allocated.
+ 	 *    Initially an __init data area is used for the head array and the
+ 	 *    kmem_list3 structures, it's replaced with a kmalloc allocated
+ 	 *    array at the end of the bootstrap.
+ 	 * 2) Create the first kmalloc cache.
+ 	 *    The struct kmem_cache for the new cache is allocated normally.
+ 	 *    An __init data area is used for the head array.
+ 	 * 3) Create the remaining kmalloc caches, with minimally sized
+ 	 *    head arrays.
+ 	 * 4) Replace the __init data head arrays for cache_cache and the first
+ 	 *    kmalloc cache with kmalloc allocated arrays.
+ 	 * 5) Replace the __init data for kmem_list3 for cache_cache and
+ 	 *    the other cache's with kmalloc allocated memory.
+ 	 * 6) Resize the head arrays of the kmalloc caches to their final sizes.
+ 	 */
+ 
+ 	node = numa_node_id();
+ 
+ 	/* 1) create the cache_cache */
+ 	INIT_LIST_HEAD(&cache_chain);
+ 	list_add(&cache_cache.next, &cache_chain);
+ 	cache_cache.colour_off = cache_line_size();
+ 	cache_cache.array[smp_processor_id()] = &initarray_cache.cache;
+ 	cache_cache.nodelists[node] = &initkmem_list3[CACHE_CACHE];
+ 
+ 	/*
+ 	 * struct kmem_cache size depends on nr_node_ids, which
+ 	 * can be less than MAX_NUMNODES.
+ 	 */
+ 	cache_cache.buffer_size = offsetof(struct kmem_cache, nodelists) +
+ 				 nr_node_ids * sizeof(struct kmem_list3 *);
+ #if DEBUG
+ 	cache_cache.obj_size = cache_cache.buffer_size;
+ #endif
+ 	cache_cache.buffer_size = ALIGN(cache_cache.buffer_size,
+ 					cache_line_size());
+ 	cache_cache.reciprocal_buffer_size =
+ 		reciprocal_value(cache_cache.buffer_size);
+ 
+ 	for (order = 0; order < MAX_ORDER; order++) {
+ 		cache_estimate(order, cache_cache.buffer_size,
+ 			cache_line_size(), 0, &left_over, &cache_cache.num);
+ 		if (cache_cache.num)
+ 			break;
+ 	}
+ 	BUG_ON(!cache_cache.num);
+ 	cache_cache.gfporder = order;
+ 	cache_cache.colour = left_over / cache_cache.colour_off;
+ 	cache_cache.slab_size = ALIGN(cache_cache.num * sizeof(kmem_bufctl_t) +
+ 				      sizeof(struct slab), cache_line_size());
+ 
+ 	/* 2+3) create the kmalloc caches */
+ 	sizes = malloc_sizes;
+ 	names = cache_names;
+ 
+ 	/*
+ 	 * Initialize the caches that provide memory for the array cache and the
+ 	 * kmem_list3 structures first.  Without this, further allocations will
+ 	 * bug.
+ 	 */
+ 
+ 	sizes[INDEX_AC].cs_cachep = kmem_cache_create(names[INDEX_AC].name,
+ 					sizes[INDEX_AC].cs_size,
+ 					ARCH_KMALLOC_MINALIGN,
+ 					ARCH_KMALLOC_FLAGS|SLAB_PANIC,
+ 					NULL);
+ 
+ 	if (INDEX_AC != INDEX_L3) {
+ 		sizes[INDEX_L3].cs_cachep =
+ 			kmem_cache_create(names[INDEX_L3].name,
+ 				sizes[INDEX_L3].cs_size,
+ 				ARCH_KMALLOC_MINALIGN,
+ 				ARCH_KMALLOC_FLAGS|SLAB_PANIC,
+ 				NULL);
+ 	}
+ 
+ 	slab_early_init = 0;
+ 
+ 	while (sizes->cs_size != ULONG_MAX) {
+ 		/*
+ 		 * For performance, all the general caches are L1 aligned.
+ 		 * This should be particularly beneficial on SMP boxes, as it
+ 		 * eliminates "false sharing".
+ 		 * Note for systems short on memory removing the alignment will
+ 		 * allow tighter packing of the smaller caches.
+ 		 */
+ 		if (!sizes->cs_cachep) {
+ 			sizes->cs_cachep = kmem_cache_create(names->name,
+ 					sizes->cs_size,
+ 					ARCH_KMALLOC_MINALIGN,
+ 					ARCH_KMALLOC_FLAGS|SLAB_PANIC,
+ 					NULL);
+ 		}
+ #ifdef CONFIG_ZONE_DMA
+ 		sizes->cs_dmacachep = kmem_cache_create(
+ 					names->name_dma,
+ 					sizes->cs_size,
+ 					ARCH_KMALLOC_MINALIGN,
+ 					ARCH_KMALLOC_FLAGS|SLAB_CACHE_DMA|
+ 						SLAB_PANIC,
+ 					NULL);
+ #endif
+ 		sizes++;
+ 		names++;
+ 	}
+ 	/* 4) Replace the bootstrap head arrays */
+ 	{
+ 		struct array_cache *ptr;
+ 
+ 		ptr = kmalloc(sizeof(struct arraycache_init), GFP_KERNEL);
+ 
+ 		local_irq_disable();
+ 		BUG_ON(cpu_cache_get(&cache_cache) != &initarray_cache.cache);
+ 		memcpy(ptr, cpu_cache_get(&cache_cache),
+ 		       sizeof(struct arraycache_init));
+ 		/*
+ 		 * Do not assume that spinlocks can be initialized via memcpy:
+ 		 */
+ 		spin_lock_init(&ptr->lock);
+ 
+ 		cache_cache.array[smp_processor_id()] = ptr;
+ 		local_irq_enable();
+ 
+ 		ptr = kmalloc(sizeof(struct arraycache_init), GFP_KERNEL);
+ 
+ 		local_irq_disable();
+ 		BUG_ON(cpu_cache_get(malloc_sizes[INDEX_AC].cs_cachep)
+ 		       != &initarray_generic.cache);
+ 		memcpy(ptr, cpu_cache_get(malloc_sizes[INDEX_AC].cs_cachep),
+ 		       sizeof(struct arraycache_init));
+ 		/*
+ 		 * Do not assume that spinlocks can be initialized via memcpy:
+ 		 */
+ 		spin_lock_init(&ptr->lock);
+ 
+ 		malloc_sizes[INDEX_AC].cs_cachep->array[smp_processor_id()] =
+ 		    ptr;
+ 		local_irq_enable();
+ 	}
+ 	/* 5) Replace the bootstrap kmem_list3's */
+ 	{
+ 		int nid;
+ 
+ 		/* Replace the static kmem_list3 structures for the boot cpu */
+ 		init_list(&cache_cache, &initkmem_list3[CACHE_CACHE], node);
+ 
+ 		for_each_online_node(nid) {
+ 			init_list(malloc_sizes[INDEX_AC].cs_cachep,
+ 				  &initkmem_list3[SIZE_AC + nid], nid);
+ 
+ 			if (INDEX_AC != INDEX_L3) {
+ 				init_list(malloc_sizes[INDEX_L3].cs_cachep,
+ 					  &initkmem_list3[SIZE_L3 + nid], nid);
+ 			}
+ 		}
+ 	}
+ 
+ 	/* 6) resize the head arrays to their final sizes */
+ 	{
+ 		struct kmem_cache *cachep;
+ 		mutex_lock(&cache_chain_mutex);
+ 		list_for_each_entry(cachep, &cache_chain, next)
+ 			if (enable_cpucache(cachep))
+ 				BUG();
+ 		mutex_unlock(&cache_chain_mutex);
+ 	}
+ 
+ 	/* Annotate slab for lockdep -- annotate the malloc caches */
+ 	init_lock_keys();
+ 
+ 
+ 	/* Done! */
+ 	g_cpucache_up = FULL;
+ 
+ 	/*
+ 	 * Register a cpu startup notifier callback that initializes
+ 	 * cpu_cache_get for all new cpus
+ 	 */
+ 	register_cpu_notifier(&cpucache_notifier);
+ 
+ 	/*
+ 	 * The reap timers are started later, with a module init call: That part
+ 	 * of the kernel is not yet operational.
+ 	 */
+ }
+ 
+ static int __init cpucache_init(void)
+ {
+ 	int cpu;
+ 
+ 	/*
+ 	 * Register the timers that return unneeded pages to the page allocator
+ 	 */
+ 	for_each_online_cpu(cpu)
+ 		start_cpu_timer(cpu);
+ 	return 0;
+ }
+ __initcall(cpucache_init);
+ 
+ /*
+  * Interface to system's page allocator. No need to hold the cache-lock.
+  *
+  * If we requested dmaable memory, we will get it. Even if we
+  * did not request dmaable memory, we might get it, but that
+  * would be relatively rare and ignorable.
+  */
+ static void *kmem_getpages(struct kmem_cache *cachep, gfp_t flags, int nodeid)
+ {
+ 	struct page *page;
+ 	int nr_pages;
+ 	int i;
+ 
+ #ifndef CONFIG_MMU
+ 	/*
+ 	 * Nommu uses slab's for process anonymous memory allocations, and thus
+ 	 * requires __GFP_COMP to properly refcount higher order allocations
+ 	 */
+ 	flags |= __GFP_COMP;
+ #endif
+ 
+ 	flags |= cachep->gfpflags;
+ 
+ 	page = alloc_pages_node(nodeid, flags, cachep->gfporder);
+ 	if (!page)
+ 		return NULL;
+ 
+ 	nr_pages = (1 << cachep->gfporder);
+ 	if (cachep->flags & SLAB_RECLAIM_ACCOUNT)
+ 		add_zone_page_state(page_zone(page),
+ 			NR_SLAB_RECLAIMABLE, nr_pages);
+ 	else
+ 		add_zone_page_state(page_zone(page),
+ 			NR_SLAB_UNRECLAIMABLE, nr_pages);
+ 	for (i = 0; i < nr_pages; i++)
+ 		__SetPageSlab(page + i);
+ 	return page_address(page);
+ }
+ 
+ /*
+  * Interface to system's page release.
+  */
+ static void kmem_freepages(struct kmem_cache *cachep, void *addr)
+ {
+ 	unsigned long i = (1 << cachep->gfporder);
+ 	struct page *page = virt_to_page(addr);
+ 	const unsigned long nr_freed = i;
+ 
+ 	if (cachep->flags & SLAB_RECLAIM_ACCOUNT)
+ 		sub_zone_page_state(page_zone(page),
+ 				NR_SLAB_RECLAIMABLE, nr_freed);
+ 	else
+ 		sub_zone_page_state(page_zone(page),
+ 				NR_SLAB_UNRECLAIMABLE, nr_freed);
+ 	while (i--) {
+ 		BUG_ON(!PageSlab(page));
+ 		__ClearPageSlab(page);
+ 		page++;
+ 	}
+ 	if (current->reclaim_state)
+ 		current->reclaim_state->reclaimed_slab += nr_freed;
+ 	free_pages((unsigned long)addr, cachep->gfporder);
+ }
+ 
+ static void kmem_rcu_free(struct rcu_head *head)
+ {
+ 	struct slab_rcu *slab_rcu = (struct slab_rcu *)head;
+ 	struct kmem_cache *cachep = slab_rcu->cachep;
+ 
+ 	kmem_freepages(cachep, slab_rcu->addr);
+ 	if (OFF_SLAB(cachep))
+ 		kmem_cache_free(cachep->slabp_cache, slab_rcu);
+ }
+ 
+ #if DEBUG
+ 
+ #ifdef CONFIG_DEBUG_PAGEALLOC
+ static void store_stackinfo(struct kmem_cache *cachep, unsigned long *addr,
+ 			    unsigned long caller)
+ {
+ 	int size = obj_size(cachep);
+ 
+ 	addr = (unsigned long *)&((char *)addr)[obj_offset(cachep)];
+ 
+ 	if (size < 5 * sizeof(unsigned long))
+ 		return;
+ 
+ 	*addr++ = 0x12345678;
+ 	*addr++ = caller;
+ 	*addr++ = smp_processor_id();
+ 	size -= 3 * sizeof(unsigned long);
+ 	{
+ 		unsigned long *sptr = &caller;
+ 		unsigned long svalue;
+ 
+ 		while (!kstack_end(sptr)) {
+ 			svalue = *sptr++;
+ 			if (kernel_text_address(svalue)) {
+ 				*addr++ = svalue;
+ 				size -= sizeof(unsigned long);
+ 				if (size <= sizeof(unsigned long))
+ 					break;
+ 			}
+ 		}
+ 
+ 	}
+ 	*addr++ = 0x87654321;
+ }
+ #endif
+ 
+ static void poison_obj(struct kmem_cache *cachep, void *addr, unsigned char val)
+ {
+ 	int size = obj_size(cachep);
+ 	addr = &((char *)addr)[obj_offset(cachep)];
+ 
+ 	memset(addr, val, size);
+ 	*(unsigned char *)(addr + size - 1) = POISON_END;
+ }
+ 
+ static void dump_line(char *data, int offset, int limit)
+ {
+ 	int i;
+ 	unsigned char error = 0;
+ 	int bad_count = 0;
+ 
+ 	printk(KERN_ERR "%03x:", offset);
+ 	for (i = 0; i < limit; i++) {
+ 		if (data[offset + i] != POISON_FREE) {
+ 			error = data[offset + i];
+ 			bad_count++;
+ 		}
+ 		printk(" %02x", (unsigned char)data[offset + i]);
+ 	}
+ 	printk("\n");
+ 
+ 	if (bad_count == 1) {
+ 		error ^= POISON_FREE;
+ 		if (!(error & (error - 1))) {
+ 			printk(KERN_ERR "Single bit error detected. Probably "
+ 					"bad RAM.\n");
+ #ifdef CONFIG_X86
+ 			printk(KERN_ERR "Run memtest86+ or a similar memory "
+ 					"test tool.\n");
+ #else
+ 			printk(KERN_ERR "Run a memory test tool.\n");
+ #endif
+ 		}
+ 	}
+ }
+ #endif
+ 
+ #if DEBUG
+ 
+ static void print_objinfo(struct kmem_cache *cachep, void *objp, int lines)
+ {
+ 	int i, size;
+ 	char *realobj;
+ 
+ 	if (cachep->flags & SLAB_RED_ZONE) {
+ 		printk(KERN_ERR "Redzone: 0x%llx/0x%llx.\n",
+ 			*dbg_redzone1(cachep, objp),
+ 			*dbg_redzone2(cachep, objp));
+ 	}
+ 
+ 	if (cachep->flags & SLAB_STORE_USER) {
+ 		printk(KERN_ERR "Last user: [<%p>]",
+ 			*dbg_userword(cachep, objp));
+ 		print_symbol("(%s)",
+ 				(unsigned long)*dbg_userword(cachep, objp));
+ 		printk("\n");
+ 	}
+ 	realobj = (char *)objp + obj_offset(cachep);
+ 	size = obj_size(cachep);
+ 	for (i = 0; i < size && lines; i += 16, lines--) {
+ 		int limit;
+ 		limit = 16;
+ 		if (i + limit > size)
+ 			limit = size - i;
+ 		dump_line(realobj, i, limit);
+ 	}
+ }
+ 
+ static void check_poison_obj(struct kmem_cache *cachep, void *objp)
+ {
+ 	char *realobj;
+ 	int size, i;
+ 	int lines = 0;
+ 
+ 	realobj = (char *)objp + obj_offset(cachep);
+ 	size = obj_size(cachep);
+ 
+ 	for (i = 0; i < size; i++) {
+ 		char exp = POISON_FREE;
+ 		if (i == size - 1)
+ 			exp = POISON_END;
+ 		if (realobj[i] != exp) {
+ 			int limit;
+ 			/* Mismatch ! */
+ 			/* Print header */
+ 			if (lines == 0) {
+ 				printk(KERN_ERR
+ 					"Slab corruption: %s start=%p, len=%d\n",
+ 					cachep->name, realobj, size);
+ 				print_objinfo(cachep, objp, 0);
+ 			}
+ 			/* Hexdump the affected line */
+ 			i = (i / 16) * 16;
+ 			limit = 16;
+ 			if (i + limit > size)
+ 				limit = size - i;
+ 			dump_line(realobj, i, limit);
+ 			i += 16;
+ 			lines++;
+ 			/* Limit to 5 lines */
+ 			if (lines > 5)
+ 				break;
+ 		}
+ 	}
+ 	if (lines != 0) {
+ 		/* Print some data about the neighboring objects, if they
+ 		 * exist:
+ 		 */
+ 		struct slab *slabp = virt_to_slab(objp);
+ 		unsigned int objnr;
+ 
+ 		objnr = obj_to_index(cachep, slabp, objp);
+ 		if (objnr) {
+ 			objp = index_to_obj(cachep, slabp, objnr - 1);
+ 			realobj = (char *)objp + obj_offset(cachep);
+ 			printk(KERN_ERR "Prev obj: start=%p, len=%d\n",
+ 			       realobj, size);
+ 			print_objinfo(cachep, objp, 2);
+ 		}
+ 		if (objnr + 1 < cachep->num) {
+ 			objp = index_to_obj(cachep, slabp, objnr + 1);
+ 			realobj = (char *)objp + obj_offset(cachep);
+ 			printk(KERN_ERR "Next obj: start=%p, len=%d\n",
+ 			       realobj, size);
+ 			print_objinfo(cachep, objp, 2);
+ 		}
+ 	}
+ }
+ #endif
+ 
+ #if DEBUG
+ /**
+  * slab_destroy_objs - destroy a slab and its objects
+  * @cachep: cache pointer being destroyed
+  * @slabp: slab pointer being destroyed
+  *
+  * Call the registered destructor for each object in a slab that is being
+  * destroyed.
+  */
+ static void slab_destroy_objs(struct kmem_cache *cachep, struct slab *slabp)
+ {
+ 	int i;
+ 	for (i = 0; i < cachep->num; i++) {
+ 		void *objp = index_to_obj(cachep, slabp, i);
+ 
+ 		if (cachep->flags & SLAB_POISON) {
+ #ifdef CONFIG_DEBUG_PAGEALLOC
+ 			if (cachep->buffer_size % PAGE_SIZE == 0 &&
+ 					OFF_SLAB(cachep))
+ 				kernel_map_pages(virt_to_page(objp),
+ 					cachep->buffer_size / PAGE_SIZE, 1);
+ 			else
+ 				check_poison_obj(cachep, objp);
+ #else
+ 			check_poison_obj(cachep, objp);
+ #endif
+ 		}
+ 		if (cachep->flags & SLAB_RED_ZONE) {
+ 			if (*dbg_redzone1(cachep, objp) != RED_INACTIVE)
+ 				slab_error(cachep, "start of a freed object "
+ 					   "was overwritten");
+ 			if (*dbg_redzone2(cachep, objp) != RED_INACTIVE)
+ 				slab_error(cachep, "end of a freed object "
+ 					   "was overwritten");
+ 		}
+ 	}
+ }
+ #else
+ static void slab_destroy_objs(struct kmem_cache *cachep, struct slab *slabp)
+ {
+ }
+ #endif
+ 
+ /**
+  * slab_destroy - destroy and release all objects in a slab
+  * @cachep: cache pointer being destroyed
+  * @slabp: slab pointer being destroyed
+  *
+  * Destroy all the objs in a slab, and release the mem back to the system.
+  * Before calling the slab must have been unlinked from the cache.  The
+  * cache-lock is not held/needed.
+  */
+ static void slab_destroy(struct kmem_cache *cachep, struct slab *slabp)
+ {
+ 	void *addr = slabp->s_mem - slabp->colouroff;
+ 
+ 	slab_destroy_objs(cachep, slabp);
+ 	if (unlikely(cachep->flags & SLAB_DESTROY_BY_RCU)) {
+ 		struct slab_rcu *slab_rcu;
+ 
+ 		slab_rcu = (struct slab_rcu *)slabp;
+ 		slab_rcu->cachep = cachep;
+ 		slab_rcu->addr = addr;
+ 		call_rcu(&slab_rcu->head, kmem_rcu_free);
+ 	} else {
+ 		kmem_freepages(cachep, addr);
+ 		if (OFF_SLAB(cachep))
+ 			kmem_cache_free(cachep->slabp_cache, slabp);
+ 	}
+ }
+ 
+ /*
+  * For setting up all the kmem_list3s for cache whose buffer_size is same as
+  * size of kmem_list3.
+  */
+ static void __init set_up_list3s(struct kmem_cache *cachep, int index)
+ {
+ 	int node;
+ 
+ 	for_each_online_node(node) {
+ 		cachep->nodelists[node] = &initkmem_list3[index + node];
+ 		cachep->nodelists[node]->next_reap = jiffies +
+ 		    REAPTIMEOUT_LIST3 +
+ 		    ((unsigned long)cachep) % REAPTIMEOUT_LIST3;
+ 	}
+ }
+ 
+ static void __kmem_cache_destroy(struct kmem_cache *cachep)
+ {
+ 	int i;
+ 	struct kmem_list3 *l3;
+ 
+ 	for_each_online_cpu(i)
+ 	    kfree(cachep->array[i]);
+ 
+ 	/* NUMA: free the list3 structures */
+ 	for_each_online_node(i) {
+ 		l3 = cachep->nodelists[i];
+ 		if (l3) {
+ 			kfree(l3->shared);
+ 			free_alien_cache(l3->alien);
+ 			kfree(l3);
+ 		}
+ 	}
+ 	kmem_cache_free(&cache_cache, cachep);
+ }
+ 
+ 
+ /**
+  * calculate_slab_order - calculate size (page order) of slabs
+  * @cachep: pointer to the cache that is being created
+  * @size: size of objects to be created in this cache.
+  * @align: required alignment for the objects.
+  * @flags: slab allocation flags
+  *
+  * Also calculates the number of objects per slab.
+  *
+  * This could be made much more intelligent.  For now, try to avoid using
+  * high order pages for slabs.  When the gfp() functions are more friendly
+  * towards high-order requests, this should be changed.
+  */
+ static size_t calculate_slab_order(struct kmem_cache *cachep,
+ 			size_t size, size_t align, unsigned long flags)
+ {
+ 	unsigned long offslab_limit;
+ 	size_t left_over = 0;
+ 	int gfporder;
+ 
+ 	for (gfporder = 0; gfporder <= KMALLOC_MAX_ORDER; gfporder++) {
+ 		unsigned int num;
+ 		size_t remainder;
+ 
+ 		cache_estimate(gfporder, size, align, flags, &remainder, &num);
+ 		if (!num)
+ 			continue;
+ 
+ 		if (flags & CFLGS_OFF_SLAB) {
+ 			/*
+ 			 * Max number of objs-per-slab for caches which
+ 			 * use off-slab slabs. Needed to avoid a possible
+ 			 * looping condition in cache_grow().
+ 			 */
+ 			offslab_limit = size - sizeof(struct slab);
+ 			offslab_limit /= sizeof(kmem_bufctl_t);
+ 
+  			if (num > offslab_limit)
+ 				break;
+ 		}
+ 
+ 		/* Found something acceptable - save it away */
+ 		cachep->num = num;
+ 		cachep->gfporder = gfporder;
+ 		left_over = remainder;
+ 
+ 		/*
+ 		 * A VFS-reclaimable slab tends to have most allocations
+ 		 * as GFP_NOFS and we really don't want to have to be allocating
+ 		 * higher-order pages when we are unable to shrink dcache.
+ 		 */
+ 		if (flags & SLAB_RECLAIM_ACCOUNT)
+ 			break;
+ 
+ 		/*
+ 		 * Large number of objects is good, but very large slabs are
+ 		 * currently bad for the gfp()s.
+ 		 */
+ 		if (gfporder >= slab_break_gfp_order)
+ 			break;
+ 
+ 		/*
+ 		 * Acceptable internal fragmentation?
+ 		 */
+ 		if (left_over * 8 <= (PAGE_SIZE << gfporder))
+ 			break;
+ 	}
+ 	return left_over;
+ }
+ 
+ static int __init_refok setup_cpu_cache(struct kmem_cache *cachep)
+ {
+ 	if (g_cpucache_up == FULL)
+ 		return enable_cpucache(cachep);
+ 
+ 	if (g_cpucache_up == NONE) {
+ 		/*
+ 		 * Note: the first kmem_cache_create must create the cache
+ 		 * that's used by kmalloc(24), otherwise the creation of
+ 		 * further caches will BUG().
+ 		 */
+ 		cachep->array[smp_processor_id()] = &initarray_generic.cache;
+ 
+ 		/*
+ 		 * If the cache that's used by kmalloc(sizeof(kmem_list3)) is
+ 		 * the first cache, then we need to set up all its list3s,
+ 		 * otherwise the creation of further caches will BUG().
+ 		 */
+ 		set_up_list3s(cachep, SIZE_AC);
+ 		if (INDEX_AC == INDEX_L3)
+ 			g_cpucache_up = PARTIAL_L3;
+ 		else
+ 			g_cpucache_up = PARTIAL_AC;
+ 	} else {
+ 		cachep->array[smp_processor_id()] =
+ 			kmalloc(sizeof(struct arraycache_init), GFP_KERNEL);
+ 
+ 		if (g_cpucache_up == PARTIAL_AC) {
+ 			set_up_list3s(cachep, SIZE_L3);
+ 			g_cpucache_up = PARTIAL_L3;
+ 		} else {
+ 			int node;
+ 			for_each_online_node(node) {
+ 				cachep->nodelists[node] =
+ 				    kmalloc_node(sizeof(struct kmem_list3),
+ 						GFP_KERNEL, node);
+ 				BUG_ON(!cachep->nodelists[node]);
+ 				kmem_list3_init(cachep->nodelists[node]);
+ 			}
+ 		}
+ 	}
+ 	cachep->nodelists[numa_node_id()]->next_reap =
+ 			jiffies + REAPTIMEOUT_LIST3 +
+ 			((unsigned long)cachep) % REAPTIMEOUT_LIST3;
+ 
+ 	cpu_cache_get(cachep)->avail = 0;
+ 	cpu_cache_get(cachep)->limit = BOOT_CPUCACHE_ENTRIES;
+ 	cpu_cache_get(cachep)->batchcount = 1;
+ 	cpu_cache_get(cachep)->touched = 0;
+ 	cachep->batchcount = 1;
+ 	cachep->limit = BOOT_CPUCACHE_ENTRIES;
+ 	return 0;
+ }
+ 
+ /**
+  * kmem_cache_create - Create a cache.
+  * @name: A string which is used in /proc/slabinfo to identify this cache.
+  * @size: The size of objects to be created in this cache.
+  * @align: The required alignment for the objects.
+  * @flags: SLAB flags
+  * @ctor: A constructor for the objects.
+  *
+  * Returns a ptr to the cache on success, NULL on failure.
+  * Cannot be called within a int, but can be interrupted.
+  * The @ctor is run when new pages are allocated by the cache.
+  *
+  * @name must be valid until the cache is destroyed. This implies that
+  * the module calling this has to destroy the cache before getting unloaded.
+  *
+  * The flags are
+  *
+  * %SLAB_POISON - Poison the slab with a known test pattern (a5a5a5a5)
+  * to catch references to uninitialised memory.
+  *
+  * %SLAB_RED_ZONE - Insert `Red' zones around the allocated memory to check
+  * for buffer overruns.
+  *
+  * %SLAB_HWCACHE_ALIGN - Align the objects in this cache to a hardware
+  * cacheline.  This can be beneficial if you're counting cycles as closely
+  * as davem.
+  */
+ struct kmem_cache *
+ kmem_cache_create (const char *name, size_t size, size_t align,
+ 	unsigned long flags,
+ 	void (*ctor)(void*, struct kmem_cache *, unsigned long))
+ {
+ 	size_t left_over, slab_size, ralign;
+ 	struct kmem_cache *cachep = NULL, *pc;
+ 
+ 	/*
+ 	 * Sanity checks... these are all serious usage bugs.
+ 	 */
+ 	if (!name || in_interrupt() || (size < BYTES_PER_WORD) ||
+ 	    size > KMALLOC_MAX_SIZE) {
+ 		printk(KERN_ERR "%s: Early error in slab %s\n", __FUNCTION__,
+ 				name);
+ 		BUG();
+ 	}
+ 
+ 	/*
+ 	 * We use cache_chain_mutex to ensure a consistent view of
+ 	 * cpu_online_map as well.  Please see cpuup_callback
+ 	 */
+ 	mutex_lock(&cache_chain_mutex);
+ 
+ 	list_for_each_entry(pc, &cache_chain, next) {
+ 		char tmp;
+ 		int res;
+ 
+ 		/*
+ 		 * This happens when the module gets unloaded and doesn't
+ 		 * destroy its slab cache and no-one else reuses the vmalloc
+ 		 * area of the module.  Print a warning.
+ 		 */
+ 		res = probe_kernel_address(pc->name, tmp);
+ 		if (res) {
+ 			printk(KERN_ERR
+ 			       "SLAB: cache with size %d has lost its name\n",
+ 			       pc->buffer_size);
+ 			continue;
+ 		}
+ 
+ 		if (!strcmp(pc->name, name)) {
+ 			printk(KERN_ERR
+ 			       "kmem_cache_create: duplicate cache %s\n", name);
+ 			dump_stack();
+ 			goto oops;
+ 		}
+ 	}
+ 
+ #if DEBUG
+ 	WARN_ON(strchr(name, ' '));	/* It confuses parsers */
+ #if FORCED_DEBUG
+ 	/*
+ 	 * Enable redzoning and last user accounting, except for caches with
+ 	 * large objects, if the increased size would increase the object size
+ 	 * above the next power of two: caches with object sizes just above a
+ 	 * power of two have a significant amount of internal fragmentation.
+ 	 */
+ 	if (size < 4096 || fls(size - 1) == fls(size-1 + REDZONE_ALIGN +
+ 						2 * sizeof(unsigned long long)))
+ 		flags |= SLAB_RED_ZONE | SLAB_STORE_USER;
+ 	if (!(flags & SLAB_DESTROY_BY_RCU))
+ 		flags |= SLAB_POISON;
+ #endif
+ 	if (flags & SLAB_DESTROY_BY_RCU)
+ 		BUG_ON(flags & SLAB_POISON);
+ #endif
+ 	/*
+ 	 * Always checks flags, a caller might be expecting debug support which
+ 	 * isn't available.
+ 	 */
+ 	BUG_ON(flags & ~CREATE_MASK);
+ 
+ 	/*
+ 	 * Check that size is in terms of words.  This is needed to avoid
+ 	 * unaligned accesses for some archs when redzoning is used, and makes
+ 	 * sure any on-slab bufctl's are also correctly aligned.
+ 	 */
+ 	if (size & (BYTES_PER_WORD - 1)) {
+ 		size += (BYTES_PER_WORD - 1);
+ 		size &= ~(BYTES_PER_WORD - 1);
+ 	}
+ 
+ 	/* calculate the final buffer alignment: */
+ 
+ 	/* 1) arch recommendation: can be overridden for debug */
+ 	if (flags & SLAB_HWCACHE_ALIGN) {
+ 		/*
+ 		 * Default alignment: as specified by the arch code.  Except if
+ 		 * an object is really small, then squeeze multiple objects into
+ 		 * one cacheline.
+ 		 */
+ 		ralign = cache_line_size();
+ 		while (size <= ralign / 2)
+ 			ralign /= 2;
+ 	} else {
+ 		ralign = BYTES_PER_WORD;
+ 	}
+ 
+ 	/*
+ 	 * Redzoning and user store require word alignment or possibly larger.
+ 	 * Note this will be overridden by architecture or caller mandated
+ 	 * alignment if either is greater than BYTES_PER_WORD.
+ 	 */
+ 	if (flags & SLAB_STORE_USER)
+ 		ralign = BYTES_PER_WORD;
+ 
+ 	if (flags & SLAB_RED_ZONE) {
+ 		ralign = REDZONE_ALIGN;
+ 		/* If redzoning, ensure that the second redzone is suitably
+ 		 * aligned, by adjusting the object size accordingly. */
+ 		size += REDZONE_ALIGN - 1;
+ 		size &= ~(REDZONE_ALIGN - 1);
+ 	}
+ 
+ 	/* 2) arch mandated alignment */
+ 	if (ralign < ARCH_SLAB_MINALIGN) {
+ 		ralign = ARCH_SLAB_MINALIGN;
+ 	}
+ 	/* 3) caller mandated alignment */
+ 	if (ralign < align) {
+ 		ralign = align;
+ 	}
+ 	/* disable debug if necessary */
+ 	if (ralign > __alignof__(unsigned long long))
+ 		flags &= ~(SLAB_RED_ZONE | SLAB_STORE_USER);
+ 	/*
+ 	 * 4) Store it.
+ 	 */
+ 	align = ralign;
+ 
+ 	/* Get cache's description obj. */
+ 	cachep = kmem_cache_zalloc(&cache_cache, GFP_KERNEL);
+ 	if (!cachep)
+ 		goto oops;
+ 
+ #if DEBUG
+ 	cachep->obj_size = size;
+ 
+ 	/*
+ 	 * Both debugging options require word-alignment which is calculated
+ 	 * into align above.
+ 	 */
+ 	if (flags & SLAB_RED_ZONE) {
+ 		/* add space for red zone words */
+ 		cachep->obj_offset += sizeof(unsigned long long);
+ 		size += 2 * sizeof(unsigned long long);
+ 	}
+ 	if (flags & SLAB_STORE_USER) {
+ 		/* user store requires one word storage behind the end of
+ 		 * the real object. But if the second red zone needs to be
+ 		 * aligned to 64 bits, we must allow that much space.
+ 		 */
+ 		if (flags & SLAB_RED_ZONE)
+ 			size += REDZONE_ALIGN;
+ 		else
+ 			size += BYTES_PER_WORD;
+ 	}
+ #if FORCED_DEBUG && defined(CONFIG_DEBUG_PAGEALLOC)
+ 	if (size >= malloc_sizes[INDEX_L3 + 1].cs_size
+ 	    && cachep->obj_size > cache_line_size() && size < PAGE_SIZE) {
+ 		cachep->obj_offset += PAGE_SIZE - size;
+ 		size = PAGE_SIZE;
+ 	}
+ #endif
+ #endif
+ 
+ 	/*
+ 	 * Determine if the slab management is 'on' or 'off' slab.
+ 	 * (bootstrapping cannot cope with offslab caches so don't do
+ 	 * it too early on.)
+ 	 */
+ 	if ((size >= (PAGE_SIZE >> 3)) && !slab_early_init)
+ 		/*
+ 		 * Size is large, assume best to place the slab management obj
+ 		 * off-slab (should allow better packing of objs).
+ 		 */
+ 		flags |= CFLGS_OFF_SLAB;
+ 
+ 	size = ALIGN(size, align);
+ 
+ 	left_over = calculate_slab_order(cachep, size, align, flags);
+ 
+ 	if (!cachep->num) {
+ 		printk(KERN_ERR
+ 		       "kmem_cache_create: couldn't create cache %s.\n", name);
+ 		kmem_cache_free(&cache_cache, cachep);
+ 		cachep = NULL;
+ 		goto oops;
+ 	}
+ 	slab_size = ALIGN(cachep->num * sizeof(kmem_bufctl_t)
+ 			  + sizeof(struct slab), align);
+ 
+ 	/*
+ 	 * If the slab has been placed off-slab, and we have enough space then
+ 	 * move it on-slab. This is at the expense of any extra colouring.
+ 	 */
+ 	if (flags & CFLGS_OFF_SLAB && left_over >= slab_size) {
+ 		flags &= ~CFLGS_OFF_SLAB;
+ 		left_over -= slab_size;
+ 	}
+ 
+ 	if (flags & CFLGS_OFF_SLAB) {
+ 		/* really off slab. No need for manual alignment */
+ 		slab_size =
+ 		    cachep->num * sizeof(kmem_bufctl_t) + sizeof(struct slab);
+ 	}
+ 
+ 	cachep->colour_off = cache_line_size();
+ 	/* Offset must be a multiple of the alignment. */
+ 	if (cachep->colour_off < align)
+ 		cachep->colour_off = align;
+ 	cachep->colour = left_over / cachep->colour_off;
+ 	cachep->slab_size = slab_size;
+ 	cachep->flags = flags;
+ 	cachep->gfpflags = 0;
+ 	if (CONFIG_ZONE_DMA_FLAG && (flags & SLAB_CACHE_DMA))
+ 		cachep->gfpflags |= GFP_DMA;
+ 	cachep->buffer_size = size;
+ 	cachep->reciprocal_buffer_size = reciprocal_value(size);
+ 
+ 	if (flags & CFLGS_OFF_SLAB) {
+ 		cachep->slabp_cache = kmem_find_general_cachep(slab_size, 0u);
+ 		/*
+ 		 * This is a possibility for one of the malloc_sizes caches.
+ 		 * But since we go off slab only for object size greater than
+ 		 * PAGE_SIZE/8, and malloc_sizes gets created in ascending order,
+ 		 * this should not happen at all.
+ 		 * But leave a BUG_ON for some lucky dude.
+ 		 */
+ 		BUG_ON(ZERO_OR_NULL_PTR(cachep->slabp_cache));
+ 	}
+ 	cachep->ctor = ctor;
+ 	cachep->name = name;
+ 
+ 	if (setup_cpu_cache(cachep)) {
+ 		__kmem_cache_destroy(cachep);
+ 		cachep = NULL;
+ 		goto oops;
+ 	}
+ 
+ 	/* cache setup completed, link it into the list */
+ 	list_add(&cachep->next, &cache_chain);
+ oops:
+ 	if (!cachep && (flags & SLAB_PANIC))
+ 		panic("kmem_cache_create(): failed to create slab `%s'\n",
+ 		      name);
+ 	mutex_unlock(&cache_chain_mutex);
+ 	return cachep;
+ }
+ EXPORT_SYMBOL(kmem_cache_create);
+ 
+ #if DEBUG
+ static void check_irq_off(void)
+ {
+ 	BUG_ON(!irqs_disabled());
+ }
+ 
+ static void check_irq_on(void)
+ {
+ 	BUG_ON(irqs_disabled());
+ }
+ 
+ static void check_spinlock_acquired(struct kmem_cache *cachep)
+ {
+ #ifdef CONFIG_SMP
+ 	check_irq_off();
+ 	assert_spin_locked(&cachep->nodelists[numa_node_id()]->list_lock);
+ #endif
+ }
+ 
+ static void check_spinlock_acquired_node(struct kmem_cache *cachep, int node)
+ {
+ #ifdef CONFIG_SMP
+ 	check_irq_off();
+ 	assert_spin_locked(&cachep->nodelists[node]->list_lock);
+ #endif
+ }
+ 
+ #else
+ #define check_irq_off()	do { } while(0)
+ #define check_irq_on()	do { } while(0)
+ #define check_spinlock_acquired(x) do { } while(0)
+ #define check_spinlock_acquired_node(x, y) do { } while(0)
+ #endif
+ 
+ static void drain_array(struct kmem_cache *cachep, struct kmem_list3 *l3,
+ 			struct array_cache *ac,
+ 			int force, int node);
+ 
+ static void do_drain(void *arg)
+ {
+ 	struct kmem_cache *cachep = arg;
+ 	struct array_cache *ac;
+ 	int node = numa_node_id();
+ 
+ 	check_irq_off();
+ 	ac = cpu_cache_get(cachep);
+ 	spin_lock(&cachep->nodelists[node]->list_lock);
+ 	free_block(cachep, ac->entry, ac->avail, node);
+ 	spin_unlock(&cachep->nodelists[node]->list_lock);
+ 	ac->avail = 0;
+ }
+ 
+ static void drain_cpu_caches(struct kmem_cache *cachep)
+ {
+ 	struct kmem_list3 *l3;
+ 	int node;
+ 
+ 	on_each_cpu(do_drain, cachep, 1, 1);
+ 	check_irq_on();
+ 	for_each_online_node(node) {
+ 		l3 = cachep->nodelists[node];
+ 		if (l3 && l3->alien)
+ 			drain_alien_cache(cachep, l3->alien);
+ 	}
+ 
+ 	for_each_online_node(node) {
+ 		l3 = cachep->nodelists[node];
+ 		if (l3)
+ 			drain_array(cachep, l3, l3->shared, 1, node);
+ 	}
+ }
+ 
+ /*
+  * Remove slabs from the list of free slabs.
+  * Specify the number of slabs to drain in tofree.
+  *
+  * Returns the actual number of slabs released.
+  */
+ static int drain_freelist(struct kmem_cache *cache,
+ 			struct kmem_list3 *l3, int tofree)
+ {
+ 	struct list_head *p;
+ 	int nr_freed;
+ 	struct slab *slabp;
+ 
+ 	nr_freed = 0;
+ 	while (nr_freed < tofree && !list_empty(&l3->slabs_free)) {
+ 
+ 		spin_lock_irq(&l3->list_lock);
+ 		p = l3->slabs_free.prev;
+ 		if (p == &l3->slabs_free) {
+ 			spin_unlock_irq(&l3->list_lock);
+ 			goto out;
+ 		}
+ 
+ 		slabp = list_entry(p, struct slab, list);
+ #if DEBUG
+ 		BUG_ON(slabp->inuse);
+ #endif
+ 		list_del(&slabp->list);
+ 		/*
+ 		 * Safe to drop the lock. The slab is no longer linked
+ 		 * to the cache.
+ 		 */
+ 		l3->free_objects -= cache->num;
+ 		spin_unlock_irq(&l3->list_lock);
+ 		slab_destroy(cache, slabp);
+ 		nr_freed++;
+ 	}
+ out:
+ 	return nr_freed;
+ }
+ 
+ /* Called with cache_chain_mutex held to protect against cpu hotplug */
+ static int __cache_shrink(struct kmem_cache *cachep)
+ {
+ 	int ret = 0, i = 0;
+ 	struct kmem_list3 *l3;
+ 
+ 	drain_cpu_caches(cachep);
+ 
+ 	check_irq_on();
+ 	for_each_online_node(i) {
+ 		l3 = cachep->nodelists[i];
+ 		if (!l3)
+ 			continue;
+ 
+ 		drain_freelist(cachep, l3, l3->free_objects);
+ 
+ 		ret += !list_empty(&l3->slabs_full) ||
+ 			!list_empty(&l3->slabs_partial);
+ 	}
+ 	return (ret ? 1 : 0);
+ }
+ 
+ /**
+  * kmem_cache_shrink - Shrink a cache.
+  * @cachep: The cache to shrink.
+  *
+  * Releases as many slabs as possible for a cache.
+  * To help debugging, a zero exit status indicates all slabs were released.
+  */
+ int kmem_cache_shrink(struct kmem_cache *cachep)
+ {
+ 	int ret;
+ 	BUG_ON(!cachep || in_interrupt());
+ 
+ 	mutex_lock(&cache_chain_mutex);
+ 	ret = __cache_shrink(cachep);
+ 	mutex_unlock(&cache_chain_mutex);
+ 	return ret;
+ }
+ EXPORT_SYMBOL(kmem_cache_shrink);
+ 
+ /**
+  * kmem_cache_destroy - delete a cache
+  * @cachep: the cache to destroy
+  *
+  * Remove a &struct kmem_cache object from the slab cache.
+  *
+  * It is expected this function will be called by a module when it is
+  * unloaded.  This will remove the cache completely, and avoid a duplicate
+  * cache being allocated each time a module is loaded and unloaded, if the
+  * module doesn't have persistent in-kernel storage across loads and unloads.
+  *
+  * The cache must be empty before calling this function.
+  *
+  * The caller must guarantee that noone will allocate memory from the cache
+  * during the kmem_cache_destroy().
+  */
+ void kmem_cache_destroy(struct kmem_cache *cachep)
+ {
+ 	BUG_ON(!cachep || in_interrupt());
+ 
+ 	/* Find the cache in the chain of caches. */
+ 	mutex_lock(&cache_chain_mutex);
+ 	/*
+ 	 * the chain is never empty, cache_cache is never destroyed
+ 	 */
+ 	list_del(&cachep->next);
+ 	if (__cache_shrink(cachep)) {
+ 		slab_error(cachep, "Can't free all objects");
+ 		list_add(&cachep->next, &cache_chain);
+ 		mutex_unlock(&cache_chain_mutex);
+ 		return;
+ 	}
+ 
+ 	if (unlikely(cachep->flags & SLAB_DESTROY_BY_RCU))
+ 		synchronize_rcu();
+ 
+ 	__kmem_cache_destroy(cachep);
+ 	mutex_unlock(&cache_chain_mutex);
+ }
+ EXPORT_SYMBOL(kmem_cache_destroy);
+ 
+ /*
+  * Get the memory for a slab management obj.
+  * For a slab cache when the slab descriptor is off-slab, slab descriptors
+  * always come from malloc_sizes caches.  The slab descriptor cannot
+  * come from the same cache which is getting created because,
+  * when we are searching for an appropriate cache for these
+  * descriptors in kmem_cache_create, we search through the malloc_sizes array.
+  * If we are creating a malloc_sizes cache here it would not be visible to
+  * kmem_find_general_cachep till the initialization is complete.
+  * Hence we cannot have slabp_cache same as the original cache.
+  */
+ static struct slab *alloc_slabmgmt(struct kmem_cache *cachep, void *objp,
+ 				   int colour_off, gfp_t local_flags,
+ 				   int nodeid)
+ {
+ 	struct slab *slabp;
+ 
+ 	if (OFF_SLAB(cachep)) {
+ 		/* Slab management obj is off-slab. */
+ 		slabp = kmem_cache_alloc_node(cachep->slabp_cache,
+ 					      local_flags & ~GFP_THISNODE, nodeid);
+ 		if (!slabp)
+ 			return NULL;
+ 	} else {
+ 		slabp = objp + colour_off;
+ 		colour_off += cachep->slab_size;
+ 	}
+ 	slabp->inuse = 0;
+ 	slabp->colouroff = colour_off;
+ 	slabp->s_mem = objp + colour_off;
+ 	slabp->nodeid = nodeid;
+ 	return slabp;
+ }
+ 
+ static inline kmem_bufctl_t *slab_bufctl(struct slab *slabp)
+ {
+ 	return (kmem_bufctl_t *) (slabp + 1);
+ }
+ 
+ static void cache_init_objs(struct kmem_cache *cachep,
+ 			    struct slab *slabp)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < cachep->num; i++) {
+ 		void *objp = index_to_obj(cachep, slabp, i);
+ #if DEBUG
+ 		/* need to poison the objs? */
+ 		if (cachep->flags & SLAB_POISON)
+ 			poison_obj(cachep, objp, POISON_FREE);
+ 		if (cachep->flags & SLAB_STORE_USER)
+ 			*dbg_userword(cachep, objp) = NULL;
+ 
+ 		if (cachep->flags & SLAB_RED_ZONE) {
+ 			*dbg_redzone1(cachep, objp) = RED_INACTIVE;
+ 			*dbg_redzone2(cachep, objp) = RED_INACTIVE;
+ 		}
+ 		/*
+ 		 * Constructors are not allowed to allocate memory from the same
+ 		 * cache which they are a constructor for.  Otherwise, deadlock.
+ 		 * They must also be threaded.
+ 		 */
+ 		if (cachep->ctor && !(cachep->flags & SLAB_POISON))
+ 			cachep->ctor(objp + obj_offset(cachep), cachep,
+ 				     0);
+ 
+ 		if (cachep->flags & SLAB_RED_ZONE) {
+ 			if (*dbg_redzone2(cachep, objp) != RED_INACTIVE)
+ 				slab_error(cachep, "constructor overwrote the"
+ 					   " end of an object");
+ 			if (*dbg_redzone1(cachep, objp) != RED_INACTIVE)
+ 				slab_error(cachep, "constructor overwrote the"
+ 					   " start of an object");
+ 		}
+ 		if ((cachep->buffer_size % PAGE_SIZE) == 0 &&
+ 			    OFF_SLAB(cachep) && cachep->flags & SLAB_POISON)
+ 			kernel_map_pages(virt_to_page(objp),
+ 					 cachep->buffer_size / PAGE_SIZE, 0);
+ #else
+ 		if (cachep->ctor)
+ 			cachep->ctor(objp, cachep, 0);
+ #endif
+ 		slab_bufctl(slabp)[i] = i + 1;
+ 	}
+ 	slab_bufctl(slabp)[i - 1] = BUFCTL_END;
+ 	slabp->free = 0;
+ }
+ 
+ static void kmem_flagcheck(struct kmem_cache *cachep, gfp_t flags)
+ {
+ 	if (CONFIG_ZONE_DMA_FLAG) {
+ 		if (flags & GFP_DMA)
+ 			BUG_ON(!(cachep->gfpflags & GFP_DMA));
+ 		else
+ 			BUG_ON(cachep->gfpflags & GFP_DMA);
+ 	}
+ }
+ 
+ static void *slab_get_obj(struct kmem_cache *cachep, struct slab *slabp,
+ 				int nodeid)
+ {
+ 	void *objp = index_to_obj(cachep, slabp, slabp->free);
+ 	kmem_bufctl_t next;
+ 
+ 	slabp->inuse++;
+ 	next = slab_bufctl(slabp)[slabp->free];
+ #if DEBUG
+ 	slab_bufctl(slabp)[slabp->free] = BUFCTL_FREE;
+ 	WARN_ON(slabp->nodeid != nodeid);
+ #endif
+ 	slabp->free = next;
+ 
+ 	return objp;
+ }
+ 
+ static void slab_put_obj(struct kmem_cache *cachep, struct slab *slabp,
+ 				void *objp, int nodeid)
+ {
+ 	unsigned int objnr = obj_to_index(cachep, slabp, objp);
+ 
+ #if DEBUG
+ 	/* Verify that the slab belongs to the intended node */
+ 	WARN_ON(slabp->nodeid != nodeid);
+ 
+ 	if (slab_bufctl(slabp)[objnr] + 1 <= SLAB_LIMIT + 1) {
+ 		printk(KERN_ERR "slab: double free detected in cache "
+ 				"'%s', objp %p\n", cachep->name, objp);
+ 		BUG();
+ 	}
+ #endif
+ 	slab_bufctl(slabp)[objnr] = slabp->free;
+ 	slabp->free = objnr;
+ 	slabp->inuse--;
+ }
+ 
+ /*
+  * Map pages beginning at addr to the given cache and slab. This is required
+  * for the slab allocator to be able to lookup the cache and slab of a
+  * virtual address for kfree, ksize, kmem_ptr_validate, and slab debugging.
+  */
+ static void slab_map_pages(struct kmem_cache *cache, struct slab *slab,
+ 			   void *addr)
+ {
+ 	int nr_pages;
+ 	struct page *page;
+ 
+ 	page = virt_to_page(addr);
+ 
+ 	nr_pages = 1;
+ 	if (likely(!PageCompound(page)))
+ 		nr_pages <<= cache->gfporder;
+ 
+ 	do {
+ 		page_set_cache(page, cache);
+ 		page_set_slab(page, slab);
+ 		page++;
+ 	} while (--nr_pages);
+ }
+ 
+ /*
+  * Grow (by 1) the number of slabs within a cache.  This is called by
+  * kmem_cache_alloc() when there are no active objs left in a cache.
+  */
+ static int cache_grow(struct kmem_cache *cachep,
+ 		gfp_t flags, int nodeid, void *objp)
+ {
+ 	struct slab *slabp;
+ 	size_t offset;
+ 	gfp_t local_flags;
+ 	struct kmem_list3 *l3;
+ 
+ 	/*
+ 	 * Be lazy and only check for valid flags here,  keeping it out of the
+ 	 * critical path in kmem_cache_alloc().
+ 	 */
+ 	BUG_ON(flags & ~(GFP_DMA | __GFP_ZERO | GFP_LEVEL_MASK));
+ 
+ 	local_flags = (flags & GFP_LEVEL_MASK);
+ 	/* Take the l3 list lock to change the colour_next on this node */
+ 	check_irq_off();
+ 	l3 = cachep->nodelists[nodeid];
+ 	spin_lock(&l3->list_lock);
+ 
+ 	/* Get colour for the slab, and cal the next value. */
+ 	offset = l3->colour_next;
+ 	l3->colour_next++;
+ 	if (l3->colour_next >= cachep->colour)
+ 		l3->colour_next = 0;
+ 	spin_unlock(&l3->list_lock);
+ 
+ 	offset *= cachep->colour_off;
+ 
+ 	if (local_flags & __GFP_WAIT)
+ 		local_irq_enable();
+ 
+ 	/*
+ 	 * The test for missing atomic flag is performed here, rather than
+ 	 * the more obvious place, simply to reduce the critical path length
+ 	 * in kmem_cache_alloc(). If a caller is seriously mis-behaving they
+ 	 * will eventually be caught here (where it matters).
+ 	 */
+ 	kmem_flagcheck(cachep, flags);
+ 
+ 	/*
+ 	 * Get mem for the objs.  Attempt to allocate a physical page from
+ 	 * 'nodeid'.
+ 	 */
+ 	if (!objp)
+ 		objp = kmem_getpages(cachep, local_flags, nodeid);
+ 	if (!objp)
+ 		goto failed;
+ 
+ 	/* Get slab management. */
+ 	slabp = alloc_slabmgmt(cachep, objp, offset,
+ 			local_flags & ~GFP_THISNODE, nodeid);
+ 	if (!slabp)
+ 		goto opps1;
+ 
+ 	slabp->nodeid = nodeid;
+ 	slab_map_pages(cachep, slabp, objp);
+ 
+ 	cache_init_objs(cachep, slabp);
+ 
+ 	if (local_flags & __GFP_WAIT)
+ 		local_irq_disable();
+ 	check_irq_off();
+ 	spin_lock(&l3->list_lock);
+ 
+ 	/* Make slab active. */
+ 	list_add_tail(&slabp->list, &(l3->slabs_free));
+ 	STATS_INC_GROWN(cachep);
+ 	l3->free_objects += cachep->num;
+ 	spin_unlock(&l3->list_lock);
+ 	return 1;
+ opps1:
+ 	kmem_freepages(cachep, objp);
+ failed:
+ 	if (local_flags & __GFP_WAIT)
+ 		local_irq_disable();
+ 	return 0;
+ }
+ 
+ #if DEBUG
+ 
+ /*
+  * Perform extra freeing checks:
+  * - detect bad pointers.
+  * - POISON/RED_ZONE checking
+  */
+ static void kfree_debugcheck(const void *objp)
+ {
+ 	if (!virt_addr_valid(objp)) {
+ 		printk(KERN_ERR "kfree_debugcheck: out of range ptr %lxh.\n",
+ 		       (unsigned long)objp);
+ 		BUG();
+ 	}
+ }
+ 
+ static inline void verify_redzone_free(struct kmem_cache *cache, void *obj)
+ {
+ 	unsigned long long redzone1, redzone2;
+ 
+ 	redzone1 = *dbg_redzone1(cache, obj);
+ 	redzone2 = *dbg_redzone2(cache, obj);
+ 
+ 	/*
+ 	 * Redzone is ok.
+ 	 */
+ 	if (redzone1 == RED_ACTIVE && redzone2 == RED_ACTIVE)
+ 		return;
+ 
+ 	if (redzone1 == RED_INACTIVE && redzone2 == RED_INACTIVE)
+ 		slab_error(cache, "double free detected");
+ 	else
+ 		slab_error(cache, "memory outside object was overwritten");
+ 
+ 	printk(KERN_ERR "%p: redzone 1:0x%llx, redzone 2:0x%llx.\n",
+ 			obj, redzone1, redzone2);
+ }
+ 
+ static void *cache_free_debugcheck(struct kmem_cache *cachep, void *objp,
+ 				   void *caller)
+ {
+ 	struct page *page;
+ 	unsigned int objnr;
+ 	struct slab *slabp;
+ 
+ 	objp -= obj_offset(cachep);
+ 	kfree_debugcheck(objp);
+ 	page = virt_to_head_page(objp);
+ 
+ 	slabp = page_get_slab(page);
+ 
+ 	if (cachep->flags & SLAB_RED_ZONE) {
+ 		verify_redzone_free(cachep, objp);
+ 		*dbg_redzone1(cachep, objp) = RED_INACTIVE;
+ 		*dbg_redzone2(cachep, objp) = RED_INACTIVE;
+ 	}
+ 	if (cachep->flags & SLAB_STORE_USER)
+ 		*dbg_userword(cachep, objp) = caller;
+ 
+ 	objnr = obj_to_index(cachep, slabp, objp);
+ 
+ 	BUG_ON(objnr >= cachep->num);
+ 	BUG_ON(objp != index_to_obj(cachep, slabp, objnr));
+ 
+ #ifdef CONFIG_DEBUG_SLAB_LEAK
+ 	slab_bufctl(slabp)[objnr] = BUFCTL_FREE;
+ #endif
+ 	if (cachep->flags & SLAB_POISON) {
+ #ifdef CONFIG_DEBUG_PAGEALLOC
+ 		if ((cachep->buffer_size % PAGE_SIZE)==0 && OFF_SLAB(cachep)) {
+ 			store_stackinfo(cachep, objp, (unsigned long)caller);
+ 			kernel_map_pages(virt_to_page(objp),
+ 					 cachep->buffer_size / PAGE_SIZE, 0);
+ 		} else {
+ 			poison_obj(cachep, objp, POISON_FREE);
+ 		}
+ #else
+ 		poison_obj(cachep, objp, POISON_FREE);
+ #endif
+ 	}
+ 	return objp;
+ }
+ 
+ static void check_slabp(struct kmem_cache *cachep, struct slab *slabp)
+ {
+ 	kmem_bufctl_t i;
+ 	int entries = 0;
+ 
+ 	/* Check slab's freelist to see if this obj is there. */
+ 	for (i = slabp->free; i != BUFCTL_END; i = slab_bufctl(slabp)[i]) {
+ 		entries++;
+ 		if (entries > cachep->num || i >= cachep->num)
+ 			goto bad;
+ 	}
+ 	if (entries != cachep->num - slabp->inuse) {
+ bad:
+ 		printk(KERN_ERR "slab: Internal list corruption detected in "
+ 				"cache '%s'(%d), slabp %p(%d). Hexdump:\n",
+ 			cachep->name, cachep->num, slabp, slabp->inuse);
+ 		for (i = 0;
+ 		     i < sizeof(*slabp) + cachep->num * sizeof(kmem_bufctl_t);
+ 		     i++) {
+ 			if (i % 16 == 0)
+ 				printk("\n%03x:", i);
+ 			printk(" %02x", ((unsigned char *)slabp)[i]);
+ 		}
+ 		printk("\n");
+ 		BUG();
+ 	}
+ }
+ #else
+ #define kfree_debugcheck(x) do { } while(0)
+ #define cache_free_debugcheck(x,objp,z) (objp)
+ #define check_slabp(x,y) do { } while(0)
+ #endif
+ 
+ static void *cache_alloc_refill(struct kmem_cache *cachep, gfp_t flags)
+ {
+ 	int batchcount;
+ 	struct kmem_list3 *l3;
+ 	struct array_cache *ac;
+ 	int node;
+ 
+ 	node = numa_node_id();
+ 
+ 	check_irq_off();
+ 	ac = cpu_cache_get(cachep);
+ retry:
+ 	batchcount = ac->batchcount;
+ 	if (!ac->touched && batchcount > BATCHREFILL_LIMIT) {
+ 		/*
+ 		 * If there was little recent activity on this cache, then
+ 		 * perform only a partial refill.  Otherwise we could generate
+ 		 * refill bouncing.
+ 		 */
+ 		batchcount = BATCHREFILL_LIMIT;
+ 	}
+ 	l3 = cachep->nodelists[node];
+ 
+ 	BUG_ON(ac->avail > 0 || !l3);
+ 	spin_lock(&l3->list_lock);
+ 
+ 	/* See if we can refill from the shared array */
+ 	if (l3->shared && transfer_objects(ac, l3->shared, batchcount))
+ 		goto alloc_done;
+ 
+ 	while (batchcount > 0) {
+ 		struct list_head *entry;
+ 		struct slab *slabp;
+ 		/* Get slab alloc is to come from. */
+ 		entry = l3->slabs_partial.next;
+ 		if (entry == &l3->slabs_partial) {
+ 			l3->free_touched = 1;
+ 			entry = l3->slabs_free.next;
+ 			if (entry == &l3->slabs_free)
+ 				goto must_grow;
+ 		}
+ 
+ 		slabp = list_entry(entry, struct slab, list);
+ 		check_slabp(cachep, slabp);
+ 		check_spinlock_acquired(cachep);
+ 
+ 		/*
+ 		 * The slab was either on partial or free list so
+ 		 * there must be at least one object available for
+ 		 * allocation.
+ 		 */
+ 		BUG_ON(slabp->inuse < 0 || slabp->inuse >= cachep->num);
+ 
+ 		while (slabp->inuse < cachep->num && batchcount--) {
+ 			STATS_INC_ALLOCED(cachep);
+ 			STATS_INC_ACTIVE(cachep);
+ 			STATS_SET_HIGH(cachep);
+ 
+ 			ac->entry[ac->avail++] = slab_get_obj(cachep, slabp,
+ 							    node);
+ 		}
+ 		check_slabp(cachep, slabp);
+ 
+ 		/* move slabp to correct slabp list: */
+ 		list_del(&slabp->list);
+ 		if (slabp->free == BUFCTL_END)
+ 			list_add(&slabp->list, &l3->slabs_full);
+ 		else
+ 			list_add(&slabp->list, &l3->slabs_partial);
+ 	}
+ 
+ must_grow:
+ 	l3->free_objects -= ac->avail;
+ alloc_done:
+ 	spin_unlock(&l3->list_lock);
+ 
+ 	if (unlikely(!ac->avail)) {
+ 		int x;
+ 		x = cache_grow(cachep, flags | GFP_THISNODE, node, NULL);
+ 
+ 		/* cache_grow can reenable interrupts, then ac could change. */
+ 		ac = cpu_cache_get(cachep);
+ 		if (!x && ac->avail == 0)	/* no objects in sight? abort */
+ 			return NULL;
+ 
+ 		if (!ac->avail)		/* objects refilled by interrupt? */
+ 			goto retry;
+ 	}
+ 	ac->touched = 1;
+ 	return ac->entry[--ac->avail];
+ }
+ 
+ static inline void cache_alloc_debugcheck_before(struct kmem_cache *cachep,
+ 						gfp_t flags)
+ {
+ 	might_sleep_if(flags & __GFP_WAIT);
+ #if DEBUG
+ 	kmem_flagcheck(cachep, flags);
+ #endif
+ }
+ 
+ #if DEBUG
+ static void *cache_alloc_debugcheck_after(struct kmem_cache *cachep,
+ 				gfp_t flags, void *objp, void *caller)
+ {
+ 	if (!objp)
+ 		return objp;
+ 	if (cachep->flags & SLAB_POISON) {
+ #ifdef CONFIG_DEBUG_PAGEALLOC
+ 		if ((cachep->buffer_size % PAGE_SIZE) == 0 && OFF_SLAB(cachep))
+ 			kernel_map_pages(virt_to_page(objp),
+ 					 cachep->buffer_size / PAGE_SIZE, 1);
+ 		else
+ 			check_poison_obj(cachep, objp);
+ #else
+ 		check_poison_obj(cachep, objp);
+ #endif
+ 		poison_obj(cachep, objp, POISON_INUSE);
+ 	}
+ 	if (cachep->flags & SLAB_STORE_USER)
+ 		*dbg_userword(cachep, objp) = caller;
+ 
+ 	if (cachep->flags & SLAB_RED_ZONE) {
+ 		if (*dbg_redzone1(cachep, objp) != RED_INACTIVE ||
+ 				*dbg_redzone2(cachep, objp) != RED_INACTIVE) {
+ 			slab_error(cachep, "double free, or memory outside"
+ 						" object was overwritten");
+ 			printk(KERN_ERR
+ 				"%p: redzone 1:0x%llx, redzone 2:0x%llx\n",
+ 				objp, *dbg_redzone1(cachep, objp),
+ 				*dbg_redzone2(cachep, objp));
+ 		}
+ 		*dbg_redzone1(cachep, objp) = RED_ACTIVE;
+ 		*dbg_redzone2(cachep, objp) = RED_ACTIVE;
+ 	}
+ #ifdef CONFIG_DEBUG_SLAB_LEAK
+ 	{
+ 		struct slab *slabp;
+ 		unsigned objnr;
+ 
+ 		slabp = page_get_slab(virt_to_head_page(objp));
+ 		objnr = (unsigned)(objp - slabp->s_mem) / cachep->buffer_size;
+ 		slab_bufctl(slabp)[objnr] = BUFCTL_ACTIVE;
+ 	}
+ #endif
+ 	objp += obj_offset(cachep);
+ 	if (cachep->ctor && cachep->flags & SLAB_POISON)
+ 		cachep->ctor(objp, cachep, 0);
+ #if ARCH_SLAB_MINALIGN
+ 	if ((u32)objp & (ARCH_SLAB_MINALIGN-1)) {
+ 		printk(KERN_ERR "0x%p: not aligned to ARCH_SLAB_MINALIGN=%d\n",
+ 		       objp, ARCH_SLAB_MINALIGN);
+ 	}
+ #endif
+ 	return objp;
+ }
+ #else
+ #define cache_alloc_debugcheck_after(a,b,objp,d) (objp)
+ #endif
+ 
+ #ifdef CONFIG_FAILSLAB
+ 
+ static struct failslab_attr {
+ 
+ 	struct fault_attr attr;
+ 
+ 	u32 ignore_gfp_wait;
+ #ifdef CONFIG_FAULT_INJECTION_DEBUG_FS
+ 	struct dentry *ignore_gfp_wait_file;
+ #endif
+ 
+ } failslab = {
+ 	.attr = FAULT_ATTR_INITIALIZER,
+ 	.ignore_gfp_wait = 1,
+ };
+ 
+ static int __init setup_failslab(char *str)
+ {
+ 	return setup_fault_attr(&failslab.attr, str);
+ }
+ __setup("failslab=", setup_failslab);
+ 
+ static int should_failslab(struct kmem_cache *cachep, gfp_t flags)
+ {
+ 	if (cachep == &cache_cache)
+ 		return 0;
+ 	if (flags & __GFP_NOFAIL)
+ 		return 0;
+ 	if (failslab.ignore_gfp_wait && (flags & __GFP_WAIT))
+ 		return 0;
+ 
+ 	return should_fail(&failslab.attr, obj_size(cachep));
+ }
+ 
+ #ifdef CONFIG_FAULT_INJECTION_DEBUG_FS
+ 
+ static int __init failslab_debugfs(void)
+ {
+ 	mode_t mode = S_IFREG | S_IRUSR | S_IWUSR;
+ 	struct dentry *dir;
+ 	int err;
+ 
+ 	err = init_fault_attr_dentries(&failslab.attr, "failslab");
+ 	if (err)
+ 		return err;
+ 	dir = failslab.attr.dentries.dir;
+ 
+ 	failslab.ignore_gfp_wait_file =
+ 		debugfs_create_bool("ignore-gfp-wait", mode, dir,
+ 				      &failslab.ignore_gfp_wait);
+ 
+ 	if (!failslab.ignore_gfp_wait_file) {
+ 		err = -ENOMEM;
+ 		debugfs_remove(failslab.ignore_gfp_wait_file);
+ 		cleanup_fault_attr_dentries(&failslab.attr);
+ 	}
+ 
+ 	return err;
+ }
+ 
+ late_initcall(failslab_debugfs);
+ 
+ #endif /* CONFIG_FAULT_INJECTION_DEBUG_FS */
+ 
+ #else /* CONFIG_FAILSLAB */
+ 
+ static inline int should_failslab(struct kmem_cache *cachep, gfp_t flags)
+ {
+ 	return 0;
+ }
+ 
+ #endif /* CONFIG_FAILSLAB */
+ 
+ static inline void *____cache_alloc(struct kmem_cache *cachep, gfp_t flags)
+ {
+ 	void *objp;
+ 	struct array_cache *ac;
+ 
+ 	check_irq_off();
+ 
+ 	ac = cpu_cache_get(cachep);
+ 	if (likely(ac->avail)) {
+ 		STATS_INC_ALLOCHIT(cachep);
+ 		ac->touched = 1;
+ 		objp = ac->entry[--ac->avail];
+ 	} else {
+ 		STATS_INC_ALLOCMISS(cachep);
+ 		objp = cache_alloc_refill(cachep, flags);
+ 	}
+ 	return objp;
+ }
+ 
+ #ifdef CONFIG_NUMA
+ /*
+  * Try allocating on another node if PF_SPREAD_SLAB|PF_MEMPOLICY.
+  *
+  * If we are in_interrupt, then process context, including cpusets and
+  * mempolicy, may not apply and should not be used for allocation policy.
+  */
+ static void *alternate_node_alloc(struct kmem_cache *cachep, gfp_t flags)
+ {
+ 	int nid_alloc, nid_here;
+ 
+ 	if (in_interrupt() || (flags & __GFP_THISNODE))
+ 		return NULL;
+ 	nid_alloc = nid_here = numa_node_id();
+ 	if (cpuset_do_slab_mem_spread() && (cachep->flags & SLAB_MEM_SPREAD))
+ 		nid_alloc = cpuset_mem_spread_node();
+ 	else if (current->mempolicy)
+ 		nid_alloc = slab_node(current->mempolicy);
+ 	if (nid_alloc != nid_here)
+ 		return ____cache_alloc_node(cachep, flags, nid_alloc);
+ 	return NULL;
+ }
+ 
+ /*
+  * Fallback function if there was no memory available and no objects on a
+  * certain node and fall back is permitted. First we scan all the
+  * available nodelists for available objects. If that fails then we
+  * perform an allocation without specifying a node. This allows the page
+  * allocator to do its reclaim / fallback magic. We then insert the
+  * slab into the proper nodelist and then allocate from it.
+  */
+ static void *fallback_alloc(struct kmem_cache *cache, gfp_t flags)
+ {
+ 	struct zonelist *zonelist;
+ 	gfp_t local_flags;
+ 	struct zone **z;
+ 	void *obj = NULL;
+ 	int nid;
+ 
+ 	if (flags & __GFP_THISNODE)
+ 		return NULL;
+ 
+ 	zonelist = &NODE_DATA(slab_node(current->mempolicy))
+ 			->node_zonelists[gfp_zone(flags)];
+ 	local_flags = (flags & GFP_LEVEL_MASK);
+ 
+ retry:
+ 	/*
+ 	 * Look through allowed nodes for objects available
+ 	 * from existing per node queues.
+ 	 */
+ 	for (z = zonelist->zones; *z && !obj; z++) {
+ 		nid = zone_to_nid(*z);
+ 
+ 		if (cpuset_zone_allowed_hardwall(*z, flags) &&
+ 			cache->nodelists[nid] &&
+ 			cache->nodelists[nid]->free_objects)
+ 				obj = ____cache_alloc_node(cache,
+ 					flags | GFP_THISNODE, nid);
+ 	}
+ 
+ 	if (!obj) {
+ 		/*
+ 		 * This allocation will be performed within the constraints
+ 		 * of the current cpuset / memory policy requirements.
+ 		 * We may trigger various forms of reclaim on the allowed
+ 		 * set and go into memory reserves if necessary.
+ 		 */
+ 		if (local_flags & __GFP_WAIT)
+ 			local_irq_enable();
+ 		kmem_flagcheck(cache, flags);
+ 		obj = kmem_getpages(cache, flags, -1);
+ 		if (local_flags & __GFP_WAIT)
+ 			local_irq_disable();
+ 		if (obj) {
+ 			/*
+ 			 * Insert into the appropriate per node queues
+ 			 */
+ 			nid = page_to_nid(virt_to_page(obj));
+ 			if (cache_grow(cache, flags, nid, obj)) {
+ 				obj = ____cache_alloc_node(cache,
+ 					flags | GFP_THISNODE, nid);
+ 				if (!obj)
+ 					/*
+ 					 * Another processor may allocate the
+ 					 * objects in the slab since we are
+ 					 * not holding any locks.
+ 					 */
+ 					goto retry;
+ 			} else {
+ 				/* cache_grow already freed obj */
+ 				obj = NULL;
+ 			}
+ 		}
+ 	}
+ 	return obj;
+ }
+ 
+ /*
+  * A interface to enable slab creation on nodeid
+  */
+ static void *____cache_alloc_node(struct kmem_cache *cachep, gfp_t flags,
+ 				int nodeid)
+ {
+ 	struct list_head *entry;
+ 	struct slab *slabp;
+ 	struct kmem_list3 *l3;
+ 	void *obj;
+ 	int x;
+ 
+ 	l3 = cachep->nodelists[nodeid];
+ 	BUG_ON(!l3);
+ 
+ retry:
+ 	check_irq_off();
+ 	spin_lock(&l3->list_lock);
+ 	entry = l3->slabs_partial.next;
+ 	if (entry == &l3->slabs_partial) {
+ 		l3->free_touched = 1;
+ 		entry = l3->slabs_free.next;
+ 		if (entry == &l3->slabs_free)
+ 			goto must_grow;
+ 	}
+ 
+ 	slabp = list_entry(entry, struct slab, list);
+ 	check_spinlock_acquired_node(cachep, nodeid);
+ 	check_slabp(cachep, slabp);
+ 
+ 	STATS_INC_NODEALLOCS(cachep);
+ 	STATS_INC_ACTIVE(cachep);
+ 	STATS_SET_HIGH(cachep);
+ 
+ 	BUG_ON(slabp->inuse == cachep->num);
+ 
+ 	obj = slab_get_obj(cachep, slabp, nodeid);
+ 	check_slabp(cachep, slabp);
+ 	l3->free_objects--;
+ 	/* move slabp to correct slabp list: */
+ 	list_del(&slabp->list);
+ 
+ 	if (slabp->free == BUFCTL_END)
+ 		list_add(&slabp->list, &l3->slabs_full);
+ 	else
+ 		list_add(&slabp->list, &l3->slabs_partial);
+ 
+ 	spin_unlock(&l3->list_lock);
+ 	goto done;
+ 
+ must_grow:
+ 	spin_unlock(&l3->list_lock);
+ 	x = cache_grow(cachep, flags | GFP_THISNODE, nodeid, NULL);
+ 	if (x)
+ 		goto retry;
+ 
+ 	return fallback_alloc(cachep, flags);
+ 
+ done:
+ 	return obj;
+ }
+ 
+ /**
+  * kmem_cache_alloc_node - Allocate an object on the specified node
+  * @cachep: The cache to allocate from.
+  * @flags: See kmalloc().
+  * @nodeid: node number of the target node.
+  * @caller: return address of caller, used for debug information
+  *
+  * Identical to kmem_cache_alloc but it will allocate memory on the given
+  * node, which can improve the performance for cpu bound structures.
+  *
+  * Fallback to other node is possible if __GFP_THISNODE is not set.
+  */
+ static __always_inline void *
+ __cache_alloc_node(struct kmem_cache *cachep, gfp_t flags, int nodeid,
+ 		   void *caller)
+ {
+ 	unsigned long save_flags;
+ 	void *ptr;
+ 
+ 	if (should_failslab(cachep, flags))
+ 		return NULL;
+ 
+ 	cache_alloc_debugcheck_before(cachep, flags);
+ 	local_irq_save(save_flags);
+ 
+ 	if (unlikely(nodeid == -1))
+ 		nodeid = numa_node_id();
+ 
+ 	if (unlikely(!cachep->nodelists[nodeid])) {
+ 		/* Node not bootstrapped yet */
+ 		ptr = fallback_alloc(cachep, flags);
+ 		goto out;
+ 	}
+ 
+ 	if (nodeid == numa_node_id()) {
+ 		/*
+ 		 * Use the locally cached objects if possible.
+ 		 * However ____cache_alloc does not allow fallback
+ 		 * to other nodes. It may fail while we still have
+ 		 * objects on other nodes available.
+ 		 */
+ 		ptr = ____cache_alloc(cachep, flags);
+ 		if (ptr)
+ 			goto out;
+ 	}
+ 	/* ___cache_alloc_node can fall back to other nodes */
+ 	ptr = ____cache_alloc_node(cachep, flags, nodeid);
+   out:
+ 	local_irq_restore(save_flags);
+ 	ptr = cache_alloc_debugcheck_after(cachep, flags, ptr, caller);
+ 
+ 	if (unlikely((flags & __GFP_ZERO) && ptr))
+ 		memset(ptr, 0, obj_size(cachep));
+ 
+ 	return ptr;
+ }
+ 
+ static __always_inline void *
+ __do_cache_alloc(struct kmem_cache *cache, gfp_t flags)
+ {
+ 	void *objp;
+ 
+ 	if (unlikely(current->flags & (PF_SPREAD_SLAB | PF_MEMPOLICY))) {
+ 		objp = alternate_node_alloc(cache, flags);
+ 		if (objp)
+ 			goto out;
+ 	}
+ 	objp = ____cache_alloc(cache, flags);
+ 
+ 	/*
+ 	 * We may just have run out of memory on the local node.
+ 	 * ____cache_alloc_node() knows how to locate memory on other nodes
+ 	 */
+  	if (!objp)
+  		objp = ____cache_alloc_node(cache, flags, numa_node_id());
+ 
+   out:
+ 	return objp;
+ }
+ #else
+ 
+ static __always_inline void *
+ __do_cache_alloc(struct kmem_cache *cachep, gfp_t flags)
+ {
+ 	return ____cache_alloc(cachep, flags);
+ }
+ 
+ #endif /* CONFIG_NUMA */
+ 
+ static __always_inline void *
+ __cache_alloc(struct kmem_cache *cachep, gfp_t flags, void *caller)
+ {
+ 	unsigned long save_flags;
+ 	void *objp;
+ 
+ 	if (should_failslab(cachep, flags))
+ 		return NULL;
+ 
+ 	cache_alloc_debugcheck_before(cachep, flags);
+ 	local_irq_save(save_flags);
+ 	objp = __do_cache_alloc(cachep, flags);
+ 	local_irq_restore(save_flags);
+ 	objp = cache_alloc_debugcheck_after(cachep, flags, objp, caller);
+ 	prefetchw(objp);
+ 
+ 	if (unlikely((flags & __GFP_ZERO) && objp))
+ 		memset(objp, 0, obj_size(cachep));
+ 
+ 	return objp;
+ }
+ 
+ /*
+  * Caller needs to acquire correct kmem_list's list_lock
+  */
+ static void free_block(struct kmem_cache *cachep, void **objpp, int nr_objects,
+ 		       int node)
+ {
+ 	int i;
+ 	struct kmem_list3 *l3;
+ 
+ 	for (i = 0; i < nr_objects; i++) {
+ 		void *objp = objpp[i];
+ 		struct slab *slabp;
+ 
+ 		slabp = virt_to_slab(objp);
+ 		l3 = cachep->nodelists[node];
+ 		list_del(&slabp->list);
+ 		check_spinlock_acquired_node(cachep, node);
+ 		check_slabp(cachep, slabp);
+ 		slab_put_obj(cachep, slabp, objp, node);
+ 		STATS_DEC_ACTIVE(cachep);
+ 		l3->free_objects++;
+ 		check_slabp(cachep, slabp);
+ 
+ 		/* fixup slab chains */
+ 		if (slabp->inuse == 0) {
+ 			if (l3->free_objects > l3->free_limit) {
+ 				l3->free_objects -= cachep->num;
+ 				/* No need to drop any previously held
+ 				 * lock here, even if we have a off-slab slab
+ 				 * descriptor it is guaranteed to come from
+ 				 * a different cache, refer to comments before
+ 				 * alloc_slabmgmt.
+ 				 */
+ 				slab_destroy(cachep, slabp);
+ 			} else {
+ 				list_add(&slabp->list, &l3->slabs_free);
+ 			}
+ 		} else {
+ 			/* Unconditionally move a slab to the end of the
+ 			 * partial list on free - maximum time for the
+ 			 * other objects to be freed, too.
+ 			 */
+ 			list_add_tail(&slabp->list, &l3->slabs_partial);
+ 		}
+ 	}
+ }
+ 
+ static void cache_flusharray(struct kmem_cache *cachep, struct array_cache *ac)
+ {
+ 	int batchcount;
+ 	struct kmem_list3 *l3;
+ 	int node = numa_node_id();
+ 
+ 	batchcount = ac->batchcount;
+ #if DEBUG
+ 	BUG_ON(!batchcount || batchcount > ac->avail);
+ #endif
+ 	check_irq_off();
+ 	l3 = cachep->nodelists[node];
+ 	spin_lock(&l3->list_lock);
+ 	if (l3->shared) {
+ 		struct array_cache *shared_array = l3->shared;
+ 		int max = shared_array->limit - shared_array->avail;
+ 		if (max) {
+ 			if (batchcount > max)
+ 				batchcount = max;
+ 			memcpy(&(shared_array->entry[shared_array->avail]),
+ 			       ac->entry, sizeof(void *) * batchcount);
+ 			shared_array->avail += batchcount;
+ 			goto free_done;
+ 		}
+ 	}
+ 
+ 	free_block(cachep, ac->entry, batchcount, node);
+ free_done:
+ #if STATS
+ 	{
+ 		int i = 0;
+ 		struct list_head *p;
+ 
+ 		p = l3->slabs_free.next;
+ 		while (p != &(l3->slabs_free)) {
+ 			struct slab *slabp;
+ 
+ 			slabp = list_entry(p, struct slab, list);
+ 			BUG_ON(slabp->inuse);
+ 
+ 			i++;
+ 			p = p->next;
+ 		}
+ 		STATS_SET_FREEABLE(cachep, i);
+ 	}
+ #endif
+ 	spin_unlock(&l3->list_lock);
+ 	ac->avail -= batchcount;
+ 	memmove(ac->entry, &(ac->entry[batchcount]), sizeof(void *)*ac->avail);
+ }
+ 
+ /*
+  * Release an obj back to its cache. If the obj has a constructed state, it must
+  * be in this state _before_ it is released.  Called with disabled ints.
+  */
+ static inline void __cache_free(struct kmem_cache *cachep, void *objp)
+ {
+ 	struct array_cache *ac = cpu_cache_get(cachep);
+ 
+ 	check_irq_off();
+ 	objp = cache_free_debugcheck(cachep, objp, __builtin_return_address(0));
+ 
+ 	/*
+ 	 * Skip calling cache_free_alien() when the platform is not numa.
+ 	 * This will avoid cache misses that happen while accessing slabp (which
+ 	 * is per page memory  reference) to get nodeid. Instead use a global
+ 	 * variable to skip the call, which is mostly likely to be present in
+ 	 * the cache.
+ 	 */
+ 	if (numa_platform && cache_free_alien(cachep, objp))
+ 		return;
+ 
+ 	if (likely(ac->avail < ac->limit)) {
+ 		STATS_INC_FREEHIT(cachep);
+ 		ac->entry[ac->avail++] = objp;
+ 		return;
+ 	} else {
+ 		STATS_INC_FREEMISS(cachep);
+ 		cache_flusharray(cachep, ac);
+ 		ac->entry[ac->avail++] = objp;
+ 	}
+ }
+ 
+ /**
+  * kmem_cache_alloc - Allocate an object
+  * @cachep: The cache to allocate from.
+  * @flags: See kmalloc().
+  *
+  * Allocate an object from this cache.  The flags are only relevant
+  * if the cache has no available objects.
+  */
+ void *kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags)
+ {
+ 	return __cache_alloc(cachep, flags, __builtin_return_address(0));
+ }
+ EXPORT_SYMBOL(kmem_cache_alloc);
+ 
+ /**
+  * kmem_ptr_validate - check if an untrusted pointer might
+  *	be a slab entry.
+  * @cachep: the cache we're checking against
+  * @ptr: pointer to validate
+  *
+  * This verifies that the untrusted pointer looks sane:
+  * it is _not_ a guarantee that the pointer is actually
+  * part of the slab cache in question, but it at least
+  * validates that the pointer can be dereferenced and
+  * looks half-way sane.
+  *
+  * Currently only used for dentry validation.
+  */
+ int kmem_ptr_validate(struct kmem_cache *cachep, const void *ptr)
+ {
+ 	unsigned long addr = (unsigned long)ptr;
+ 	unsigned long min_addr = PAGE_OFFSET;
+ 	unsigned long align_mask = BYTES_PER_WORD - 1;
+ 	unsigned long size = cachep->buffer_size;
+ 	struct page *page;
+ 
+ 	if (unlikely(addr < min_addr))
+ 		goto out;
+ 	if (unlikely(addr > (unsigned long)high_memory - size))
+ 		goto out;
+ 	if (unlikely(addr & align_mask))
+ 		goto out;
+ 	if (unlikely(!kern_addr_valid(addr)))
+ 		goto out;
+ 	if (unlikely(!kern_addr_valid(addr + size - 1)))
+ 		goto out;
+ 	page = virt_to_page(ptr);
+ 	if (unlikely(!PageSlab(page)))
+ 		goto out;
+ 	if (unlikely(page_get_cache(page) != cachep))
+ 		goto out;
+ 	return 1;
+ out:
+ 	return 0;
+ }
+ 
+ #ifdef CONFIG_NUMA
+ void *kmem_cache_alloc_node(struct kmem_cache *cachep, gfp_t flags, int nodeid)
+ {
+ 	return __cache_alloc_node(cachep, flags, nodeid,
+ 			__builtin_return_address(0));
+ }
+ EXPORT_SYMBOL(kmem_cache_alloc_node);
+ 
+ static __always_inline void *
+ __do_kmalloc_node(size_t size, gfp_t flags, int node, void *caller)
+ {
+ 	struct kmem_cache *cachep;
+ 
+ 	cachep = kmem_find_general_cachep(size, flags);
+ 	if (unlikely(ZERO_OR_NULL_PTR(cachep)))
+ 		return cachep;
+ 	return kmem_cache_alloc_node(cachep, flags, node);
+ }
+ 
+ #ifdef CONFIG_DEBUG_SLAB
+ void *__kmalloc_node(size_t size, gfp_t flags, int node)
+ {
+ 	return __do_kmalloc_node(size, flags, node,
+ 			__builtin_return_address(0));
+ }
+ EXPORT_SYMBOL(__kmalloc_node);
+ 
+ void *__kmalloc_node_track_caller(size_t size, gfp_t flags,
+ 		int node, void *caller)
+ {
+ 	return __do_kmalloc_node(size, flags, node, caller);
+ }
+ EXPORT_SYMBOL(__kmalloc_node_track_caller);
+ #else
+ void *__kmalloc_node(size_t size, gfp_t flags, int node)
+ {
+ 	return __do_kmalloc_node(size, flags, node, NULL);
+ }
+ EXPORT_SYMBOL(__kmalloc_node);
+ #endif /* CONFIG_DEBUG_SLAB */
+ #endif /* CONFIG_NUMA */
+ 
+ /**
+  * __do_kmalloc - allocate memory
+  * @size: how many bytes of memory are required.
+  * @flags: the type of memory to allocate (see kmalloc).
+  * @caller: function caller for debug tracking of the caller
+  */
+ static __always_inline void *__do_kmalloc(size_t size, gfp_t flags,
+ 					  void *caller)
+ {
+ 	struct kmem_cache *cachep;
+ 
+ 	/* If you want to save a few bytes .text space: replace
+ 	 * __ with kmem_.
+ 	 * Then kmalloc uses the uninlined functions instead of the inline
+ 	 * functions.
+ 	 */
+ 	cachep = __find_general_cachep(size, flags);
+ 	if (unlikely(ZERO_OR_NULL_PTR(cachep)))
+ 		return cachep;
+ 	return __cache_alloc(cachep, flags, caller);
+ }
+ 
+ 
+ #ifdef CONFIG_DEBUG_SLAB
+ void *__kmalloc(size_t size, gfp_t flags)
+ {
+ 	return __do_kmalloc(size, flags, __builtin_return_address(0));
+ }
+ EXPORT_SYMBOL(__kmalloc);
+ 
+ void *__kmalloc_track_caller(size_t size, gfp_t flags, void *caller)
+ {
+ 	return __do_kmalloc(size, flags, caller);
+ }
+ EXPORT_SYMBOL(__kmalloc_track_caller);
+ 
+ #else
+ void *__kmalloc(size_t size, gfp_t flags)
+ {
+ 	return __do_kmalloc(size, flags, NULL);
+ }
+ EXPORT_SYMBOL(__kmalloc);
+ #endif
+ 
+ /**
+  * kmem_cache_free - Deallocate an object
+  * @cachep: The cache the allocation was from.
+  * @objp: The previously allocated object.
+  *
+  * Free an object which was previously allocated from this
+  * cache.
+  */
+ void kmem_cache_free(struct kmem_cache *cachep, void *objp)
+ {
+ 	unsigned long flags;
+ 
+ 	BUG_ON(virt_to_cache(objp) != cachep);
+ 
+ 	local_irq_save(flags);
+ 	debug_check_no_locks_freed(objp, obj_size(cachep));
+ 	__cache_free(cachep, objp);
+ 	local_irq_restore(flags);
+ }
+ EXPORT_SYMBOL(kmem_cache_free);
+ 
+ /**
+  * kfree - free previously allocated memory
+  * @objp: pointer returned by kmalloc.
+  *
+  * If @objp is NULL, no operation is performed.
+  *
+  * Don't free memory not originally allocated by kmalloc()
+  * or you will run into trouble.
+  */
+ void kfree(const void *objp)
+ {
+ 	struct kmem_cache *c;
+ 	unsigned long flags;
+ 
+ 	if (unlikely(ZERO_OR_NULL_PTR(objp)))
+ 		return;
+ 	local_irq_save(flags);
+ 	kfree_debugcheck(objp);
+ 	c = virt_to_cache(objp);
+ 	debug_check_no_locks_freed(objp, obj_size(c));
+ 	__cache_free(c, (void *)objp);
+ 	local_irq_restore(flags);
+ }
+ EXPORT_SYMBOL(kfree);
+ 
+ unsigned int kmem_cache_size(struct kmem_cache *cachep)
+ {
+ 	return obj_size(cachep);
+ }
+ EXPORT_SYMBOL(kmem_cache_size);
+ 
+ const char *kmem_cache_name(struct kmem_cache *cachep)
+ {
+ 	return cachep->name;
+ }
+ EXPORT_SYMBOL_GPL(kmem_cache_name);
+ 
+ /*
+  * This initializes kmem_list3 or resizes varioius caches for all nodes.
+  */
+ static int alloc_kmemlist(struct kmem_cache *cachep)
+ {
+ 	int node;
+ 	struct kmem_list3 *l3;
+ 	struct array_cache *new_shared;
+ 	struct array_cache **new_alien = NULL;
+ 
+ 	for_each_online_node(node) {
+ 
+                 if (use_alien_caches) {
+                         new_alien = alloc_alien_cache(node, cachep->limit);
+                         if (!new_alien)
+                                 goto fail;
+                 }
+ 
+ 		new_shared = NULL;
+ 		if (cachep->shared) {
+ 			new_shared = alloc_arraycache(node,
+ 				cachep->shared*cachep->batchcount,
+ 					0xbaadf00d);
+ 			if (!new_shared) {
+ 				free_alien_cache(new_alien);
+ 				goto fail;
+ 			}
+ 		}
+ 
+ 		l3 = cachep->nodelists[node];
+ 		if (l3) {
+ 			struct array_cache *shared = l3->shared;
+ 
+ 			spin_lock_irq(&l3->list_lock);
+ 
+ 			if (shared)
+ 				free_block(cachep, shared->entry,
+ 						shared->avail, node);
+ 
+ 			l3->shared = new_shared;
+ 			if (!l3->alien) {
+ 				l3->alien = new_alien;
+ 				new_alien = NULL;
+ 			}
+ 			l3->free_limit = (1 + nr_cpus_node(node)) *
+ 					cachep->batchcount + cachep->num;
+ 			spin_unlock_irq(&l3->list_lock);
+ 			kfree(shared);
+ 			free_alien_cache(new_alien);
+ 			continue;
+ 		}
+ 		l3 = kmalloc_node(sizeof(struct kmem_list3), GFP_KERNEL, node);
+ 		if (!l3) {
+ 			free_alien_cache(new_alien);
+ 			kfree(new_shared);
+ 			goto fail;
+ 		}
+ 
+ 		kmem_list3_init(l3);
+ 		l3->next_reap = jiffies + REAPTIMEOUT_LIST3 +
+ 				((unsigned long)cachep) % REAPTIMEOUT_LIST3;
+ 		l3->shared = new_shared;
+ 		l3->alien = new_alien;
+ 		l3->free_limit = (1 + nr_cpus_node(node)) *
+ 					cachep->batchcount + cachep->num;
+ 		cachep->nodelists[node] = l3;
+ 	}
+ 	return 0;
+ 
+ fail:
+ 	if (!cachep->next.next) {
+ 		/* Cache is not active yet. Roll back what we did */
+ 		node--;
+ 		while (node >= 0) {
+ 			if (cachep->nodelists[node]) {
+ 				l3 = cachep->nodelists[node];
+ 
+ 				kfree(l3->shared);
+ 				free_alien_cache(l3->alien);
+ 				kfree(l3);
+ 				cachep->nodelists[node] = NULL;
+ 			}
+ 			node--;
+ 		}
+ 	}
+ 	return -ENOMEM;
+ }
+ 
+ struct ccupdate_struct {
+ 	struct kmem_cache *cachep;
+ 	struct array_cache *new[NR_CPUS];
+ };
+ 
+ static void do_ccupdate_local(void *info)
+ {
+ 	struct ccupdate_struct *new = info;
+ 	struct array_cache *old;
+ 
+ 	check_irq_off();
+ 	old = cpu_cache_get(new->cachep);
+ 
+ 	new->cachep->array[smp_processor_id()] = new->new[smp_processor_id()];
+ 	new->new[smp_processor_id()] = old;
+ }
+ 
+ /* Always called with the cache_chain_mutex held */
+ static int do_tune_cpucache(struct kmem_cache *cachep, int limit,
+ 				int batchcount, int shared)
+ {
+ 	struct ccupdate_struct *new;
+ 	int i;
+ 
+ 	new = kzalloc(sizeof(*new), GFP_KERNEL);
+ 	if (!new)
+ 		return -ENOMEM;
+ 
+ 	for_each_online_cpu(i) {
+ 		new->new[i] = alloc_arraycache(cpu_to_node(i), limit,
+ 						batchcount);
+ 		if (!new->new[i]) {
+ 			for (i--; i >= 0; i--)
+ 				kfree(new->new[i]);
+ 			kfree(new);
+ 			return -ENOMEM;
+ 		}
+ 	}
+ 	new->cachep = cachep;
+ 
+ 	on_each_cpu(do_ccupdate_local, (void *)new, 1, 1);
+ 
+ 	check_irq_on();
+ 	cachep->batchcount = batchcount;
+ 	cachep->limit = limit;
+ 	cachep->shared = shared;
+ 
+ 	for_each_online_cpu(i) {
+ 		struct array_cache *ccold = new->new[i];
+ 		if (!ccold)
+ 			continue;
+ 		spin_lock_irq(&cachep->nodelists[cpu_to_node(i)]->list_lock);
+ 		free_block(cachep, ccold->entry, ccold->avail, cpu_to_node(i));
+ 		spin_unlock_irq(&cachep->nodelists[cpu_to_node(i)]->list_lock);
+ 		kfree(ccold);
+ 	}
+ 	kfree(new);
+ 	return alloc_kmemlist(cachep);
+ }
+ 
+ /* Called with cache_chain_mutex held always */
+ static int enable_cpucache(struct kmem_cache *cachep)
+ {
+ 	int err;
+ 	int limit, shared;
+ 
+ 	/*
+ 	 * The head array serves three purposes:
+ 	 * - create a LIFO ordering, i.e. return objects that are cache-warm
+ 	 * - reduce the number of spinlock operations.
+ 	 * - reduce the number of linked list operations on the slab and
+ 	 *   bufctl chains: array operations are cheaper.
+ 	 * The numbers are guessed, we should auto-tune as described by
+ 	 * Bonwick.
+ 	 */
+ 	if (cachep->buffer_size > 131072)
+ 		limit = 1;
+ 	else if (cachep->buffer_size > PAGE_SIZE)
+ 		limit = 8;
+ 	else if (cachep->buffer_size > 1024)
+ 		limit = 24;
+ 	else if (cachep->buffer_size > 256)
+ 		limit = 54;
+ 	else
+ 		limit = 120;
+ 
+ 	/*
+ 	 * CPU bound tasks (e.g. network routing) can exhibit cpu bound
+ 	 * allocation behaviour: Most allocs on one cpu, most free operations
+ 	 * on another cpu. For these cases, an efficient object passing between
+ 	 * cpus is necessary. This is provided by a shared array. The array
+ 	 * replaces Bonwick's magazine layer.
+ 	 * On uniprocessor, it's functionally equivalent (but less efficient)
+ 	 * to a larger limit. Thus disabled by default.
+ 	 */
+ 	shared = 0;
+ 	if (cachep->buffer_size <= PAGE_SIZE && num_possible_cpus() > 1)
+ 		shared = 8;
+ 
+ #if DEBUG
+ 	/*
+ 	 * With debugging enabled, large batchcount lead to excessively long
+ 	 * periods with disabled local interrupts. Limit the batchcount
+ 	 */
+ 	if (limit > 32)
+ 		limit = 32;
+ #endif
+ 	err = do_tune_cpucache(cachep, limit, (limit + 1) / 2, shared);
+ 	if (err)
+ 		printk(KERN_ERR "enable_cpucache failed for %s, error %d.\n",
+ 		       cachep->name, -err);
+ 	return err;
+ }
+ 
+ /*
+  * Drain an array if it contains any elements taking the l3 lock only if
+  * necessary. Note that the l3 listlock also protects the array_cache
+  * if drain_array() is used on the shared array.
+  */
+ void drain_array(struct kmem_cache *cachep, struct kmem_list3 *l3,
+ 			 struct array_cache *ac, int force, int node)
+ {
+ 	int tofree;
+ 
+ 	if (!ac || !ac->avail)
+ 		return;
+ 	if (ac->touched && !force) {
+ 		ac->touched = 0;
+ 	} else {
+ 		spin_lock_irq(&l3->list_lock);
+ 		if (ac->avail) {
+ 			tofree = force ? ac->avail : (ac->limit + 4) / 5;
+ 			if (tofree > ac->avail)
+ 				tofree = (ac->avail + 1) / 2;
+ 			free_block(cachep, ac->entry, tofree, node);
+ 			ac->avail -= tofree;
+ 			memmove(ac->entry, &(ac->entry[tofree]),
+ 				sizeof(void *) * ac->avail);
+ 		}
+ 		spin_unlock_irq(&l3->list_lock);
+ 	}
+ }
+ 
+ /**
+  * cache_reap - Reclaim memory from caches.
+  * @w: work descriptor
+  *
+  * Called from workqueue/eventd every few seconds.
+  * Purpose:
+  * - clear the per-cpu caches for this CPU.
+  * - return freeable pages to the main free memory pool.
+  *
+  * If we cannot acquire the cache chain mutex then just give up - we'll try
+  * again on the next iteration.
+  */
+ static void cache_reap(struct work_struct *w)
+ {
+ 	struct kmem_cache *searchp;
+ 	struct kmem_list3 *l3;
+ 	int node = numa_node_id();
+ 	struct delayed_work *work =
+ 		container_of(w, struct delayed_work, work);
+ 
+ 	if (!mutex_trylock(&cache_chain_mutex))
+ 		/* Give up. Setup the next iteration. */
+ 		goto out;
+ 
+ 	list_for_each_entry(searchp, &cache_chain, next) {
+ 		check_irq_on();
+ 
+ 		/*
+ 		 * We only take the l3 lock if absolutely necessary and we
+ 		 * have established with reasonable certainty that
+ 		 * we can do some work if the lock was obtained.
+ 		 */
+ 		l3 = searchp->nodelists[node];
+ 
+ 		reap_alien(searchp, l3);
+ 
+ 		drain_array(searchp, l3, cpu_cache_get(searchp), 0, node);
+ 
+ 		/*
+ 		 * These are racy checks but it does not matter
+ 		 * if we skip one check or scan twice.
+ 		 */
+ 		if (time_after(l3->next_reap, jiffies))
+ 			goto next;
+ 
+ 		l3->next_reap = jiffies + REAPTIMEOUT_LIST3;
+ 
+ 		drain_array(searchp, l3, l3->shared, 0, node);
+ 
+ 		if (l3->free_touched)
+ 			l3->free_touched = 0;
+ 		else {
+ 			int freed;
+ 
+ 			freed = drain_freelist(searchp, l3, (l3->free_limit +
+ 				5 * searchp->num - 1) / (5 * searchp->num));
+ 			STATS_ADD_REAPED(searchp, freed);
+ 		}
+ next:
+ 		cond_resched();
+ 	}
+ 	check_irq_on();
+ 	mutex_unlock(&cache_chain_mutex);
+ 	next_reap_node();
+ out:
+ 	/* Set up the next iteration */
+ 	schedule_delayed_work(work, round_jiffies_relative(REAPTIMEOUT_CPUC));
+ }
+ 
+ #ifdef CONFIG_PROC_FS
+ 
+ static void print_slabinfo_header(struct seq_file *m)
+ {
+ 	/*
+ 	 * Output format version, so at least we can change it
+ 	 * without _too_ many complaints.
+ 	 */
+ #if STATS
+ 	seq_puts(m, "slabinfo - version: 2.1 (statistics)\n");
+ #else
+ 	seq_puts(m, "slabinfo - version: 2.1\n");
+ #endif
+ 	seq_puts(m, "# name            <active_objs> <num_objs> <objsize> "
+ 		 "<objperslab> <pagesperslab>");
+ 	seq_puts(m, " : tunables <limit> <batchcount> <sharedfactor>");
+ 	seq_puts(m, " : slabdata <active_slabs> <num_slabs> <sharedavail>");
+ #if STATS
+ 	seq_puts(m, " : globalstat <listallocs> <maxobjs> <grown> <reaped> "
+ 		 "<error> <maxfreeable> <nodeallocs> <remotefrees> <alienoverflow>");
+ 	seq_puts(m, " : cpustat <allochit> <allocmiss> <freehit> <freemiss>");
+ #endif
+ 	seq_putc(m, '\n');
+ }
+ 
+ static void *s_start(struct seq_file *m, loff_t *pos)
+ {
+ 	loff_t n = *pos;
+ 
+ 	mutex_lock(&cache_chain_mutex);
+ 	if (!n)
+ 		print_slabinfo_header(m);
+ 
+ 	return seq_list_start(&cache_chain, *pos);
+ }
+ 
+ static void *s_next(struct seq_file *m, void *p, loff_t *pos)
+ {
+ 	return seq_list_next(p, &cache_chain, pos);
+ }
+ 
+ static void s_stop(struct seq_file *m, void *p)
+ {
+ 	mutex_unlock(&cache_chain_mutex);
+ }
+ 
+ static int s_show(struct seq_file *m, void *p)
+ {
+ 	struct kmem_cache *cachep = list_entry(p, struct kmem_cache, next);
+ 	struct slab *slabp;
+ 	unsigned long active_objs;
+ 	unsigned long num_objs;
+ 	unsigned long active_slabs = 0;
+ 	unsigned long num_slabs, free_objects = 0, shared_avail = 0;
+ 	const char *name;
+ 	char *error = NULL;
+ 	int node;
+ 	struct kmem_list3 *l3;
+ 
+ 	active_objs = 0;
+ 	num_slabs = 0;
+ 	for_each_online_node(node) {
+ 		l3 = cachep->nodelists[node];
+ 		if (!l3)
+ 			continue;
+ 
+ 		check_irq_on();
+ 		spin_lock_irq(&l3->list_lock);
+ 
+ 		list_for_each_entry(slabp, &l3->slabs_full, list) {
+ 			if (slabp->inuse != cachep->num && !error)
+ 				error = "slabs_full accounting error";
+ 			active_objs += cachep->num;
+ 			active_slabs++;
+ 		}
+ 		list_for_each_entry(slabp, &l3->slabs_partial, list) {
+ 			if (slabp->inuse == cachep->num && !error)
+ 				error = "slabs_partial inuse accounting error";
+ 			if (!slabp->inuse && !error)
+ 				error = "slabs_partial/inuse accounting error";
+ 			active_objs += slabp->inuse;
+ 			active_slabs++;
+ 		}
+ 		list_for_each_entry(slabp, &l3->slabs_free, list) {
+ 			if (slabp->inuse && !error)
+ 				error = "slabs_free/inuse accounting error";
+ 			num_slabs++;
+ 		}
+ 		free_objects += l3->free_objects;
+ 		if (l3->shared)
+ 			shared_avail += l3->shared->avail;
+ 
+ 		spin_unlock_irq(&l3->list_lock);
+ 	}
+ 	num_slabs += active_slabs;
+ 	num_objs = num_slabs * cachep->num;
+ 	if (num_objs - active_objs != free_objects && !error)
+ 		error = "free_objects accounting error";
+ 
+ 	name = cachep->name;
+ 	if (error)
+ 		printk(KERN_ERR "slab: cache %s error: %s\n", name, error);
+ 
+ 	seq_printf(m, "%-17s %6lu %6lu %6u %4u %4d",
+ 		   name, active_objs, num_objs, cachep->buffer_size,
+ 		   cachep->num, (1 << cachep->gfporder));
+ 	seq_printf(m, " : tunables %4u %4u %4u",
+ 		   cachep->limit, cachep->batchcount, cachep->shared);
+ 	seq_printf(m, " : slabdata %6lu %6lu %6lu",
+ 		   active_slabs, num_slabs, shared_avail);
+ #if STATS
+ 	{			/* list3 stats */
+ 		unsigned long high = cachep->high_mark;
+ 		unsigned long allocs = cachep->num_allocations;
+ 		unsigned long grown = cachep->grown;
+ 		unsigned long reaped = cachep->reaped;
+ 		unsigned long errors = cachep->errors;
+ 		unsigned long max_freeable = cachep->max_freeable;
+ 		unsigned long node_allocs = cachep->node_allocs;
+ 		unsigned long node_frees = cachep->node_frees;
+ 		unsigned long overflows = cachep->node_overflow;
+ 
+ 		seq_printf(m, " : globalstat %7lu %6lu %5lu %4lu \
+ 				%4lu %4lu %4lu %4lu %4lu", allocs, high, grown,
+ 				reaped, errors, max_freeable, node_allocs,
+ 				node_frees, overflows);
+ 	}
+ 	/* cpu stats */
+ 	{
+ 		unsigned long allochit = atomic_read(&cachep->allochit);
+ 		unsigned long allocmiss = atomic_read(&cachep->allocmiss);
+ 		unsigned long freehit = atomic_read(&cachep->freehit);
+ 		unsigned long freemiss = atomic_read(&cachep->freemiss);
+ 
+ 		seq_printf(m, " : cpustat %6lu %6lu %6lu %6lu",
+ 			   allochit, allocmiss, freehit, freemiss);
+ 	}
+ #endif
+ 	seq_putc(m, '\n');
+ 	return 0;
+ }
+ 
+ /*
+  * slabinfo_op - iterator that generates /proc/slabinfo
+  *
+  * Output layout:
+  * cache-name
+  * num-active-objs
+  * total-objs
+  * object size
+  * num-active-slabs
+  * total-slabs
+  * num-pages-per-slab
+  * + further values on SMP and with statistics enabled
+  */
+ 
+ const struct seq_operations slabinfo_op = {
+ 	.start = s_start,
+ 	.next = s_next,
+ 	.stop = s_stop,
+ 	.show = s_show,
+ };
+ 
+ #define MAX_SLABINFO_WRITE 128
+ /**
+  * slabinfo_write - Tuning for the slab allocator
+  * @file: unused
+  * @buffer: user buffer
+  * @count: data length
+  * @ppos: unused
+  */
+ ssize_t slabinfo_write(struct file *file, const char __user * buffer,
+ 		       size_t count, loff_t *ppos)
+ {
+ 	char kbuf[MAX_SLABINFO_WRITE + 1], *tmp;
+ 	int limit, batchcount, shared, res;
+ 	struct kmem_cache *cachep;
+ 
+ 	if (count > MAX_SLABINFO_WRITE)
+ 		return -EINVAL;
+ 	if (copy_from_user(&kbuf, buffer, count))
+ 		return -EFAULT;
+ 	kbuf[MAX_SLABINFO_WRITE] = '\0';
+ 
+ 	tmp = strchr(kbuf, ' ');
+ 	if (!tmp)
+ 		return -EINVAL;
+ 	*tmp = '\0';
+ 	tmp++;
+ 	if (sscanf(tmp, " %d %d %d", &limit, &batchcount, &shared) != 3)
+ 		return -EINVAL;
+ 
+ 	/* Find the cache in the chain of caches. */
+ 	mutex_lock(&cache_chain_mutex);
+ 	res = -EINVAL;
+ 	list_for_each_entry(cachep, &cache_chain, next) {
+ 		if (!strcmp(cachep->name, kbuf)) {
+ 			if (limit < 1 || batchcount < 1 ||
+ 					batchcount > limit || shared < 0) {
+ 				res = 0;
+ 			} else {
+ 				res = do_tune_cpucache(cachep, limit,
+ 						       batchcount, shared);
+ 			}
+ 			break;
+ 		}
+ 	}
+ 	mutex_unlock(&cache_chain_mutex);
+ 	if (res >= 0)
+ 		res = count;
+ 	return res;
+ }
+ 
+ #ifdef CONFIG_DEBUG_SLAB_LEAK
+ 
+ static void *leaks_start(struct seq_file *m, loff_t *pos)
+ {
+ 	mutex_lock(&cache_chain_mutex);
+ 	return seq_list_start(&cache_chain, *pos);
+ }
+ 
+ static inline int add_caller(unsigned long *n, unsigned long v)
+ {
+ 	unsigned long *p;
+ 	int l;
+ 	if (!v)
+ 		return 1;
+ 	l = n[1];
+ 	p = n + 2;
+ 	while (l) {
+ 		int i = l/2;
+ 		unsigned long *q = p + 2 * i;
+ 		if (*q == v) {
+ 			q[1]++;
+ 			return 1;
+ 		}
+ 		if (*q > v) {
+ 			l = i;
+ 		} else {
+ 			p = q + 2;
+ 			l -= i + 1;
+ 		}
+ 	}
+ 	if (++n[1] == n[0])
+ 		return 0;
+ 	memmove(p + 2, p, n[1] * 2 * sizeof(unsigned long) - ((void *)p - (void *)n));
+ 	p[0] = v;
+ 	p[1] = 1;
+ 	return 1;
+ }
+ 
+ static void handle_slab(unsigned long *n, struct kmem_cache *c, struct slab *s)
+ {
+ 	void *p;
+ 	int i;
+ 	if (n[0] == n[1])
+ 		return;
+ 	for (i = 0, p = s->s_mem; i < c->num; i++, p += c->buffer_size) {
+ 		if (slab_bufctl(s)[i] != BUFCTL_ACTIVE)
+ 			continue;
+ 		if (!add_caller(n, (unsigned long)*dbg_userword(c, p)))
+ 			return;
+ 	}
+ }
+ 
+ static void show_symbol(struct seq_file *m, unsigned long address)
+ {
+ #ifdef CONFIG_KALLSYMS
+ 	unsigned long offset, size;
+ 	char modname[MODULE_NAME_LEN], name[KSYM_NAME_LEN];
+ 
+ 	if (lookup_symbol_attrs(address, &size, &offset, modname, name) == 0) {
+ 		seq_printf(m, "%s+%#lx/%#lx", name, offset, size);
+ 		if (modname[0])
+ 			seq_printf(m, " [%s]", modname);
+ 		return;
+ 	}
+ #endif
+ 	seq_printf(m, "%p", (void *)address);
+ }
+ 
+ static int leaks_show(struct seq_file *m, void *p)
+ {
+ 	struct kmem_cache *cachep = list_entry(p, struct kmem_cache, next);
+ 	struct slab *slabp;
+ 	struct kmem_list3 *l3;
+ 	const char *name;
+ 	unsigned long *n = m->private;
+ 	int node;
+ 	int i;
+ 
+ 	if (!(cachep->flags & SLAB_STORE_USER))
+ 		return 0;
+ 	if (!(cachep->flags & SLAB_RED_ZONE))
+ 		return 0;
+ 
+ 	/* OK, we can do it */
+ 
+ 	n[1] = 0;
+ 
+ 	for_each_online_node(node) {
+ 		l3 = cachep->nodelists[node];
+ 		if (!l3)
+ 			continue;
+ 
+ 		check_irq_on();
+ 		spin_lock_irq(&l3->list_lock);
+ 
+ 		list_for_each_entry(slabp, &l3->slabs_full, list)
+ 			handle_slab(n, cachep, slabp);
+ 		list_for_each_entry(slabp, &l3->slabs_partial, list)
+ 			handle_slab(n, cachep, slabp);
+ 		spin_unlock_irq(&l3->list_lock);
+ 	}
+ 	name = cachep->name;
+ 	if (n[0] == n[1]) {
+ 		/* Increase the buffer size */
+ 		mutex_unlock(&cache_chain_mutex);
+ 		m->private = kzalloc(n[0] * 4 * sizeof(unsigned long), GFP_KERNEL);
+ 		if (!m->private) {
+ 			/* Too bad, we are really out */
+ 			m->private = n;
+ 			mutex_lock(&cache_chain_mutex);
+ 			return -ENOMEM;
+ 		}
+ 		*(unsigned long *)m->private = n[0] * 2;
+ 		kfree(n);
+ 		mutex_lock(&cache_chain_mutex);
+ 		/* Now make sure this entry will be retried */
+ 		m->count = m->size;
+ 		return 0;
+ 	}
+ 	for (i = 0; i < n[1]; i++) {
+ 		seq_printf(m, "%s: %lu ", name, n[2*i+3]);
+ 		show_symbol(m, n[2*i+2]);
+ 		seq_putc(m, '\n');
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ const struct seq_operations slabstats_op = {
+ 	.start = leaks_start,
+ 	.next = s_next,
+ 	.stop = s_stop,
+ 	.show = leaks_show,
+ };
+ #endif
+ #endif
+ 
+ /**
+  * ksize - get the actual amount of memory allocated for a given object
+  * @objp: Pointer to the object
+  *
+  * kmalloc may internally round up allocations and return more memory
+  * than requested. ksize() can be used to determine the actual amount of
+  * memory allocated. The caller may use this additional memory, even though
+  * a smaller amount of memory was initially specified with the kmalloc call.
+  * The caller must guarantee that objp points to a valid object previously
+  * allocated with either kmalloc() or kmem_cache_alloc(). The object
+  * must not be freed during the duration of the call.
+  */
+ size_t ksize(const void *objp)
+ {
+ 	if (unlikely(ZERO_OR_NULL_PTR(objp)))
+ 		return 0;
+ 
+ 	return obj_size(virt_to_cache(objp));
+ }
diff -Ncwr -x .orig -x .rej linux-2.6.23.1/mm/slab.c.rej linux-2.6.23.1km/mm/slab.c.rej
*** linux-2.6.23.1/mm/slab.c.rej	1970-01-01 09:00:00.000000000 +0900
--- linux-2.6.23.1km/mm/slab.c.rej	2008-02-05 20:48:38.000000000 +0900
***************
*** 0 ****
--- 1,36 ----
+ ***************
+ *** 3699,3705 ****
+   	cachep = __find_general_cachep(size, flags);
+   	if (unlikely(cachep == NULL))
+   		return NULL;
+ ! 	return __cache_alloc(cachep, flags, caller);
+   }
+   
+   
+ --- 3700,3708 ----
+   	cachep = __find_general_cachep(size, flags);
+   	if (unlikely(cachep == NULL))
+   		return NULL;
+ ! 	a = __cache_alloc(cachep, flags, caller);
+ !  	kmalloc_account(a, size, size);
+ ! 	return a;
+   }
+   
+   
+ ***************
+ *** 3804,3809 ****
+   	struct kmem_cache *c;
+   	unsigned long flags;
+   
+   	if (unlikely(!objp))
+   		return;
+   	local_irq_save(flags);
+ --- 3807,3814 ----
+   	struct kmem_cache *c;
+   	unsigned long flags;
+   
+ + 	kfree_account(objp, ksize(objp));
+ + 
+   	if (unlikely(!objp))
+   		return;
+   	local_irq_save(flags);
